Z532uNJyG5y	Iterative Graph Self-Distillation	long	Summary:This paper proposes a self-distillation based graph augmentation mechanism to alleviate the drawbacks of existing MI based models w.r.t. their high dependency towards negative sampling. Quantitatively the proposed model achieves encouraging results. However it would have been better if the system designs and significant difference of IGSD from existing work are discussed.Strength:- This work has clearly discussed a drawback of existing unsupervised MI based models which is the leading approach in graph classification- They propose a mechanism to address this issue with satisfiable quantitative results on unsupervised setting and extended semi-supervised setting with self-training also supported quantitatively.- Paper is clear in general, with a clear research problem, proposes mechanism for unsupervised/semi-supervised graph representation domain and encouraging quantitative results.Weakness:-There is a lack of qualitative analysis and discussion of the proposed method. -In Section 4.3 "Performance with different amount of negative pairs", it is not clear the reasoning of the provided observation from Figure 3a.-It is not clear the motivation behind selecting a teacher-student network for obtaining different views of the graph. These networks are normally used for knowledge transfer, but here used for contrastive learning. How is this more beneficial than an ensemble model w/o knowledge transfer step of Eq. 3. -The core difference of IGSD from CMC-graph is that CMC uses MI based contrastive between local patch representation and graph rep, while IGSD uses L2 based contrastive between 2 graph representations. Input, Encoders and projections are the same for both architectures. It could be useful to add some analysis to discuss these differences and their contributions to clearly understand the significance of IGSD.-This paper seems to have state-of-the-art results (although it is based on graph kernel). Why the results are not included?Convolutional Kernel Networks for Graph-Structured Data, ICML-2020=======================after rebuttal:I thank the authors for the response. I still have concerns re their comparison with GCKN (ICML-2020). The reproduced results by the authors are different significantly from the published one in Table 1 of GCKN paper (~5%). E.g. MUTAG is 92.8 in original paper, but the authors report 87.2 for GCKN. The difference is significant.Therefore, I will keep my original rating.  |||| rating: 5: Marginally below acceptance threshold |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature	Overall Comments:Learning graph-level representations with only labels has been explored by many works. However, it's not easy to annotate every graph. This paper applies the ideas from semi-supervised classification task to improve the representation quality learned by graph neural network. Specifically the proposed solution combines several kinds of existing techniques including diffusion graph augmentation, mean teacher consistency, debiased contrastive loss and pseudo class consistency. Finally they are combined together to act as a regularization term by utilizing the unlabelled data. From this point of view, the novelty of this work is incremental, but it's still an interesting work for improving graph-level representations.Clarity:The presentation is not clear enough. There exists many claims that are not clear, shown as follows:1. In the last sentence of 3rd paragraph in introduction section, it's difficult to get the connection between negative samples mining and self-distillation strategy. Why using the self-distillation can alleviate the dependency on negative samples mining? The unsupervised objective in Equation 4 still depends on negative samples. 2. In Section 2.1, the notation for augmentations. Why are the graphs G_L attached without labels after being augmented?3. In Section 2.3, authors firstly use PPR to augment node features, then randomly remove edges to create a corrupted graphs. According to the description, the question is how many views that will be used in follow sections? I guess that the graph feature from original graph will be fed to student network, and the augmented corrupted graph will be fed to teacher network.Questions for Rebuttal:1. Please clarify the mentioned questions above.2. The proposed method contains an encoder, projector, and predictor. The question is why we need a projector g to get a higher dimension z? Does it have a big influence on the performance? Could you please give the complete definition of function g and h?3. The definition of L^{con}  in Equation 2 is for positive sample extracted from the same graph G_i. However, the complete unsupervised loss needs negative samples G_j. Could you please also give the definition for L^{G_i, G_j}?4. The overall loss consists of supervised and unsupervised loss. The L^{sup} has conflict to the first term in Equation 7. Both of them use labels but it's difficult to tell which one should be aligned with the supervised loss shown in the ablation study (Table 2). The SupCon has never been shown in the main content before. Please pay attention to make it clear. |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	------------------------------------update after reading the authors' response.The authors didn't address my question "Did the authors perform a significance study?" A significance test such as double-sided t-test is needed to verifying whether the proposed method is significantly better than baselines.-------------------------------This paper proposed a distillation approach for unsupervised graph representation learning. The approach partially builds upon contrastive self-supervised learning which contrasts pairs of augmented graphs. The approach is extended to the semi-supervised setting. The authors performed evaluation in graph classification and regression tasks. I recommend to reject this paper, due to the following major concerns: 1) experimental results are not strong; 2) important baselines were not compared; 3) important details such as optimal hyperparameter values are missing. My major concerns of this paper include:1. The improvement of the proposed approach over baselines seem not significant. For example, in Table 1, comparing the mean and standard deviation of the proposed approach and CMC-GRAPH, it seems that the difference is not statistically significant. Did the authors perform a significance study?2. In the experiments, why the authors didn't compare with GCC, which is a contrastive self-supervised learning approach applied to graph classification?3. There are many other unsupervised graph representation learning methods. The authors need to compare with more to substantiate this work.4. In hyperparameter tuning, the authors gave the range of hyperparameters tuned, but didn't give the optimal value of the hyperparameters, which make the paper difficult to reproduce.5. In table 1, the authors excluded some results since they need more than 1 day to obtain. It is common for deep learning models to run several days to obtain results. I don't think it is proper to exclude these results simply because the runtime is more than 24 hours.However, the paper does have a few strong points.1. The ablation studies are well designed and the results are insightful.2. The paper is well-written and easy to follow, with a clear organization.3. The experiments were conducted on a rich collection of datasets. Other comments.1. In equation (3), the authors can draw a connection with MoCo.2. In Table, why didn't report mean and standard deviation of the results?3. For this result "When batch size isgreater than 32, IGSD outperforms CMC-Graph and the performance gap becomes larger as the batchsize increases.",  can the authors provide a reason that can possibly explain this phenomenon?4. The authors can add some statistics of the datasets used in Figure 2. |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper proposed a method for learning graph-level representation in an unsupervised contrastive way. Instead of contrasting between graph-level representation and patch representation like InfoGraph [1], they contrast graph-level representation of a graph to its augmented variation using a teacher-student framework.* why use InfoNCE objective instead of the Jensen-Shannon mutual information objective used in InfoGraph [1] ? * The major concern about this paer is that the proposed method encourages the closeness of augmented views from the same graph instances but provide no guarantee that the transformation used (graph diffusion and sparsification with PPR + random remove edges in this paper) would be label-preserving. For example, in molecular datasets, if we drop an edge and that edge happens to be in a structural motif, it will drastically change the attributes/labels of the molecule. * $v$ represents nodes in section 2.1 and 2.2 and it represents graph instances in section 3.1 and Figure 1. This can be confusing I suggest changing the notation in section 3.1 and Figure 1.1 to $G$[1] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization. arXiv preprint arXiv:1908.01000, 2019. |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
H1MzKs05F7	Adversarial Vulnerability of Neural Networks Increases with Input Dimension	long	The paper studies how the vulnerability of a neural network model depends on its input dimension. The authors prove that for an *untrained* model, randomly initialized with Xavier initialization, the gradient of the loss wrt the input is essentially independent of the architecture and task. This implies that the major factor affecting the norm of that gradient is the input dimension. They then support their argument by experiments measuring the relation between adversarial vulnerability and gradient norm using various *trained* models (including adversarially regularized ones).I find the main theoretical result interesting. While this is a known fact for the simple case of linear classifiers, extending it to arbitrarily deep networks is a valuable contribution. The proof crucially relies on properties of the specific initialization scheme to show that the gradient does not change too much during backproparagation through the layers. The most significant limitation of the result (which the authors kindly acknowledge) is that this result only holds at initialization. Hence it cannot distinguish between different training methods or between how different architectures evolve during training. Since the situation in adversarial robustness is much more nuanced, I am skeptical about the significance of such statements.On the experimental side, the finding that gradient regularization improves adversarial robustness to small epsilon values has been made multiple times in the past (as the authors cite in the related work section). It is worth noting that the epsilon considered is 0.005 in L_inf (1.275/255) which is pretty small. This value corresponds to the "small-epsilon regime" where the behavior of the model is fairly linear around the original inputs and thus defenses such as FGSM-training and gradient regularization are effective.The authors also perform an interesting experiment where they train models on downsampled ImageNet datasets and find that indeed larger input dimension leads to more vulnerable models.While I find the results interesting, I do not see clear implications. The fact that the vulnerability of a classifier depends on the L1 norm of the input gradient is already known for any locally linear classifier (i.e. deep models too), and it is fairly clear that the L1 norm will have a dimension dependence. The fact that it does not depend on architecture or task at initialization is interesting but of limited significance in my opinion. Given that the experimental results are also not particularly novel, I recommend rejection.[UPDATE]: Given the overall discussion and paper updates, I consider the current version of the paper (marginally) crossing the ICLR bar. I update my score from a 5 to a 6.Minor comments to the authors:-- I think || x ||_* is more clear than |||x||| for the dual norm.-- Consider using lambda for the regularization, epsilon is confusing since it is overloaded. |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper argues that adversarial vulnerability of neural networks increases with input dimension. Theoretical and empirical evidence are given which connect the l_p norm of the gradient of the training objective with the existence of small-worst case l_q perturbations. This connection is made by assuming that the learned function is well approximated by a linear function local to the sampled input x. By making assumptions on the initialization scheme for some simple architectures, the authors show that the l_p norm of the gradient for randomly initialized network will be large, and provide empirical evidence that these assumptions hold after training. These assumptions imply bounds on the typical magnitude of the gradient of the loss with respect to a single input coordinate, this then implies that the overall gradient norm will depend on the input dimension.I found this paper well written. The mathematical assumptions are presented in a clear, easy to understand manner. Also high level intuition is given around their main theorems which help the reader understand the main ideas. However, I have a  number of concerns about this work.The first is, I do not buy the motivation for studying the "phenomenon" of small worst-case l_p perturbations. I realize this statement applies to a large body of literature, but since the publication of  [1] we are still lacking concrete motivating scenarios for the l_p action space. I would encourage the authors instead to ask the closely related but more general question of how we can improve model generalization outside the natural distribution of images, such as generalization in the presence of commonly occurring image corruptions [2]. It's possible that the analysis in this work could better our understanding model generalization in the presence of different image corruptions, indeed by making similar linearity assumptions as considered in this work, test error in additive Gaussian noise can be linked with distance to the decision boundary [3,4]. However, this particular question was not explored in this work.Second, the work is one of many to relate the norm of the gradient with adversarial robustness (for example, this has been proposed as a defense mechanism in [5,6]). I also suspect that the main theorem relating gradient norm to initialization should easily follow for more general settings using the mean field theory developed by [7,8] (this would be particularly useful for removing assumption H1, which assumes the ReLU activation is a random variable independent of the weights). Overall, I don't see how gradient norms explain why statistical classifiers make mistakes, particularly for more realistic attacker action spaces [9]. Even for "small" l_p adversarial examples there seem to be limitations as to how much gradient norms can explain the phenomenon --- for example even max margin classifiers such as SVM's have "adversarial examples". Furthermore, adversarial training has been shown to reach a point where the model is "robust" locally to training points but this robustness does not generalize to the points in the test set [10]. In fact, for the synthetic data distributions considered in [10], it's proven that no learning algorithm can achieve robustness given insufficient training data.Finally, the main conclusion of this work "adversarial vulnerability of neural networks increases with input dimension" is an overly general statement which needs a much more nuanced view. While experiments shown in [11] support this conclusion for naturally trained networks, it is shown that when adversarial training is applied the model is more robust when the input dimension is higher (see Figure 4 a. and b.). Perhaps the assumptions for Theorem 4 are violated for these adversarially trained models. 1. https://arxiv.org/abs/1807.067322. https://arxiv.org/abs/1807.016973. https://arxiv.org/abs/1608.089674. https://openreview.net/forum?id=S1xoy3CcYX&noteId=BklKxJBF57.5. https://arxiv.org/abs/1704.088476. https://arxiv.org/abs/1608.076907. https://arxiv.org/abs/1611.012328. https://arxiv.org/abs/1806.053939. https://arxiv.org/abs/1712.0966510. https://arxiv.org/abs/1804.1128511. https://arxiv.org/pdf/1809.02104.pdf |||| rating: 4: Ok but not good enough - rejection |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature	The authors provide a compelling theoretical explanation for a large class of adversarial examples.  While this explanation (rooted in the norm of gradients of neural networks being the culprit for the existence of adversarial examples) is not new, they unify several old perspectives, and convincingly argue for genuinely new scaling relationships (i.e. \sqrt(d) versus linear in d scaling of sensitivity to adversarial perturbations versus input size). They prove a number of theorems relating these scaling relationships to a broad swathe of relevant model architectures, and provide thorough empirical evidence of their work.I can honestly find very little to complain about in this work--the prose is clear, and the proofs are correct as far as I can tell (though I found Figure 4 in the appendix (left panel) to not be hugely compelling.  More data here would be great!)As much of the analysis hinges on the particularities of the weight distribution at initialization, could the authors comment on possible defenses to adversarial attack by altering this weight distribution? (By, for example, imposing that the average value must grow like 1/d)? |||| rating: 9: Top 15% of accepted papers, strong accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper analyzes the relationship between "adversarial vulnerability" with input dimensionality of neural network. The paper proves that, under certain assumptions, as the input dimensionality increases, neural networks exhibit increasingly large gradients thus are more adversarially vulnerable. Experiments were done on neural networks trained by penalizing input gradients and FGSM-adversarial training. Similar trends on vulnerability vs dimensionality are found.The paper is clearly written and easy to follow. I appreciate that the authors also clearly stated the limitation of the theoretical analysis.The theoretical analyses on vulnerability and dimensionality is novel and provide some insights. But it is unlikely such analysis is significant There are a few reasons:- This analysis only seems to work for "well-behaved" models. For models with gradient masking, obfuscated gradients or even non-differentiable models, it is not clear that how this will apply. (and I appreciate that the authors also acknowledge this in the paper.) It is unclear how this specific gradient based analysis can help the understanding of the adversarial perturbation phenomena. After all, the first order Taylor expansion argument on top of randomly initialized weights is oversimplifying the complicated problem.- One very important special case of the point above: the analysis probably cannot cover the  adversarially PGD trained models [MMS+17] and the certifiably robust ones. Such models may have small gradients inside the box constraint, but can have large gradients between different classes.On the empirical results, the authors made a few interesting observations, for example the close correspondence between "Adv Train" and "Grad Regu" models. My concern is that the experiments were done on a narrow range of models, which only have "weak" adversarial training / defenses.Adversarial robustness is hard to achieve. What matters the most is "why the strongest model is still not robust?" not "why some weak models are not robust?" It is especially worrisome to me that the paper does not cover the adversarially-augmented training based iterative attacks, e.g. PGD TRAINED models [MMS+17] which is the SOTA on MNIST/CIFAR10 L_\infty robustness benchmark.Without comprehensive analyses on SOTA robust models, it is hard to justify the validity of the theoretical analysis in this paper, and the conclusions made by the paper.For example, re: the last sentence in the conclusion: "They hence suggest to tackle adversarial vulnerability by designing new architectures (or new architectural building blocks) rather than by new regularization techniques." The reasoning is not obvious to me given the current evidence shown in the paper.[MMS+17] Madry A, Makelov A, Schmidt L, Tsipras D, Vladu A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 |||| rating: 5: Marginally below acceptance threshold |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature
c3MWGN_cTf	Policy Optimization in Zero-Sum Markov Games: Fictitious Self-Play Provably Attains Nash Equilibria	long	This paper is about the design and analysis of policy optimization algorithms that provably converge to a Nash equilibrium at a sublinear rate for a class of zero sum Markov games, which are one of the simplest settings of multi-agent RL --- in particular, for zero sum Markov games satisfying a Lipschitz regularity condition. Each players adopts an entropy-regularized policy optimization method (which the authors call as smooth Fictitious Self Play).This is an important topic and the question studied is an important step to take given that we don't know whether Fictitious Self Play is guaranteed to converge in Markov games. However, I am quite surprised by very important related work missing in the paper. For instance, the NeurIPS 2019 paper on "Policy Optimization Provably Converges to Nash Equilibrium in Linear Quadratic Games" is not cited even though it is quite close to the topic of this paper: it also studies a policy optimization algorithm, linear quadratic games are also zero-sum Markov games, the objective studied is also non-convex non-concave. Of course LQ games are special class of zero-sum Markov games, but this paper makes some assumptions like Lipschitz regularity as well. Therefore claims like this paper is the first to prove convergence guarantees of policy optimization algorithms for zero sum Markov games are not quite true. The somewhat less related, but still quite close NeurIPS 2019 paper "Model-based multi-agent RL in zero-sum Markov games with near optimal sample-complexity" is not cited as well. These papers are not obscure: a simple search for the submitted paper's title brings up these papers.Also, the Lipschitz regularity assumption being made is important enough that it is good to add it in the abstract, as the abstract feels misleading otherwise. And the importance/restrictiveness of this assumption is ideally discussed in more detail in the introduction. Overall I think this would make a good paper after fixing the above, but not right now.  |||| rating: 5: Marginally below acceptance threshold |||| confidence: 2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper	This paper studies the problem of learning to play a Nash equilibrium in two-player, zero-sum Markov games.  This is a longstanding problem, with many algorithms proposed but relatively few theoretical convergence guarantees, and most of those either for quite restricted settings or with strong assumptions.  This is in stark contrast to the stateless setting of Normal Form Games, where we have many strong theoretical convergence guarantees.  The main algorithm is a version of the classic fictitious play algorithm.  Like prior adaptations of fictitious play to Markov Games, it operates on the Q-values, but a key novelty (at least in the stateful setting; similar ideas were recently applied in a special case of normal form games by Swenson and Poor 2019) is the use of a particular form of regularization in the best response process.  The main result is that as long as the game satisfies Lipschitz and Concentratability properties for each player when the other plays optimally and the policy updates are sufficiently accurate then play converges to a Nash equilibrium.I like this paper quite a bit.  It tackles a hard problem  and makes solid progress.  I think the algorithm and analysis are both nice contributions and definitely intend to study the latter further as I think aspects of it may be useful in other settings.  Overall the presentation, while dense, is clear.  However, I believe there are a few issues that could use additional discussion: 1) Why does the uniqueness, or lack thereof, of the Nash equilibrium not matter to the results?  Quite a bit of prior work has had caveats when they are not unique.  The results seem to hold if the assumptions are true for at least one equilibrium, presumably because of the minimax properties in a zero-sum setting, but I’m not quite clear how this interacts with the assumptions.  For example, if one but not all the equilibra cause the game to satisfy Assumption 4.2 and 4.3 what causes the guarantees to still hold even if initially play gravitates toward some equilibrium where they do not?2) I’m not quite clear how to interpret the convergence guarantee in Theorem 4.5.  The text after the theorem talks about the policy sequence converging to a neighborhood, while the theorem itself is about the averages across the sequence of policies.  It would help to have some more detailed discussion of exactly what sort of convergence behavior we should expect.3) I’m intrigued by the observation at the end that this algorithm is Hannan consistent under stronger assumptions.  There has been some work recent work exploring connections between regret minimization and RL and it would be worth discussing a bit how this observation relates to that literature, e.g.: @inproceedings{hennes2020neural,  title={Neural Replicator Dynamics: Multiagent Learning via Hedging Policy Gradients},  author={Hennes, Daniel and Morrill, Dustin and Omidshafiei, Shayegan and Munos, R{\'e}mi and Perolat, Julien and Lanctot, Marc and Gruslys, Audrunas and Lespiau, Jean-Baptiste and Parmas, Paavo and Du{\`e}{\~n}ez-Guzm{\'a}n, Edgar and others},  booktitle={Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},  pages={492--501},  year={2020}} |||| rating: 8: Top 50% of accepted papers, clear accept |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	The authors consider self-play in zero-sum discounted two-player Markov games with compact state space and finite actions. They present a smooth fictitious self-play algorithm where each player adopts an entropy-regularized policy optimization method with the average of the past generated Q-values. Under appropriate assumptions, among which a Lipschitz regularity of the Markov game, the authors prove that this algorithm approximates the Nash equilibrium at a rate O(1/T) where T is the number of iteration.-Contributions-algorithmic: Smooth FSP algorithm a smooth version of fictitious self-play.-theoretical: Convergence rate of order O(1/T) of Smooth FSP under appropriate conditions.-Score justification/Main commentsThe paper is well written. The proofs seem correct but I did not check everything in detail (see specific comments). My main concern is that the different assumptions made are a bit ad hoc (sometimes the assumption relates directly on the sequence of policies generated by the algorithm). And thus it is hard to assess if the provided bound is relevant or a trivial consequence of the assumptions (see specific comments below). As a sanity check and for a simpler proof it could be interesting to first present and analyze Smooth FSP without the estimation and approximation part first. In fictitious play, each player plays the best response against the average of the past policies played by the adversary. Here it is not really the case since the policy used by one player is a  (smooth) best-response against a weighted average of the past Q-value, which depends also on the policy played by that player. Thus the link with Fictitious play is not completely clear. I’m also curious about the reduction of the presented algorithm to matrix game. Do we recover a known algorithm, and what can we say about the convergence rate of the algorithm?  -Detailed comments P2: “remains less less understood” and what do you mean by “classical optimization”?P4, Section 3.1: the mixed policy as you defined it is not a policy (you cannot express as a certain function of s) thus talking about its Q-value does not make sense.P5: Is the normalization parameter \kappa_{t+1,(i)} a learning rate of the normalization constant such that the probability sum to one? In the second case, it should be additive.P6: I do not understand the last sentence before Section 3.3 what do you mean by obtained from (3.3) and “which operates in the functional space given the marginalized”?P5, Section 3.3: \Theta is not defined, how do you parameterized \cE_{\theta} exactly? What do you mean by “the estimator of the marginalized[...]”, how do you construct it?  P13, Appendix A: maybe you should say that you consider the Lagrangian on the constrained optimization problem and if it the case also add the constraint on the fact that the \pi(a|s)\geq 0.P6: In (4.3), you mean when \nu_t is close enough to \nu^*?P6, Assumption 4.2: could you provide a non-trivial example where this assumption is correct? Furthermore, the assumption is made on the algorithm that you propose rather than on the model is very suspicious. In particular, since the sequence \pi_{\theta^t} is a random sequence (because based on estimated quantities) in which sense the inequality holds? Almost surely?P7, (4.7): h is an integrable function with respect to which measure? And similarly, the L1-norm is defined with which measure?P7, Assumption 4.3: Could you provide a non-trivial example where this assumption is correct? And this assumption is not weaker than the one proposed by Radanovi et al because the quantities E_{\nu^*}[ KL(…)] and max_{s} || ...||_1 are not comparable. P7, Assumption 4.4: Again since you are manipulating random quantities you should precise about what you mean by this inequality. Furthermore, there is in fact no assumption here but just introducing the notations \epsilon_t and \epsilon_t’ (if we allow them to be in \bar{R}). The assumption would rather be that \sigma = O(1). And I’m not totally convinced it is a reasonable assumption. For example in (4.1) if \nu^t is singular with respect to \nu^* then the MSE computed with \nu_t will provide no information for the state where \nu^* is supported.P8, Theorem 4.5: In fact M_i depends on \lambda_i trough V_{(i)}^{max}…, thus it is not clear at all if you can set \lambda_i \geq 2M_i. In fact, you are adding an additional constraint on the different parameters. Could you make this explicit in the statement of the theorem? P16: In (C.10) in seems that you used t+1-(\lambda_i+M_i)/(max_{i} \lambda_i+M_i) \leq t which is wrong.P16: In (C.11) the left-hand side should be divided by T.P23: In (G.2) it should be \log(\bar{\pi}^i_{t+1}) instead of \log(\pi^i_{t+1})?P23, end of the proof of Lemma C.4: it is \tilde{Q}. |||| rating: 5: Marginally below acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	This paper studies the two-player zero-sum Markov game using fictitious self-play (FSP) strategies. The authors proposed a novel entropy regularized policy optimization method for both agents. They proved the sequence of joint policies converges to a neighborhood of a Nash equilibrium at a sublinear rate. The paper is well written though the notations are a little bit complicated for readers to understand. The results seem to be rigorous. One drawback is that the proposed algorithm is not evaluated using any empirical studies. Since the algorithm is new to the literature, it would be expected to see how it performs compared with other baseline methods in experiments. Have you considered the stochastic variance reduced policy gradient methods? There has been an active line of work (see [1-5] for some examples) that shows the variance reduction techniques can improve the convergence rate of policy optimization methods in the single-agent setting. It would be interesting to know whether the convergence of the smooth FSP can be also improved using the same techniques.[1] Papini M, Binaghi D, Canonaco G, Pirotta M, Restelli M. Stochastic Variance-Reduced Policy Gradient. InInternational Conference on Machine Learning 2018.[2] Xu P, Gao F, Gu Q. An improved convergence analysis of stochastic variance-reduced policy gradient. In Uncertainty in Artificial Intelligence 2019.[3] Shen Z, Ribeiro A, Hassani H, Qian H, Mi C. Hessian aided policy gradient. In International Conference on Machine Learning 2019.[4] Xu, P., Gao, F. and Gu, Q. Sample Efficient Policy Gradient Methods with Recursive Variance Reduction. In International Conference on Learning Representations 2020.[5] Huang F, Gao S, Pei J, Huang H. Momentum-Based Policy Gradient Methods. In International Conference on Machine Learning 2020.Equation (2.3) is not entropy regularized. Instead, the state-reward function is entropy regularized.How is the mean squared error in (3.7) solved? In the proposed algorithm, it is assumed that this can be exactly solved. However, a practical approximation of this solution will cause additional estimation error. As is required in Equation (4.13), it seems that the authors assume the estimation error to be roughly in the order of 1/t^2. I am not sure whether this strong convergence can be established using sampled data for (3.7).The convergence result in Theorem 4.5 is upper bounded by a very large term \lambda_i \log|A^i|, where \lambda_i is larger than the Lipschitz constant, and |A^i| is the size of the action space. If I understand it correctly, both quantities are nonvanishing and thus the result in Theorem 4.5 is not convergent. I did not see any discussion in the paper to address this issue or discuss how the neighborhood can be shrunk to a smaller region.  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
NjImFaBEHl	Divide and Contrast: Source-free Domain Adaptation via Adaptive Contrastive Learning	long	The authors proposed a method for source-free unsupervised domain adaptation (SFUDA) task. The key idea is to combine the advantages of global alignment [1] and feature consistency [2]. The authors divided the target data into source-like and target-specific samples and treat them by different learning methods. The authors demonstrate extensive experiments on three datasets and verify the performance of the proposed method Strengths1.The paper is well-organized and easy to follow.2. The "divide and contrast" strategy is simple but effective; it can exploit both the global and local structures of target data.3. The proposed Exponential-MMD loss is novel, it also makes sense to align the source-like and target-specific samples to reduce distribution mismatch.4. The experiment and ablations are sufficient to support the conclusion.Weaknesses1. The section of preliminaries and analysis is a bit unclear to introduce the task.2. A more recent work that outperforms proposed approach ([a]) is not compared against.[a] Liang, Jian, et al. "Source data-absent unsupervised domain adaptation through hypothesis transfer and labeling transfer." IEEE Transactions on Pattern Analysis and Machine Intelligence (2021).3. Eqn. 4 and Eqn. 5 share the same parameter \tau but they are different from each other. The threshold \tau in Eqn. 4 is set to 0.95 according to supplementary material following [47], however, I could not find the reference [47]. Besides, the choice of the threshold \tau is not included in the ablation study, it would be necessary to see the ablation of choosing the threshold \tau.4. As described in Discussion(L200-L207), unlike previous method [3], Eqn. 5 jointly achieves class-wise adaptation and instance-wise adaptation. It would be interesting if the authors could compare the proposed loss with separate class-wise adaptation and instance-wise adaptation losses.5. When do the method update the source-like set and class centroids, after one batch or one epoch. Yes  |||| rating: 5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper presents a source-free domain adaptation method. Previous methods either use self-supervised pseudo labeling to conduct class-wise global alignment or leverage local structure to enforce feature consistency. This work combines the idea of both. The proposed method divides target samples into source-like and target-specific ones. Source-like samples are used for global class clustering and target-specific samples are used for learning local structures. The two are further aligned using maximum mean discrepancy loss. Strengths are as follows.The idea is interesting. The target samples are divided based on confidence output of source classifier. Different groups are treated differently, either globally in class-level or locally in instance-level. Two different groups are aligned to encourage consistency that is also interesting. The presentation is generally good. Ablation study is conducted for each part.Weaknesses are as follows.The major weakness is the performance compared to prior work. It shows that the results of the proposed method are just marginally good compared to [2] and [3] in 2 out of 3 datasets (table 1 and table 3). The performance is good in table 2. Can the authors explain more about the baseline results? For example, how the official code was used and how the authors ensure the hyper parameters are reasonably well tuned?  No  |||| rating: 5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper proposes a new source-free unsupervised domain adaptation method named DaC. The key idea is to leverage the advantages of existing “global” methods and “local” counterparts. Specifically, DaC uses the source model to split the target data into source-like and target-specific samples. After that, an adaptive contrastive learning strategy is used to achieve class-wise adaptation (global) and local consistency (local). Finally, MMD is used to minimize the distribution mismatch between source-like samples and target-specific samples. The proposed method achieves the best performance on widely used benchmarks. ---Originality: the proposed method is a combination of well-known techniques, but achieves good performance. Integrating contrastive learning into source-free domain adaptation is novel and brings new insights into this community.---Quality: Strengths: this work is technically sound with theoretical proof and empirical evaluation. The effectiveness of the proposed regularizations are evaluated on various downstream tasks. The work is complete.Weakness: (1) the major concern is the performance. As shown in Table 1 and Table 3, compared to existing SOTA methods (NRC and CPGA), the performance improvement is quite limited. Although it outperforms the baseline SHOT by a large margin, it is still hard to convince readers. (2) The proposed method is not evaluated on Digit datasets (MNIST, SVHN, and USPS) and the Office31 dataset which are two important benchmarks. Any reason behind this?---Clarity: this paper is well written and well organized. It is easy to follow. Detailed implementation details are also provided in the supplementary.---Significance: compared to the baseline SHOT, the result is important. Specifically, the proposed method brings a larger improvement than SHOT. However, it is not clear to me whether other methods can borrow the same idea and get performance improvement.--- Not applicable.  |||| rating: 5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly. |||| confidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.	This paper is about unsupervised domain adaptation when the source data is unavailable at the time of adaptation. Authors term this problem as source-free unsupervised domain adaptation (SFUDA). Authors make the insightful  observation that existing SFUDA techniques use all pseudo labels on target data including noisy labels or enforce local feature consistency at the expense of being source-biased. Motivated by these observations, authors propose a new SFUDA approach that they call Divide and Contrast (DaC) which divides the target data into two disjoint groups: source-like samples and target-specific samples. An adaptive learning framework is presented that treats each of these target sample groups differently during the training process. Authors present both theoretical results and numerical results that illustrate the superiority of DaC over state of the art methods for SFUDA. StrengthsAuthors illustrate well (e.g., Fig. 1) the tradeoffs involved in the global and local approaches to SFUDA.The proposed DaC approach for SFUDA is a novel approach to address the limitations of prior methods to SFUDA.Authors present theoretical derivations to justify the DaC approach.Numerical results shown on multiple datasets are pretty convincing of the superiority of DaC approach.Weaknesses"Memory bank" is invoked early on without sufficient explanation. It is explained in Section 4.2.2, but may make the paper more readable if this introduction could be provide earlier in the manuscript.No error bars are provided for the numerical results. Authors do not explicitly discuss the limitations of the proposed method, but mention semi-supervised SFUDA  and source-free open-set DA as possible research extension topics. If space permits, it may be interesting to provide a few more sentences about each of these topics and how DaC might fare in those cases.  |||| rating: 8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
r7L91opmsr	Partial Rejection Control for Robust Variational Inference in Sequential Latent Variable Models	long	This paper describes an SMC algorithm to sample the posterior distribution of latent states $p_\theta(z_{1:T}|x_{1:T})$ in a latent variable models $p_\theta(x_{1:T}, z_{1:T})$. The authors consider a completely general setting (the authors assume Eq.(1) but clearly there is nothing to assume here, this the standard Bayes rule). It is well known that the vanilla SMC sampler is a good candidate for ELBO because it provides an unbiased estimator of the likelihood. But the authors prefer here to use a more sophisticated version of the  SMC algorithm, which features a partial rejection algorithm, which amounts to eliminate proposed particles which are "large enough" likelihood.It is difficult for an expert in SMC algorithms to understand the algorithm as it is described (one must even guess the meaning of the notations). Equation 3) is rather misleading because it is not understood that one continues the acceptance/rejection sampling step until the sample is accepted [Peters' paper, a little less ambitious in its generality, is much more readable]. Also, the form of the rejection probability does not help to understand the action beyond the scene. The level of generality here is a killer: what the hyperparameter $M(i,t-1)$ means, what is the meaning of this form of rejection probability (we see something like a Barker ratio), this is very puzzlingThe rejection probability modifies the mutation kernel and should be taken into account when computing the importance weights (Eqs. (4) and (5)). This implies to estimate a quantity $\alpha_t(z_{1:t}^i)$ which is not tractable. The authors suggest (similar to Peters (2012)) to use a Monte Carlo estimator of these quantities. To sample the ancestors variables from Eq. (7) with the $\alpha_t(z_{1:t}^i)$ defined in Eq. (6), the authors use the "Dice-Enterprise" algorithm. It does not seem necessary to appeal to such  beautiful algorithm to understand the validity of the algorithm described page 3. Here again, everything is done to frighten the reader and not much is done to explain what is being done... The results presented are encouraging and shows that the proposed approach outperforms IWAE and FIVO for a given calculation time. This result is clearly interesting and shows that partial rejection helps despite the additional difficulties linked with the intractability which requires an additional layer of complexity. I like the paper even if I have found it extremely unfriendly to read !   |||| rating: 7: Good paper, accept |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature	### Review update following author discussionI've read the author responses as well as some of the discussion with the other reviewers. Overall, this is valuable work and I've considered raising my score, but I think a weak accept is appropriate, all things considered.  I've raised the confidence score for my review, as I understand the technical details better now. I think a key strength compared to prior work is the empirical validation of the approach on a variational RNN.  However, the significance of the paper in terms of the novelty of the ideas, both conceptual and technical are overstated in my opinion, hence the weak accept. One other point of feedback for the final version in case of an accept:I also share R3's concern about the paper's positioning in relation to the extremely general dice enterprise framework (or, even the bernoulli factory, for that matter) as somewhat misleading for the particular use case in Section 2.2.  The exact same multinomial sampling scheme is in fact more succinctly presented (proposed?) in BRPF [Schmon et. al, 2019] as the "Bernoulli Race" (which the authors have cited).  I would think that the current scheme is a special case of the "Bernoulli race" that uses a particular form of the acceptance probability parameterization similar to prior work (e.g. VRS [Grover et. al. 2018]). See Section 3, specifically e.g. Eq (10) and Proposition 2 from BRPF [Schmon et. al., 2019], which can be compared to Eq (3) and Proposition 1 in the current submission, respectively. Minor nit: In step 3 of the algorithm (right below Eq (8)), you use the notation $z_t$ for sampling a new variable from $q_\phi$. However, this $z_t$ has nothing to do with the latent variables that are used in computing the constants $c^i_t$. The way it's written makes it appear as if there's a circular depdendence of the $c_t$ on $z_t$ and then the $z_t$ is again resampled, which changes the $c_t$. For this reason, it maybe better to use a completely unrelated variable for the sample from $q_\phi$ in step 3. ### Key StrengthsThe paper puts together several ideas from prior works (partial rejection control/SMC, variational inference, dice-enterprise) and also evaluates these ideas for latent variable sequential state space model benchmarks. The experimental results compare favorably to prior works like FIVO and IWAE and demonstrate that using partial rejection control is beneficial in a variety of benchmarks. ### Key WeaknessesIt seems like a key contribution in terms of novelty/theory is in fixing the bias in prior works using Partial Rejection Control in SMC (e.g. Peters et. al. 2012). More information to support this claim in terms of why the prior estimate is biased would be helpful in assessing the strengths of the paper's contributions (Peters et. al don't seem to explicitly focus on the exact bias for PRC).  Having said that, the experiments seem to show that unbiased gradients are worse than a biased version (Figure 2, left bottom), so it seems like focusing on the exact bias is not that important.  ### Additional Comments* The proof of unbiasedness (Prop 2) says "it is easy to show that Eq (15) is an unbiased estimator" and refers to prior work by [Naesseth et. al] for details. More clarity here would be helpful (especially around the term $Q_{VSMC-PRC}$) for assessing this claim, given that this is one of the main contributions claimed in the paper (besides also commenting on where the bias in prior work is coming from). * The $Z()$ function first appears in Equation (9), without a prior reference/definition. You might want to introduce this around Eq (4), where the integral appears. There's also a reference to what will later be $Z$ as $p^i_t$ in Eq (8).* Using partial forms of rejection to proposal distribution samples specifically for variational inference (rather than SMC more generally) has also been considered in prior work [R. Gummadi, "Resampled Belief Networks for Variational Inference", Advances in Approximate Bayesian Inference Workshop, 2014].  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	Summary:The submission suggests a new variational bound for sequential latent variable models. Unlike previous work that optimize this bound using ‘standard’ particle filters with unbiased resampling, the new bound is constructed based on a partial rejection control step and uses a dice enterprise for sampling the ancestor variables. Positives:The combination of partial rejection control and dice enterprise for variational inference is new and interesting. Particle filters with partial rejection control have been used before for constructing (biased) bounds based on the marginal likelihood. However, using a dice-enterprise step allows for a new unbiased bound which makes it possible to consider a lower bound on the log-likelihood via variational ideas that can be optimized with standard techniques. Empirical experiments suggest that the method outperforms previous work.Negatives:Does the complexity of the new bound not scale linearly with K (while K=1 for FIVO)? This seems to be not accounted for in the experiments. Choosing larger N=16 also has a better performance in the FIVO paper. Recommendation:I vote for acceptance of the paper. However, I think that the experimental section should be improved.Comments:Variational bounds can also be constructed by targeting a smoothing distribution (Lawson et al, 2019) and particle filters with complexity N^2 based on a marginal Fisher identity have been suggested (POYIADJIS et al, 2011) for parameter estimation that avoid estimator variances scaling quadratically in time. I was wondering if there is a connection between such filters and the method suggested here, particularly for K=N?Can you explain the connection between the variance of the estimator for the normalizing constant obtained from particle filters and the tightness of the variational bound in more details?Are the signal-to-noise gradient issues for large N or K?How do the methods in the experiments compare for a larger number of particles?Is there some useful practical advice on choosing the ratio N/K and gamma? |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The paper considers SMC to construct variational approximations. SMC methodscan be improved with partial rejection control (PRC). Then it is not obviousthat one can obtain unbiased estimators of the normalizing constant, as inplain SMC. The authors consider a way of obtaining unbiased estimators in thatsetting.  Experiments include a linear Gaussian state-space model and recurrentneural networks on polyphonic music datasets.---The problem is interesting. The notation is a bit cumbersome at times, but it is the case for most papers on SMC. The writing is mostly clear. The experimentsinclude a mix of toy and more realistic examples. The fact that SMC with PRC can still produce unbiased estimators of themarginal likelihood was described by Kudlicka, Murray, Schoen, Lindsten,"Particle filter with rejection control and unbiased estimator of the marginallikelihood". The authors do cite that paper, but I did not understand exactlywhy that paper does not completely solve the problem that the authors consider.At first glance it seems that the Kudlicka et al paper would apply "out of thebox" when replacing the prior by an arbitrary proposal in SMC, i.e. using aproposal "$q_t$" instead of "$f_t$" and the ratio "$g_t f_t/q_t$" for the weights.My understanding is that Kudlicka et al focus on the bootstrap particle filterbecause PRC is a remarkably generic approach to improve it, applicable evenwhen sampling from the model prior is the only option; not because the samereasoning would not apply for generic proposals. Thus it was not clear to me that there's a need for another paper showing thatSMC with PRC can still provide unbiased estimators of the marginal likelihood.If the authors made a convincing case that the extension to general proposalswithin SMC with PRC requires significant work, their contribution would be moreconvincing.Furthermore, the manuscript makes references to Bernoulli factories and diceenterprise.  In fact, the manuscript addresses the problem of categoricalsampling with unbiased estimators of the underlying probabilities.  The problemwas addressed in "Bernoulli Race Particle Filters" by Schmon, Doucet,Deligiannidis.  That paper is cited by the authors, but the authors do not makeit clear that their algorithm (in Section 2.2) is exactly the same as"Algorithm 2" of that paper, their Proposition 1 is exactly "equation (12)" ofthat paper, etc.  Furthermore, I don't think their algorithm is indeed a "diceenterprise" as in the terminology of Morina et al; I believe the references tothe Bernoulli factory literature would be sending most readers in the wrongdirection.Based on these flaws I do not think that the manuscript is suitable forpublication. Perhaps a clearer explanation of the specific shortcomings ofKudlicka et al's work would make the paper more convincing.---Small comments:- page 2: "We further assume that the joint density [...]"in fact that decomposition always holds, it is not an assumption.It is just p(a,b) = p(a)p(b|a). - page 2: A SMC sampler -> An SMC sampler- page 2 and later: there is some inconsistency between the notation M(i,t-1) and M(t-1,i).- The latex command \eqref seems to have been used instead of \ref, in various places.- page 6 "utlizing the best of both worlds" |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
BJldVWPTPN	Simplifying Automated Pattern Selection for Planning with Symbolic Pattern Databases	long	The presented paper introduces an extension to existing pattern generation algorithms for PDB heuristics. The work is fairly incremental, mostly turning screws in a large space of existing methods and combining them in new ways. Also, it does not become very clear what the concrete contribution of this paper is. How does it differ from existing methods, such as those presented in "On Creating Complementary Pattern Databases" by Franco et al. (2017)? I guess a separate related work section could help a lot in clarifying the difference to existing methods and the contribution of this work.The way the paper is currently written, by interleaving background onexisting methods (which btw. makes more than half of the paper), withremarks about which ingredients of these methods are used in the presented approach with or without modifications, makes it hard tofollow. More generally, the write-up is not very good and could besignificantly improved. Also, at least in the presentation, the suggested changes are not very systematic, there is no consistent thread. Rather, I have the impression that various existing methods are combined to somehow achieve higher total coverage.Still, the experimental evaluation is not very convincing. When choosing the right benchmark set (IPC'18), the new method slightly outperforms the shown existing methods (including Complementary2). When adding the IPC'11+14 benchmarks, Complementary2 is slightly ahead. What happens when using all IPC benchmarks ('98-18)?Nevertheless, I think the topic of the paper is interesting for theHSDIP audience and the presented ideas might foster interestingdiscussions, since automatic pattern generation for PDB heuristicsseems to be a hot topic these day.If the paper gets accepted, I urge the authors to work on the write-upand introduce a separate related work section.Minor comments:- abstract: "...which*,* if expressed symbolically*,*..."- next sentence: "in the automatic* generation"- please remove the copyright statements from the first page.- Def.1: s_o*and*s_*- Def.1: the sets {\cal A} and {\cal A}^+ are not defined- figure 1 floats into the margin- "Franco et al... shows that ... (Franco etal)." remove one of the cites.- end of paragraph below Def.4: "symbolic symbolic"- "contribute to more that on*ce*"- "the correlation between the cross product [..] is rather weak."I agree that it is weaker than for explicit representations, but still"rather weak" is not a good description.- the cite of FD "[Helmert, 2006]" has a different format than the other cites- the style of writing is sometimes very informal, e.g."Baeckstroem prefers the SAS+ formalism", or "in the most prestigiousand attended [..] track"- for consistency, please put the caption of table 2 below the table- the bibliography does not seem to be in AAAI style- the references Edelkamp 2002a and b refer to the same paper, samefor Holte and Hernadvoelgyi.- there is a typo in the reference to Preditis 1993 *heuristics* |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The paper certainly addresses an interesting research question and I think trying to understand what made the Complementary planners work well in IPC 2018 is a worthwhile endeavor. Unfortunately, I don't think the paper in its current form takes us closer to this goal. The paper presents two algorithms but many details are left out. This makes it impossible to understand how the algorithms work and how they differ from previous work. Furthermore, the experiment section is missing many essential comparisons, making it impossible to judge how the algorithms compare to previous work.Introduction:* Each concrete plan has an abstract counterpart in an induced abstraction of the same length, not "a shorter one".* The meaning of a pattern in the sliding-tile puzzle depends on the representation. It can be a set of tiles or a set of positions.Background:* No need to introduce STRIPS, PDDL and PSVN. It doesn't seem you're using these later in the paper.* Definition 1: there's no definition for \phi, \mathcal{A}, and the concept of plans.* "The set of reachable states is generated on-the-fly" -> it is defined declaratively* You describe "spurious states" in the background section, but never evaluate their impact or describe how you deal with them.* I think "unreachable states" would be a better name than "spurious states" if that is indeed what you mean.Pattern Databases:* The codomain of heuristics should include \infty.* "the sum of heuristic values obtained via projection to a disjoint variable set is admissible"  -> This is only true if no two operators affect the same variable.* "The projection of state variables induces a projection of operators and requires 0/1 cost partitioning"  -> No, any cost partitioning algorithm can be used. Or you can maximize over the estimates instead of summing them.* Definition 5: "maximize the average heuristic [value]"  -> why not the heuristic value of the initial state or a set of samples? I think this needs some motivation.Pattern Selection and Cost Partitioning:* "SCP relies on only using those costs which each heuristic uses to create an abstract plan."  -> This sounds misleading: SCP preserves *all* heuristic estimates under the given cost function.* "[SCP] is time consuming compared to other cost partitioning methods (Pommerening, Helmert, and Bonet 2017)"  -> the reference does not mention SCP. Seipp, Keller, Helmert (ICAPS 2017) state  "[SCP] can be computed at negligible overhead during the construction of the heuristic"Greedy Selection:* The way I understand it, Gamer uses *steepest-ascent* hill climbing and Gamer-style uses *simple* hill climbing. If that's the case, I think using these terms might clarify the presentation.BP-PDB:* "Even though reducing the number of PDBs used to group all possible variables does not guarantee a better PDB, by having a smaller PDB collections, it is less likely to miss interactions between variables due to them being placed on different PDBs."  -> This suggests trying to base the pattern generation on the variable interactions and not on their domain sizes as bin packing does.* "the method will start generating pattern collections stochastically"  -> How?* "We then decide whether to add a pattern collection to the list of selected patterns if it is estimated that adding such PDB will speed up search."  -> How?* "To evaluate the fitness function"  -> Which fitness function?GreedyPDB:* The limits of 50s and 75s for bin packing seem very large, given that most tasks have less than 1000 variables and the greedy bin packing algorithms FFI and FFD should run very fast.* The first sentence of the last paragraph in this section appears twice in the paper.* I guess some variable names are wrong in Algorithm 1?: SelPDBs vs. P_sel and SizeLim vs. Size* Why do you iteratively increase the size limit in the PACKER function? I thought either the variables fit or they don't.* How does Generate_P work?* How exactly does GreedyPDB differ from Complementary 1 and 2?Experiments:* Figure 2 should probably use a histogram or similar instead of a line plot.* Which time and memory limits do you use?* Tables 1 and 2 are superfluous since they are subsumed in Table 3.* I don't think there's an advantage in knowing the coverage score of an oracle planner.* What is "advanced" bin packing?* The paper states that GreedyPDB is a "new state-of-the-art in cost-optimal planning". However, Table 3 shows that Complementary 2 solves more tasks than GreedyPDB in 14 domains, while the opposite is true in only 6 domains.* How do the parameters of GreedyPDB influence the resulting heuristic?* I think the experiments focus too much on comparing whole planners and too little on comparing pattern selection algorithms.* Keeping all other parts of the planner the same, the paper should compare different approaches for selecting patterns. Table 3 goes into the right direction by evaluating the three Greedy PDB variants, but this comparison only includes new pattern selection algorithms. The comparison should also include the following pattern selection algorithms:  * the algorithms used in BP-PDB and Complementary 1 and 2  * the Gamer pattern selection (without it, we cannot judge whether "Gamer-style" has any merit)  * the hill climbing pattern generator (Haslum et al., AAAI 2007)  * the systematic pattern generation methods of patterns up to size 2 and 3 (Pommerening et al., IJCAI 2013), which have been shown to yield very strong heuristics.References:* Some of the references for Stefan Edelkamp are missing a venue.Style:* Horizontal and vertical lines in tables only help if not all cells are fully surrounded by lines. One line below the header should be sufficient.* Your inline citation macro seems to put extra whitespace between the authors and the year.* No need to abbreviate domain names in Table 3, there's enough space.* The paper has numerous typos. I recommend having it proof-read.Reproducibility:* BP-PDB only has a very high-level description and the pseudo-code for GreedyPDB is also missing some crucial details (e.g., EM algorithm). The paper does not promise to make any code or results available. Therefore, it won't be possible to replicate the results or build on them in future work. |||| rating: 4: Ok but not good enough - rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
FDmIo6o09H	Environment Diversification with Multi-head Neural Network for Invariant Learning	long	The paper discusses the problem of shift between train and test data distributions. The authors assume that training data originate from multiple 'environments', each with particular distribution. They propose a new framework (EDNIL) through which they can infer the environments from data (without human labeling) and then train an environment invariant model. The principal assumption is that of the existence of `invariant features` - the distribution of the outputs conditioned on these is the same across all the environments. The loss is a combination of multiple terms motivated through information theoric arguments. The algorithm consists of alternating steps to infer the environments and train the invariant model. Multiple experiments are provided to corroborate the effectiveness of the method. Train/test distribution shift is an important and fully answered problem, particularly relevant for all practical applications where the standard iid assumptions typically do not hold. As such the paper addresses a significant question for the community.I found the paper somewhat difficult to parse and follow. Most importantly, I found the description of the whole setup (shift from train to test distributions or a mix of multiple environments in train and test) rather confusing. But I have more question marks (see Questions) indicating a fairly low clarity of the paper. In the end the solution is an alternating optimization in which through the first step the environments are inferred though a form of supervised clustering (similarity metric is the model accuracy) and then this info is used to train a prediction model. If my interpretation is correct, this does not seem very novel. Societal impact - not addressed, not directly relevant.Limitations - invalidity of graphical model assumption is mentioned. Yes, is reasonable.   |||| rating: 5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	Paper proposes a new method to infer environment labels that can then be used for invariant learning (such as IRM methods). Essentially, it uses a neural network for clustering data into environments with appropriate loss functions. The method is more scalable and less sensitive to initializations than the prior methods. Experiments show significant improvements for distribution-shifted tasks without environment labels. Strengths:1. Paper tackles an important challenge of robust learning with no environment labels and solves the problems with existing works.2. Experiments in varied tasks, including images and text, show significant performance improvements. Experiments also help answer questions about the effect of initializations, effect of number of environments.Weaknesses:1. Clarity can be improved a lot; especially Section 2.2 and Section 3 (Methodology). As a result, the paper’s contributions are muddled with existing work (for example, which of the learning objectives and loss terms are novel?). 2. Paper can be improved a lot with better justifications for the choices made in the paper. Currently the loss functions seem a little arbitrary; for example, why are Equations (5) and (9) the best ways to achieve their respective goals. 3. There are no theoretical guarantees provided by the prior works, but this can be future work.  Yes  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	The authors propose a novel environment-free invariant learning method that uses an auxiliary network to learn environment-specific features, from which environment inferences can be derived. These inferred environments labels are then used to train and invariant model.UPDATE 2022-aug-05: The paper's clarity has been improved so I am increasing my score by one. I like the idea of the paper, and was impressed by the experimental results showing that the proposed method EDNIL realizes good worst-group performance in a variety of settings, while addressing the sensitivity to initialization that we see in EIIL. In my opinion, the experimental results are sufficient but the presentation should be improved (see below for specific suggestions).Strengths* I like the experimental results. The “effortless initialization” result (Fig 7), showing that the proposed method is not sensitive to initial ERM features, which is a known issue with EIIL, is especially impressive.* The paper does a good job of describing related approaches to environment-free invariant learning, and comparing against these methods empirically.Weaknesses* The paper can be hard to read at times. Some parts would benefit from additional exposition. For example the idea of “diversity” of inferred environments is mentioned in passing with a citation, but it is not fully explained what type of diversity is needed, and how it can be measured formally? The cited paper (https://arxiv.org/abs/2004.05007) mentions that the correlation pattern between Y and spurious features should vary across environments…is the outcome we hope for when introducing L_ED? No [line 430]. I suggest that the authors add a discussion of possible societal impacts to a future revision. Given the connections of invariant learning to fairness (https://arxiv.org/abs/2010.07249) and the use of a fairness benchmark in the experiments, there should be plenty to discuss.  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.	In the present paper, the authors tackle the challenging problem of learning a supervised ML model capable of identifying and adapting to different environments hidden in the training data. The method, EDNIL, is composed of two jointly learned models, that take care of the environment identification, the learning of the invariant representations and the label predictions, produced by a multi-headed neural network. The proposed model is compared to different alternative models from the literature of the field, in different challenging benchmarks, and the results show that it closely achieves the best possible invariant learning performance. AFTER REBUTTALThe authors have addressed the main concerns raised during the review. I confirm my assessment of this paper as solid and deserving of acceptance in the conference. The authors introduce a very flexible and effective learning framework, incorporating the main strengths of the previously introduced models and surpassing many of the limitations thereof. The novel method is shown to perform extremely well when compared to its competitors, achieving close to the performance of IRM with access to the oracle environments. I think the main weakness of this paper is in the density of the presentation and in the abundance of acronyms that make the read quite challenging, especially for a non-expert. I think the main limitations of the paper are well acknowledged by the authors.  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
LqRSh6V0vR	Reinforcement Learning Benchmarks for Traffic Signal Control	long	This contribution consists of an OpenAI Gym style wrapper around the SUMO traffic simulation package, a selection of traffic scenarios that can be run using that package, as well as a set of (RL) algorithms that can use this interface.The authors ran all of the provided algorithms against all provided environments, and report performance figures for those runs, and make the code for doing so available. This system seems potentially very useful for making Traffic Signal Control easier to experiment with in an RL setting, since the OpenAI Gym API is ubiquitous, and this system handles featurization of the action and observation spaces in a convenient way.It is also helpful that there are selected traffic scenarios covering a range of relevant realistic conditions that any algorithm attempting to solve this problem should handle.The paper also provides a reasonable introduction to the topic, as well as a description of how Traffic Signal Control is modeled as an MDP. The primary weakness of this paper is that its main contribution, the benchmark, is provided under a no-derivatives license.This makes it illegal for other researchers to build on this work by modifying the provided code, which drastically decreases the value of the contribution.I recognize that the individual traffic scenarios are licensed under restrictive licenses (for understandable reasons), but there's no clear reason why this restriction is present for the new contributions made in this paper.The packaging and documentation would also both benefit from additional work.The existing documentation consists of this paper, as well as a small readme describing some of the installation requirements and how to run the code, with the expectation that the user works out of the code directory provided.Working out of another author's code directory is not convenient compared to being able to install the library through python's standard packaging system.The vast majority of frequently used RL benchmarks provide the ability to install them, and it is not difficult to add support for doing so.It would also be preferable if the different algorithms presented used a single neural network library, instead of requiring multiple ones with unclear version requirements to be installed. In particular, dependencies on versions of tensorflow that lack officially supported builds are highly inconvenient.The paper has a few minor formatting issues (mostly unresolved references), and spends too long criticizing CityFlow, but this is not consequential.  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	This paper introduces a benchmark for RL-based control of traffic lights. The primary advantages touted over existing work include: use of the SUMO simulator; traffic scenarios based on real-world road layouts and traffic levels; a standard Gym interface; and a set of reference algorithms for the proposed environments. Experiments show that independent PPO/DQN tend to do quite well at the end of training, but take far longer to converge than specialised algorithms like MPLight. [S1] Paper makes a good case for the importance of studying the traffic control problem and having accurate simulators. Arguments in favour of this new simulator are clearly laid out (although note first weakness below).[S2] On execution: Writing was generally clear, choice of experiments seemed reasonable. [W1] It is unclear to me how much value this new benchmark provides for the community relative to the various existing benchmarks. The strongest argument seems to be in Section 2.3.1, which claims that existing benchmarks are less realistic because they either use simplified or arbitrarily over-complicated versions of real-world traffic grids. I do not know how much this matters for evaluation, though—it may be that the simplifications in question don't affect relative rankings of different methods at all, and the experiments in this paper do not try to evaluate how rankings change when moving from, e.g., the Jinming & Feng simulator to RESCO.[W2] There are a few important experimental details missing from the paper.  |||| rating: 7: Good paper, accept |||| confidence: 2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper	This paper proposes 1) benchmarking signal control tasks based on well-established traffic scenarios; 2) implementation of various RL algorithms on these signal control problems; 3) comparision and analysis of these RL algorithms under varying sensing assumptions. The benchmarking proposed in this paper could be very helpful for future study on using RL for traffic signal control. - A standarized benchmarking on RL for traffic signal could serve as the foundation to help better future algorithm design for such problems, which could bring lots of benefits for real-world traffic control.- The paper is well written, and the benchmarking tasks are carefully chosen to match the real-world scenarios;- The experiments are well-conducted. A comprehensive set of baseline controllers and RL controllers are evaluated. The implementation are also validated by check against previous work, and detailed analysis are given for comparing the performance of the benchmarked algorithms. - It would be interesting to see how the RL algorithms would work under weaker sensing abilities;- Can the author give any discussion on how much sensing information mentioned in the paper can be accurately measured in the real-world traffic control scenarios. i.e., how realistic are the sensing assumptions are?  |||| rating: 7: Good paper, accept |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	- Proposes benchmarks to study the problem of congestion control using realistic traffic situations - Presents baselines and  thorough comparative evaluations  - Current progress in congestion control algorithms are evaluated using different settings avoiding fair comparisons between them. This is a well motivated paper that positions itself as an effort to consolidate the diverging field - Presents a set of baselines and evaluations to outline a fair comparison between various state of art algorithms.- Baselines are validated before they are used for comparisons. - line 72-32: Be precise and revise the premise and claim about model-based methods and deep q learning algorithms - Provide more explanation wrt yellow light -- (a) is the duration of yellow light subsumed inside a phase? (b) How is wait time etc. affected by the yellow light (c) is yellow light mandatory after every phase transitions? (d) How are "free right turn on red" etc situations handelled?- Line 166: Missing reference and typo - While I'm convinced, please justify why allowing algorithms the choice of state and rewards is a good choice for fair comparison give they are being evaluated on two different MDPs- Sec 4.1 - the pronounced trend for FMA2C and IDQN isn't well justified. This is too large of a gap too gloss over   |||| rating: 8: Top 50% of accepted papers, clear accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
vY0bnzBBvtr	Provably More Efficient Q-Learning in the One-Sided-Feedback/Full-Feedback Settings	long	##################################################################################Summary:This paper is motivated by using reinforcement learning (RL) methods in inventory control. In particular, it customizes Q-learning for a special one-sided feedback/full-feedback setting. This combination of two areas is intriguing. The main contribution of the paper is a new algorithm leveraging the model structure, so that the regret no longer depends on the size of state and action space. ##################################################################################Reasons for score:  I like the idea of introducing RL methods to classical operations research problems. The application of Q-learning in inventory control seems promising. My major concern is about the clarity of the paper and the novelty of theoretical analysis (See cons below). I think the paper is not suitable for ICLR in this current version.##################################################################################Strengths:- The one-sided feedback/full-feedback model is of high practical value. It is an abstraction and generalization of inventory control and applies to many other famous problems.- The proposed HQL and FQL algorithms are novel and well-motivated by the one-sided-feedback/full-feedback assumption.- The theoretical and empirical comparison with existing Q-leaning methods shows the benefits of utilizing model structures.##################################################################################Weakness:- The theoretical analysis is similar to Jin et al. (2018). It is not evident to me what new challenges are in the proof. Besides, the authors use Azuma-Hoeffding's inequality instead of Bernstein one so that the dependence on H is very loose.- In numerical experiments, the results would be more convincing if the authors could take a longer length of episode (currently H = 1, 3 or 5) and compare with traditional inventory control algorithms in the same setting.  - The writing needs significant improvement.* The model assumptions should be highlighted. The one-sided-feedback/full-feedback models are unconventional. It would help readers to better comprehend problem set-up if subsections 2.1 & 2.2 are written in a more organized way.* Section 4: Main Results lacks necessary discussion.* Section 5: Overview of Proof is lengthy and there is neither an evident clue nor an outline. Some technical results are more suitable to appear in appendix.* In the reviewer's humble opinion, Section 6: Example Applications could be placed before model description, so that the motivations are clearer at first glimpse.##################################################################################Typo: * Algorithm 1, Q-function update step, the subscript of value function $V$ |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper proposes two algorithms, Half-Q-Learning and Full-Q-Learning for the classic inventory control problem. It establishes cardinality-independent regret bonds for the two algorithms in terms of length of episode and horizon. Numerical results are presented to demonstrate the effectiveness of the algorithms. Plus- The paper is well written and the ideas are explained clearly. The proof flow is also explained clearly. - The inventory control problem in consideration is important. Exploiting the feedback structure in this problem is interesting. Minus- The regret performance w.r.t. H and running time are worse than prior works. - The experimental section could be improved, e.g., compare the algorithms with larger H values and running times. Detailed comments - The lowercase letter “k” above Eq. (1) should be “K”. - The reviewer would suggest the authors to place the inventory control problem before the formulation. This way, it is easier to see what assumptions are reasonable and satisfied by the target application. - It will be interesting if the authors could discuss why FQL and HQL are able to eliminate the SA factor from the regret but results in a larger regret w.r.t. H. Intuitively, the former is due to the feedback structure and a larger computational cost. But the latter is less clear and it would be nice to provide some intuition. - The experimental setting appears to be quite simplistic. The reviewer would suggest experimenting more scenarios to evaluation the performance. - The experiments use a fairly small H. The reviewer wonders what happens if the H value is larger? From the comparison results with Aggregate QL and QL-UCB in Table 1, it seems that FQL and HQL should perform worse than QL-UCB. It would also be interesting to compare the running time of the algorithms.  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	This paper proposes Q-learning based algorithms called Elimination-Based Half-Q-Learning (HQL) and Full-Q-Learning (FQL). In the one-sided-feedback setting, the proposed algorithm improves the regret bounds over existing methods in terms of the dependency on the size of state-action space. Numerical experiments are provided to show the performance of the algorithm.Overall, I vote for rejecting. The detailed comments are as follows:Pros:- By incorporating domain-specific structures into Q-learning algorithms, the author developed new algorithms tailored to one-sided-feedback/full-feedback models. The algorithm improves the regret bound in terms of the dependency on the state-action space.Cons:- Although the algorithm improves the regret bounds with respect to the state-action space, the time complexity grows linearly with S and A (in Table 1). Therefore, I'm skeptical of the claim that "the algorithms are barely hampered by even infinitely large state-action sets".- The exposition in Section 4 could be improved. In the current version, there are only statements of two theorems. I think the authors should spend more space in Section 4 instead of Section 5. It would be helpful if authors can provide interpretation of the theorems and detailed comparison with existing results. And it would be nicer to provide examples that satisfy the assumptions made in Section 2.- I felt that the numerical experiment is not so convincing. For example, the episode length seems to be too small (H <= 5). In addition, since authors claim that the algorithms scale well to large state-action sets, it would be better to conduct the numerical experiment in that regime to show the efficiency of the algorithm.- The writing quality could be improved. There are several grammar mistakes and typos. |||| rating: 4: Ok but not good enough - rejection |||| confidence: 2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper	After rebuttal:My main concerns are addressed, and I changed my score to 5 accordingly.------Motivated by OR problems, this paper extends Q-learning algorithm to one-sided-feedback and full-feedback settings. With additional assumptions, this paper proves a $\sqrt{T}$-regret bound with no dependence on the size of state/action spaces. Both proposed algorithms are theoretically and empirically shown to be more efficient than general Q-learning algorithms.The novelty of this paper lies in applying reinforcement learning algorithms to the inventory control problem. It is interesting to see how can Q-learning algorithm be customized to problems with more structure. Technically, it is also novel to use elimination based algorithms in tabular reinforcement learning. However, the assumptions of this paper is a bit too strong, which is the major weakness of this paper. For example, it is assumed that the next state $x_{h+1}(\cdot)$ is increasing in $y_h$, and the feasible action set is also monotonic, which means that the state and action spaces must be 1-dimensional. It is also assumed that both the transition and the reward depend only on action and environment randomness. But in the example (e.g. Backlogged model), the reward is a function of $x_h,y_h$. As a result, I'm not totally convinced that the assumptions in this paper is general and realistic.After spending considerable amount of time, I still have some concerns about the technical soundness. To be more specific:1. In Line 13 of Alg. 1, how is trajectory simulated? To be more specific, how does the algorithm choose action $y$ for step $h+1,\cdots,\tau_{h}^{k}(x,y)$ and how is the next state generated?2. Proof of Lemma 7 (page 3 of appendix): in the inequality before Eq. (12), how is the term $\tilde{r}^i-\tilde{r}^*$ bounded? If I understand correctly, $\tilde{r}^i$ is the cumulative reward of the trajectory generated by the algorithm, and $\tilde{r}^*$ is the reward generated by the optimal policy. When the trajectory is longer than 1 time step, the actions of the two trajectories on step $h+1$ may be different when the algorithm has not converged to optimal.Additional comments:1. Since the state space is infinite and continuous, I'm wondering how function approximation RL algorithms behave in this setting.2. It is mentioned in Sec. 6 that the state space $x_h\in \mathbb{R}$ is continuous, but how to execute the HQL algorithm for continuous state space? Although the regret has no dependence on the size of state and action space, the time and space complexity do.3. In the bandit setting, the elimination algorithm has instance dependent regret (i.e., sum_i (1/Delta_i) log(T)). I'm wondering whether HQL algorithm has similar guarantee?4. Why is lost-sales model one-sided? What can the RL agent observe in this setting? If the agent can observe $D_h$, isn't it the case where the agent can compute the reward function for all possible $y$? If the agent can only observe min(y, D), how can the agent infer min(y-1,D)?In summary, my main concerns are about the assumptions and technical soundness. Therefore, I would not recommend acceptance for the paper at this point. |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
k3462dQtQhg	A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks	long	Backdoor attack is becoming increasingly relevant in ML applications. This paper introduces a benchmark for backdoor attacks and defenses in the context of NLP models. To foster research progress in this area, the authors unify different aspects of backdoor attacks under a common framework by introducing evaluation protocols and metrics for improving the way defense and attack algorithms are benchmarked. The OpenBackdoor framework can be used by future attackers or defenders coming up with new algorithms. - Attack scenarios considered in this paper make benchmarking move towards practical evaluation.- Several defense techniques are evaluated and compared based on their training/inference categorization. - Documentation is missing at this link https://openbackdoor.readthedocs.io/en/latest/- There is no discussion on the config file structure and specification in the github directory- Usage of Universal Sentence Encoder to distinguish between semantic shifts and backdoor triggers may not be a good strategy if the sentences change significantly while being semantically similar. - Shouldn't stealthiness be a part of effectiveness since rejecting poisoned samples can be a part of the defense strategy?  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	This manuscript proposes a framework for evaluating backdoor learning in textual models. The authors provide a critique of evaluation in prior work and present "OpenBackdoor" which comprises implementation of several attacks and tasks.  The manuscript addresses an important practical issue in the security of deep learning systems, focusing on pre-trained language models. The manuscript makes a good observation on the lack of standard evaluation approaches for different backdooring attacks and defences for deep learning of text. The framework appears to bring together a collection of attacks and defences from prior work in an easy-to-access repository.  - In section 2.1.2, three "attack scenarios" are presented, classified along the axes of how much data is provided by an adversary (Scenario 1: the dataset is poisoned, Scenario 2: the backdoored model is released with the expectation that it is fine-tuned by the end user, Scenario 3: the backdoored model is released as-is for direct use). Are these scenarios the only meaningful attack scenarios in this domain? Consider the scenario where the defender wants to fine-tune a "clean" model with poisoned data. Is this encompassed under Scenario 1? For instance, it could be the case that there is a malicious contributor of data. Perhaps it would be more useful to specify the various dimensions from which we could draw any combination to specify the threat model. For example, the ability of the attacker to train/retrain, the ability of the defender to train/retrain, the ability of the attacker to poison the initial training data, the ability of the attacker to poison any fine-tuning data, the ability of the defender to acquire clean data, etc...a scenario could be constructed by any collection of these elements (so there could possibly be more than just three scenarios). It does seem that the authors are maybe thinking on these lines also, given that the paper states that "one attack model is not limited to a single scenario...attack models which release datasets can also be used for training and releasing poisoned models." Then, it could be clarified and discussed why the three proposed scenarios are the only ones that we should be concerned with (if this is in fact the case). In scenario II it's mentioned that "attackers can only use plain text datasets" -- what is "plain"? Why is this restriction required?- It is mentioned that there are only two kinds of defence methods (detection or correction), focusing only on the data. Prior work such as fine-pruning [a] or noise-augmented retraining [b] has shown some success in modifying the backdoored model directly (by changing weights). Are these approaches applicable as defences for text models? Perhaps this should be discussed. [a] Liu, K., Dolan-Gavitt, B., Garg, S. (2018). Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks. In: Bailey, M., Holz, T., Stamatogiannakis, M., Ioannidis, S. (eds) Research in Attacks, Intrusions, and Defenses. RAID 2018. Lecture Notes in Computer Science(), vol 11050. Springer, Cham. https://doi.org/10.1007/978-3-030-00470-5_13[b] Akshaj Kumar Veldanda, Kang Liu, Benjamin Tan, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg. 2021. NNoculation: Catching BadNets in the Wild. In Proceedings of the 14th ACM Workshop on Artificial Intelligence and Security (AISec '21). Association for Computing Machinery, New York, NY, USA, 49–60. https://doi.org/10.1145/3474369.3486874- For completeness of this paper, what is the average perplexity increase rate and the grammar error increase rate? How are these calculated and why are they the best metrics for measuring stealthiness? Do these metrics serve as good proxies for capturing human perceptibility? What would one expect the baseline values for these metrics to be in typical (clean) datasets? At this point, it's a little bit challenging for me to appreciate how much a trigger would need to affect the input to appear suspicious -- can concrete examples of this be presented and discussed? - In 3.2.1, the "attack evaluation protocols". What is a protocol in this context? Some of the terminology could perhaps be tightened up. - As a benchmarking paper, it would be useful to gain some insight into why the chosen dataset/evaluation approach is suitable as a benchmark. Why were the various datasets chosen? Are they representative of the full range of applications that a pre-trained language model would be used for? Is text classification the only class of problems for which backdooring is a potential threat?- Table 3 has results for 4 models, but the models were not clearly introduced or discussed, so it's a bit hard to understand the significance of these results. In the generation of these results, how many times were the choice of sample to poison varied? Why are the train/dev/test splits fixed in the state proportions? Could/should these dataset splits be varied as part of a comprehensive evaluation of attack/defence success? - What are the different types of triggers? In the discussion of section 5.1, it's not clear what a "sentence trigger" is; what is it compared to for the finding that it is "most effective"? Perhaps some background or reference information is missing. - In the experiments of Scenario III, why were only the SST and IMDB datasets used, when so many other datasets appear to be available?- In response to the proposal for CUBE, could an attacker who controls the training process set up the training in such a way as to penalize the loss if the poisoned data is easily separable after dimensionality reduction?   |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	This paper argues the existing textual backdoor research (attack and defense) suffers from two deficiencies of ambiguous settings for different scenarios and incomplete evaluation metrics. To address the issues, the paper makes the following contributions:- Categorizing the existing works into three scenarios and discussing their unique evaluation methodologies.- Proposing two supplemental metrics including stealthiness and validity to evaluate textual backdoor learning.- Proposing a simple training-time defense baseline.- Developing a toolkit OpenBackdoor and providing comprehensive  benchmark experiments. 1. Besides benchmarks, the author makes some additional yet simple contributions.2. The paper focuses on a potentially important research track textual backdoor learning, which may be relevant to broad deep learning researchers in the future.3. Good accessibility of datasets, code, and benchmarks. 1. Unclear review of related works. The authors refer to related works in their organizational structure, which is hard to know the overview of related works. It would be better to provide a review of related works following the original research line or timeline in the main paper or appendix.2. The methods for evaluating stealthiness and validity are model-dependent, which is uneasy to be acknowledged widely, as the model for evaluating stealthiness and validity is possibly attacked.  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	This paper 1) categorizes existing NLP backdoor learning work into three practical scenarios and 2) provides evaluation metrics not only considering ASR/CACC, but also stealthy and semantic-preserving of poisoned text. They also open-sourced the toolkit openbackdoor for implementations. 1. In backdoor learning, especially NLP domain, there is limited framework and benchmark. This work provide an useful and practical NLP backdoor learning benchmark. It would contribute a lot to the backdoor community, especially in NLP.2. Open-sourced toolkit, with solid evaluation and results posted. 3. The benchmark experiments are clear and the parameters are provided. 1. A recent NLP backdoor detection work called 'T-Miner' is missing. If possible, adding it would be better. But it will not influnce the fact that this work is already a good benchmark work.2. Other than the codes, are there well-trained NLP backdoor models online available(Trained with paper's codes)?  |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
h1IHI5sV4UQ	Reconstruction on Trees and Low-Degree Polynomials	long	This paper studies the effectiveness of using the restricted computational model class of low-degree polynomials for assessing statistical-computation gaps for high-dimensional statistical inference problems.  Using the problem of reconstruction on trees, the authors identify problem settings for which average-case reconstruction is impossible using low-degree polynomials yet reconstruction is possible with computationally efficient methods.   Strengths (major)- significance: Information-computation tradeoffs for high dimensional statistical inference problems are important.  There are several restricted computational models that have been used to study gaps for different problems (planted clique, sparse PCA, etc.).  Low-degree polynomials are a prominent model.  The authors show that there are problems where thresholds identified using low-degree polynomials do not match thresholds for existence of computationally efficient methods.  - The extension of the lower bound to investigate when kernel ridge regression would fail is interesting.- The authors provide substantial discussions.Strengths (minor)- While work is theoretical, the authors include an experiment (Fig 1) investigate performance of low-degree polynomials for small but non-zero $\lambda_2$, suggesting that the inaccuracy of low-degree polynomials to capture computational hardness may not narrowly occur on the limit case (of $\lambda_2=0$) analytically studied in the paper.Weaknesses - I did not identify any major weaknesses.Very minor notes: - line 30, acronym CSP not defined- line 78 – the acronym for statistical query ‘SQ’ is used multiple times before being defined - Line 95 - Notation $c$ not introduced yet, why not $x_r$ for realization of root variable?- Line 103 – $\rho$, a subscript appearing in the conditioning event, is not defined. - Lines 117 and 122 missing parenthesis- line 189 – a constant $c$ for $N^c$ is discussed, but that is different than the $c$ as the root variable realization, right? If so, I’d suggest not overloading notation.- line 210 capitalize ‘markov’- Line 212 should ‘Suppose that …’ be there?  Line 218 has a condition on $m/\delta$ and in 212 the notation $m$, $\delta$, $\epsilon$, and $c$ not yet specified- Line 217-218, I don’t think the notation $\varphi(\cdot)$ was introduced yet yes  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	Investigates the problem of tree reconstruction (from leaves to root) through low degree polynomials. Low-degree polynomials as a computational model used to study (in)tractability of learning/inference problems seems to be a useful and important model, so studying when it does/doesn't reflect the wider class of polynomial-time algorithms is very important. The results in this paper provide an important insight in this direction. The assumptions are pretty carefully justified, and the authors are largely working in standard models. The main limitation I can see is that the tree structure that makes this analysis go through is somewhat limited.  |||| rating: 8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	This paper studies the problem of reconstruction on trees through low degree polynomials. The authors show that there exists simple tree models in which nontrivial reconstruction of the root value is possible in polynomial time, and if the tree is unknown but given samples with correlated root assignments, nontrivial reconstruction is possible with a statistical query algorithm. The paper also provide a result related to RBF kernel ridge regression for predicting root coloration. An open question about low degree polynomials and the KS threshold is also proposed. The topic of this paper is completely out of my area, and any technical comments I make will probably be unfair to the authors.Regarding organization: I can hardly follow the paper. Partly it is because the topic is out of my area. However in the current shape, everything including introduction, preliminaries, definitions, theorems, remarks, are all mixed into the two massive sections. While I understand there might be a lot of contents in the paper, I think the presentation can definitely be improved. I also don't know what are the exact contributions in this paper. For example Theorem 5 (Mossel and Peres 2003) appears under Section 1.2 Our Results. Is it a new result, or from a prior work? Not applicable.    |||| rating: 5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly. |||| confidence: 1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.	This paper studies the problem of tree reconstruction on $d$-ary trees; the root of the tree is given a spin $X_\rho \sim \nu$, which is then propagated down to the leaves according to a Markov channel $M$. The problem is then, given the spins $X_L$ at the leaves, to recover the original root spin $X_\rho$. Several variants of this model are considered: with/without noise at the leaves, with the underlying tree known or not, or with several realizations of the same tree process.The focus in this paper is on low-degree polynomial reconstruction: for which values of $M$ can $X_\rho$ be estimated by a low-degree polynomial in the leaves, i.e. a function of the form$$ f(X_L) = \sum_{S\subset L, |S| = D} f_S(X_S) , $$where $X_S$ is the subset of leaves in $S$. It is already known that when $d |\lambda_2(M)|^2 > 1$, a linear ($D = 1$) estimator suffices; on the other hand, general reconstruction using belief propagation is possible for almost all $M$.The authors show that if $\lambda_2(M) = 0$, then no polynomial algorithm of degree $\leq N^c$, where $N$ is the number of leaves, can recover the true root spin. The proof is based on the property that $M^k$ is of rank 1 for some k, and hence the correlation between a vertex $x$ and its $k$-th ancestor is 0. As a corollary, they show that a kernel ridge regression method needs at least $e^{N^c}$ samples to learn the tree reconstruction problem, since it needs to approximate a polynomial of degree at least $N^c$.The article also contains a positive result: for the case where the underlying tree is unknown (equivalently, where the leaves are known up to permutation), they show that for a fixed root spin $X_\rho$ and a polynomial number of samples from the tree process started at $X_\rho$, there exists a reconstruction algorithm that recovers $X_\rho$ better than random chance. The tree reconstruction problem is ubiquitous in many inference problems (e.g. community detection), for which computational-to-statistical gaps are still fairly unexplained. It's interesting to see a low degree polynomial approach to this problem, which bridges the gap between the census reconstruction problem and BP approaches. The paper is overall well-written and easy to read; the introduced notions are clearly defined (with the notable exception of the VSTAT oracle), and the results are nicely presented. It is especially interesting that the impossibility result extends to $O(N^c)$ degrees, although this might just be a consequence of $\lambda_2(M) = 0$.The main weakness of this paper, in my opinion, is its specificity: all proofs hinge on the specific properties that occur when $M^k$ is of rank one, which implies very string independence properties between tree nodes. The tree structure is similarly rigid, with only the $d$-ary tree considered. However, this is a good first step which I hope will inspire more work on this topic.  The limitations have been adequately addressed.  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
CPfjKI8Yzx	Robust Imitation via Decision-Time Planning	long	IMITATION LEARNING REVIEW============================The gist of the paper is to improve IRL with the so-called IMPLANT algorithm which is supposed to improve control policies in learning controllers.PROS====It would be nice to have a framework for minimizing model estimation errors in imitation learning. This paper presents a framework for mitigating the learned dynamics during imitation learning using an augmented cost fuction at test time. The authors called this IMPLANT> CONS====I do not see how this algorithm is an MPC framework: where is the action/control law computed in lines 7-16 of Algorithm 1?What did the authors mean in line 11 when they wrote, "First action a_0^{(i)} ~ \pi_\theta ... ". * How was \pi in line 11 computed?* Is there an upper bound on the length of the horizon H?* If there is, how does the policy perform for slowly-varying trajectories at lower horizons versus fast-changing trajectories at longer planning horizons?* I am concerned that the reward function in equation (4) is just a restatement of the optimality principle and do not see why this is being rebranded as a new algorithm.* Of course, the rollout policy would guarantee better performance. What is the predicate for choosing a *random rollout policy* as the authors mentioned on line 4? What is the point of using a mixture?* Since you mentioned that the rollouts can be parallel-wise executed, did you give any thoughts to the fact that the control law has to be tightly scheduled in the feedback before the next iteration of the MPC is run?Grammar errors ==============There were a few sentence structures in the paper that could use some revision e.g. "A" popular class of approaches"-->approach |||| rating: 4: Ok but not good enough - rejection |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature	The authors propose a method to enhance imitation learning by using MPC on the reward function learned by an imitation learning approach. The chosen MPC approach uses the learned policy to generate candidates for a search process that maximizes the reward. To learn the reward function, the authors propose to use GAIL. Besides showing improved performance in a typical benchmark scenario, the authors compare their method with GAIL in various scenarios where some form of transfer is required. The writing is generally clear and easy to understand.Historically, IRL methods relied on given or learned transition models as they relied on solving for a policy in an inner loop. Recently, we have seen a handful of methods introduced which do not have that short-coming and with it, we have seen a shift towards model-free methods. It is generally understood in IRL that the learned reward function can be used with any reinforcement learning or with any planning algorithm such as MPC but while some model-based approaches have been introduced, the merits of model-based imitation learning have not been thoroughly compared to recent model-free methods to my knowledge. While this makes the paper relevant, the use of a single existing MPC approach with a learned reward function is a relatively minor contribution. My main issue, however, lies in the evaluation as the dynamics model is generally just given to the agent. While the authors acknowledge that the dynamics model could be learned, that is not the case in any of the experiments and it is of little surprise that a MPC approach with perfect knowledge of the transition dynamics is able to outperform reinforcement learning approaches that have to learn from interaction. Furthermore, the authors claim their approach to be zero-shot in the transfer-experiments in comparison to AIRL. This claim is hardly defensible when IMPLANT is handed perfect knowledge of the new dynamics while AIRL has no knowledge of the new dynamics.Another major issue with this work stems from an apparent misunderstanding of the role of the discriminator in GAIL. While GAIL treats the discriminator as a reward function in an inner loop, it is not a real IRL method. As the policy converges to the expert’s policy, the learned discriminator can no longer tell the data apart and converges to a constant. On a practical level, the proposed MPC approach uses the learned policy to generate proposals and therefore corresponds more to an iterative step on this policy. This explains why the agent is able to achieve higher scores on benchmark tasks; however, the discriminator by itself is not generally useable as a reward function like the authors suggest. This is underlined by the results that “GAIL - reward only” fails to learn. Instead, the authors should use true IRL methods as the basis for their approach.Finally, the proposed evaluation is limited as the authors choose 3 of the easiest tasks from the mujoco benchmark. The lack of results on humanoid as the hardest task is problematic. |||| rating: 4: Ok but not good enough - rejection |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature	#######################################################################Summary: The paper considers imitation learning problem in the presence of small perturbations and nuisances at test-time. In particular, it proposes Imitation with Planning at Test-time (IMPLANT), a new algorithm for imitation learning that incorporates decision-time planning within an inverse reinforcement learning algorithm. To counteract the imperfection due to policy optimization in RL step, IMPLANT uses the reward function estimated in the IRL step for decision-time planning. The effectiveness of the proposed method has been empirically evaluated on two kinds of setups, i.e., the default ‘no-transfer’ setting and the ‘transfer’ setting where the test dynamics is a perturbated version of the training dynamics. #######################################################################Reasons for score:  Overall, I vote for a weak acceptance. I like the idea of using test-time planning for tackling the test-time perturbation in imitation learning. My major concern is about the clarity of the paper and some additional environments (see cons below). Hopefully the authors can address my concern in the rebuttal period.  #######################################################################Pros:  1. The paper takes one practical issue of imitation learning: test-time perturbation.   2. For me, the proposed IMPLANT method is novel for the issue of test-time perturbation in imitation learning. Specifically, IMPLANT builds on top of GAIL, and learns an additional value estimation during training, and utilizes the estimated reward function, value function for closed-loop planning based on model-predictive control (MPC). The design is interesting. 3. This paper provides comprehensive experiments, including both qualitative analysis and quantitative results, to show the effectiveness of the proposed framework. In particular, IMPLANT outperforms the BC and GAIL-based baselines in the ‘non-transfer’ setting of imitation with limited expert trajectories, and in the transfer setting with causal confusion, motor noise and transition noise. #######################################################################Cons:   1. For the motivation, it would be better to provide more details about it, which seems not very clear to me. Particularly, it is unclear why the planning is introduced, and why such design can address the issue studied in the paper. Additionally, since this paper claims any IRL methods can be used in the training time, I suggest the authors to showcase and discuss the performance with other IRL methods than GAIL at training time. 2. For the zero-shot transition setting in section 4.3, IMPLANT is only robust to the noise with small sigma. It would be better to explain why the IMPLANT can achieve this level of robustness and which issue limits the performance in the small perturbation. This might help the authors to further improve the robustness of IMPLANT.  3. Although the proposed method has been evaluated in diverse settings, the environments are limited to three continuous control tasks. It would be more convincing if the authors can provide more cases with both continuous and discrete action space in the rebuttal period.  #######################################################################Questions during rebuttal period:  Please address and clarify the cons above.   |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	__Summary__The paper proposes to combine imitation learning (GAIL) with model predictive control to improve on the policy, especially when the system dynamics are noisier at test time.Here, MPC uses the reward function and value function learned by GAIL and assumes access to a model of the test-dynamics.The proposed approach, IMPLANT, is compared to GAIL and behavioral cloning on MuJoCo experiments in a standard imitation learning setting and for three slightly modified test scenarios that add either action noise, transition noise, or causal confusing which adds the last actions to the state space during training and replaces this information by noise at test time.__Strong points / weak points__+ (+) The paper is well-written and very easy to follow+ (+) Using MPC on a reward function that was learned by IRL can be reasonable- (-) Lack of contribution / novelty- (-) Using the GAIL reward doesn't seem sound. An actual IRL method should be used- (-) additional assumptions compared to competitors (access to noisy model, computational tractability of online planning) reduce practicability and are not regarded during the comparisons__Recommendation__I recommend rejecting the submission because I can't find a noteworthy contribution.__Supporting Arguments__- *Lack of contribution:* I don't want to be gatekeeping, but I really don't see any noteworthy contribution. The standard approach for apprenticeship learning is to learn a reward function by IRL and then optimize this reward function to learn a policy. There is no requirement to use the same algorithm to optimize the reward that used for inferring it. It is not surprising that reoptimizing the reward performs better in face of dynamic perturbations compared to a direct policy transfer; after all, this is one of the main motivations for learning a reward function. It is also not surprising that a model-based MPC approach based on the policy can perform better than directly using the parametric policy, even when there are no changes in the dynamics. So, I actually don't even see the research question here. There can be a small contribution due to the empirical evaluation, however, the chosen baselines perform direct policy transfer and, thus, do not seem that interesting.- *Using GAIL for IRL:* The paper repeatably refers to GAIL as an IRL approach. However, GAIL does not infer a reward function in accordance with the problem formulation of IRL. The "reward function" used by GAIL is only valid for small policy updates of the current policy (generator). *Maximizing* this reward function will in general not result in any reasonable policy. For optimal policy and discriminator, the reward function would even be constant. So using this reward function for MPC doesn't seem sound. The evaluation did show that optimizing the reward function via MPC can be beneficial when using a small enough horizon and when sampling from the parametric policy, but this is not that surprising since the MPC control will in that case tend to remain close to the parametric policy. - *Additional assumptions:* The paper argues that IMPLANT is "zero-shot, unlike the proposed solutions of Fu et al. (2017) and de Haan et al. (2019) which require further interactions" with the test environment. However, I would argue that MPC should be treated like a reinforcement learning method here; it also requires interactions with the test environment (i.e., sampling from the dynamics model). I don't see why it would not be fair to use a policy-parametric RL algorithm (e.g. AIRL + reoptimizing) instead of MPC for optimizing the reward. Actually, I think that the test setting would then still favor MPC since it requires costly online optimization which is not always feasible in practice.__Questions__1. Are there problem settings where IMPLANT is applicable, but IRL+RL (e.g. AIRL + reoptimizing) is not?2. The paper mentions that a random rollout policy for MPC performs worse than using the learned policy. Is this also the case for a very large number of rollouts? Is the resulting control also worse in terms of the learned reward function or only w.r.t. the true reward function? __Additional Feedback__My main concern is that the current submission has too little contribution and, thus, I think that more research needs to be done before publishing a paper on this topic. Performing MPC after IRL seems way too little as a paper story. It's hard to suggest a direction for further research because I do not see the research question to start with. Regarding the combination of IRL and MPC, it might be interesting to investigate using MPC within the IRL loop. For example, it can be challenging to learn reward functions for multimodal demonstration based on a parametric (and usually unimodal) policy. The current paper could be improved by using an IRL reward and by comparing the approach with IRL+reoptimizing. However, at least for me, this would not affect my recommendation because these changes do not address my main concern.I hope this review is not too discouraging; the paper does have strong points in the presentation. |||| rating: 3: Clear rejection |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature
ABZSAe9gNeg	Differentially Private Synthetic Data: Applied Evaluations and Enhancements	long	The proposed method works as follows. Given samples are partitioned into two parts; one is for classifier training and the other is for data synthesizer training. Both are trained in a differentially private manner. After training, the DP synthesizer generates samples and the DP classifier labels them so that the resulting samples can be used as training samples. By the post-processing theorems, the resulting are differentially private, which are published as synthesized samples.The idea is interesting, simple, and unique. Also, experimental results demonstrate that the  models trained with the proposed method give s better F1 score compared to the existing methods. One limitation of this manuscript is that the reason why the proposed scheme can give better classification accuracy is not discussed. Also, the reason why the RMSE of the regression model trained with this scheme is worse than other methods is not examined, either. One quick thought is that the proposed scheme preserves the cluster structure of the samples well and therefore the classification model trained with the resulting sample has good accuracy. In contrast, the metric structure behind the samples is not preserved well and therefore the regression model does not have good RMSE. I am not sure this is correct or not, but anyway, I think further consideration on these issues will be interesting and needed for this type of experimental study to find a clue to improve data synthesization with DP guarantee.  Minor:FIg 6 is in the Appendix, not in the main body. Also, many important claims (mainly in experimental results) are given with results in the Appendix. The main claim should be constructed with the contents in the main body.   |||| rating: 4: Ok but not good enough - rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The authors proposed QUAIL, an algorithm that uses a supervised model and a synthetic data model to generate synthetic data that is good for downstream tasks. It also shows some empirical evaluations of the algorithm.The technical part, especially the experiments, might need some improvement. The main issue is that we cannot draw a clear conclusion from the experimental results. I would suggest taking one or two epsilon values, and look at the results in more detail to see if we can find any trend. Also, generating synthetic data under differential privacy is not an easy task, so I think it's ok to skip the regime where epsilon < 1, or try even larger epsilon to get a reasonable utility first.However, I would like to say that I very much appreciate the authors' effort in conducting experiments on quite a few datasets, and their integrity in presenting all the results no matter positive or negative.The paper can be better organized, for example,- The notations in the algorithm should be clearly explained/defined before the algorithm (for example, N and X), and some intuition can be added after the algorithm description. In the algorithm description itself, I think maybe it's clearer to defined the target dimension and the rest of the dimension separately, i.e. defining one sample as (feature, target) and later on we will have (synthetic feature, synthetic target). In the "split" part, I guess eps_C should be (1-p)*eps.- The part below Theorem 3.1 might better be put into the experiment section than the algorithm section.- In quite a few figures, we see interesting trends like some algorithm can have worse utility as epsilon grows. So it might be important to report the standard deviation of the algorithm for readers to better understand what was going on. Also, the texts in the figures can be made larger.- The paper called the algorithm "ensemble method". I feel like ensemble means something specific in ML, and simply using two different models together doesn't quite seem like ensemble. Maybe I'm understanding something here but it should be better explained. |||| rating: 4: Ok but not good enough - rejection |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	The paper proposes QUAIL, an ensemble of a generative model and a classifier, where both are trained with differential privacy, in order to generate a differentially private dataset (from the generative model) and a label vector (from the classifier). The paper further compared QUAIL with "other" differentially private generative models based on conditional GAN.My main concern with the paper is that there is no clear contribution. It is hard to judge whether the main claim is that the paper presents a survey of the current differentially private generative models, in which case, the survey part is very short and not in-depth. Or is that the the paper proposes QUAIL, which in many cases performs worse than other generative models. My some other concerns are detailed below:- There is no mention of \delta used for (\epsilon,\delta) - differential privacy- Please use something else than \delta for difference between performance measures as it can get confusing- As the version of GAN used in the paper is conditional, shouldn't the generator be differentially private as well? i.e. how are we protecting the privacy of the labels?- There is a mention of PATECTGAN performing better even compared to the non-noisy model trained on real data, how is this possible, please add some explanation. |||| rating: 4: Ok but not good enough - rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
G3fswMh9P8y	FedAvg with Fine Tuning: Local Updates Lead to Representation Learning	long	Motivated by the empirical success of Federated Averaging to generalize to new tasks (after fine-tuning), this paper seeks to connect this empirical success with a theoretical argument for how well FedAvg is capable of learning good representations, which is not an explicit objective of the optimization. Theoretical guarantees are presented for a multi-task linear regression setting and further empirical results demonstrate the effectiveness of learning representations with image classification tasks. ### Strengths- The paper is extremely well written and well organized. The mathematical notation is clearly described and easy to follow. - The theoretical results are well presented, with clear assumptions and, at each steps, the assumptions and statements are described in an intuitive way.- While the result under study (effectiveness of FedAvg for Representation Learning) has been shown (empirically) before, the formalization is an original contribution- The experiments are well described and effective at supporting the main theoretical claims### Weaknesses- The paper ends abruptly. It would be nice to have a conclusion section to tie up the paper. Perhaps some of the problem setting and notation could be abbreviated or moved to the appendix.- The main result is proved in simplified setting (linear regression tasks), which is far from the usual application of FedAvg for deep learning The paper adequately addresses its limitations.  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	This work shows that FedAvg learns better representations than Distributed SGD; with a largely theoretical contribution, and some well executed small-scale empirical results. This paper:* considers multi-task linear regression in a non-convex setting (i.e., prediction is the composition of a learnable subspace matrix, and a linear head), where the shared component across tasks lives in a low-dimensional subspace. The works shows that FedAvg can regress to the correct concept (given a good initialization), because local client-updates in FedAvg encourage diverse high-rank representations, whereas Distributed SGD finds low-rank representations.* provide empirical experiments (both linear regression and CNN-based image classification), showing that FedAvg discovers better representations, that generalize better than Distributed SGD. Originality: Despite the growing literature of work on the role of federated learning in representation learning, this work is very interesting and quite timely. Rather than proposing a new method, the authors provide an analysis explaining why FedAvg can produce more generalizable representations than Distributed SGD.Quality: Despite minor errors (probably typos in the math), this work is sound.Clarity: The exposition is clear, although the proof sketches are a little obscure; and cannot be as readily comprehended without going through the proofs themselves.I am happy to recommend this work for acceptance. Some minor limitations on the analysis should be stated; e.g.,* The second term in the min condition in theorem 1 can easily be vacuous for any reasonable number of iterations, and thus the theorem requires all clients to participate in each round… this should be made clear in a remark following the theorem. Numerical results in section 5.1 also have all clients participate in each round.  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper provides theoretical analysis of FedAvg with Fine Tuning. When the local model weights for clients lie on a linear subspace, theorem 1 suggests FedAvg gets linear convergence, while proposition 1 suggests Distributed GD may diverge for representation learning. Some empirical results on a toy problem are used to verify the theorem, and results on CIFAR-10 and CIFAR-100 shows FedAvg+FineTune outperforms D-SGD+FineTune. Strengths+ FedAvg is one of the most popular algorithms in FL. Though the connection between FedAvg and MAML/Reptile is known, the theoretical justification of the advantage of FedAvg for representation is (to my knowledge) novel. Weakness- The paper can probably improve on clarity. I find it difficult to connect Theorem 1 to the FedAvg algorithm described in Section 2 and the objective eq. (5). - As the authors acknowledged, previous work has drawn connections between FedAvg and MAML, and empirically shown FedAvg+FineTune is better than SGD+FineTune. The main contribution of this paper is theoretical, while the assumptions are very strong.  NA  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	This paper analyzes both theoretically and empirically the performances of FedAvg with local fine-tuning as a post-processing step. More precisely, they consider the scenario where the model parameters are split into two distinct parts: a shared backbone model and a personalized head model. This scenario encompasses for instance convolutional neural networks where the backbone model extract high-level features and the head performs prediction. In order to derive theoretical results, the authors restrict their analysis to the linear case where the backbone parameter is a matrix $B \in \mathbb{R}^{d \times k}$ with $k < d$ and $\omega$ is the head.Their main result is a Theorem showing the generalization power of FedAvg with at least 2 local updates via convergence towards the ground-truth backbone parameter $B_\star$. They validate their theory with experiments. Overall, the paper is well-written and clear. The related work section acknowledging previous works is sufficient.My main concerns / remarks are:* The work by Collins et al. (ICML 2021) on shared representations should be compared more deeply with the proposed analysis since the two setting bear some similarity. Some comparison has been done in the supplement but should appear in the main paper in my opinion.* How difficult the analysis beyond the linear case is? N/A  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
ryetZ20ctX	Defensive Quantization: When Efficiency Meets Robustness	long	Summary of paperThis paper presents an approach  for quantising neural networks such that the resulting quantised model is robust to adversarial and random perturbations.The core idea of the paper is to enforce the Lipschitz constant of each linear layer of the network approximately close to 1. Since the Lipschitz constant of the neural network is bounded by the product of theLipschitz constant of its linear layer (assuming Lipschitz 1 activation functions) the Lipschitz constant of the trained neural network is bounded by 1. This results in a model which is robust to adversarial and random noise ad all directions in the model space are non-expansive. Algorithmically, controlling the Lipschitz constant is achieved by using the orthogonal regulariser presented in the paper Cisse et.al which has the same motivation for this work but for standard neural network training but not quantising. The authors presents thorough experimental study showing why standard quantisation schemes are prone to adversarial noise and demonstrate clearly how this approach improves robustness of quantised network and sometimes even improve over the accuracy of original model. Review:The paper is well written with clear motivation and very easy to follow. The core idea of using orthogonal regulariser for improving the robustness of neural network models have been presented in Cisse et.al and the authors re-use it for improving the robustness of quantised models. The main contribution of this work is in identifying that the standard quantised models are very vulnerable to adversarial noise which is illustrated through experiments and then empirically showing that the regulariser presented in Cisse et. al improves the robustness of quantised models with rigorous experiments. The paper add value to the research community through thorough experimental study as well as in industry since quantised models are widely used and the presented model is simple and easy to use. Some suggestions and ideas:1. It will be great if the authors could add a simple analytical explanation why the quantised networks are not robust.                      2. The manifold of Orthogonal matrices does not include all 1 - Lipschitz matrices and also the Orthogonal set is not convex. I think a better strategy for this problem is to regularise the spectral norm to be 1.  Regularising the spectral norm is computationally cheaper than Orthogonal regulariser when combined with SGD using power iterations.  Moreover the regulariser part of the model becomes nice and convex.3. Another strategy to control the Lipschitz constant of the network is to directly penalise the norm of the Jacobian as explained in Improved Training of Wasserstein GANs (Gulrajani et. al). |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	Summary: The paper proposes a regualrization scheme to protect quantized neural networks from adversarial attacks. The authors observe that quantized models become less robust to adversarial attacks if the quantization includes the inner layers of the network. They propose a Lipschitz constant filtering of the inner layers' input-output to fix the issue.  Strengths:The key empirical observation that fully quantized models are more exposed to adversarial attacks is remarkable in itself and the explanation given by the authors is reasonable. The paper shows how a simple regularization scheme may become highly effective when it is supported by a good understanding of the underlying process.Weaknesses:Except for observing the empirical weakness of fully quantized models, the technical contribution of the paper seems to be limited to combining the Lipschitz-based regularization and quantization. Has the Lipschitz technique already been proposed and analysed elsewhere? If not, the quality of the paper would be improved by investigating a bit more the effects of the regularization from an empirical and theoretical perspective. If yes, are there substantial differences between applying the scheme to quantized models and using it on full-precision networks? It looks like the description of the Lipschitz method in Section 4 is restricted to linear layers and it is not clear if training is feasible/efficient in the general case. Questions:- has the Lipschitz technique been proposed and analysed elsewhere? Is the robustness of full-precision models under adversarial attacks also improved by Lipschitz regularization?- how popular is the practice of quantizing inner layers? Has the performance of fully quantized models ever been compared to full-precision or partially quantized models in an extensive way (beyond adversarial attack robustness)? - are the adversarial attacks computed using the full-precision or the quantized models? would this make any difference?- the description of the Lipschitz regularization given in Section 4 assumes the layers to be linear. Does the same approach apply to non-linear layers? Would the training be feasible in this case?  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	imho, this manuscript is clearly written, addresses a confusing point in the current literature, clarifies some issues, and provides a novel and useful approach to mitigate those issues. reading the other comments online, the authors seem to have addressed those concerns as well. |||| rating: 7: Good paper, accept |||| confidence: 2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper
04OPxj0jGN_	AnimeRun: 2D Animation Visual Correspondence from Open Source 3D Movies	long	- 2D animation data are expensive to collect since they have different characteristics with 3D animations. Simply projecting 3D animations to 2D are not sufficient so that the author proposes a novel approach to make 2D cartoons for 3D.- Taking benefits from 3D-agnostic methods, they acquire various kinds of labels, which cannot be automatically acquired in 2D cartoons. - Compared to previous datasets, AnimeRun is larger and composed of better cartoon qualities.- Easy to follow the paper's claims due to clear writing. Their supplementary materials and experiments clearly evidence the effectiveness of the proposed dataset.- Great verification about dataset quality.  - Too few benchmarks to check tendencies of recent models. They have only evaluated two models, i.e., PWC-Net and RAFT. It would be better to evaluate more baseline models to show that their data have similar data distributions to the prior datasets.- Their proposed data generation methods are limited for transparent objects. Moreover, several effects such as motion blur and bokeh are removed so that these kinds of effects cannot be included in the proposed dataset although they are often found on 2D cartoons.- Data generation steps are still expensive since they rely on 3D animation datasets. According to the Table 2,  the number of images used for training strongly correlates to the performance. Moreover, the evaluated model achieves better performance when training data are finetuned on FlyingThings3D.  Due to the small dataset size, the AnimeRun cannot be used standalone.- It uses many external methods; however, their details are not fully given. For better reproducibility, the authors should provide more details to make their paper self-contained.- I agree that the proposed dataset has many advantages compared to prior datasets. However, I cannot find any real-world applications using 2D cartoon datasets. Why is this dataset needed? Authors should elaborate more about applicability on real-world tasks or add experiments about the usage of the proposed dataset.  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper presents a new 2D animation dataset for pixel-wise (optical flow) and region-wise (segment matching) correspondence. This dataset is made by converting three high-quality 3D movies into 2D animation, thus it is characterized by rich image composition and complex motion. Authors clearly explain he motivation behind this dataset and what makes it different from existing ones. They argue about their design choices, they establish a benchmark and evaluating existing methods on this. Finally, they offer a great analysis that might be useful for future works.   **[Good Story]**Authors have a nice story to say. They clearly explain their motivation that has to do with how existing 2D datasets are made and what properties do they lack of. They list the drawbacks of them and come to fix them with their dataset. They argue about their design and label synthesis choices, which completely make sense. They provide a fair and insightful statistical comparison with existing datasets. Finally, they establish a benchmark, draw some concrete conclusions and present a great analysis. While there might be some weaknesses if you zoom in, the big pic is that there is a story well-said.**[Contribution]** Authors contribute two-fold. 1. They observe a gap on existing datasets and find an elegant solution of filling it. Their idea is converting 3D movies into 2D animation, so that their dataset has both rich image composition and more complex motion. This can be clearly seen in the video attached. 2. They establish a new benchmark on pixel-wise and region-wise correspondence and provide an insightful analysis.**[Form]**The paper is well-written and really easy to follow. Figures are explanatory, video helps presenting the idea and in general one will not find it difficult to follow the paper. **[Benchmark]**The benchmark could have been more rich. Authors choose to evaluate two models for pixel-wise correspondence and one model for region-wise one. While one can draw conclusions from the analysis based on those experiments, it would be probably better to include some more to make it more concrete.**[Dataset size]**I guess that the dataset's size is kinda small, am I right? Can it be used alone for training? I am asking because I observed that you also use FlyingThings3D.**[Real-world applications]**While you do mention real-world applications and how this dataset can be useful (L13-17), I would suggest explaining it more. **[Code]**Code is not released yet, I guess it would be released soon, right?  |||| rating: 8: Top 50% of accepted papers, clear accept |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	This paper presents a new dataset for visual correspondence of 2D animation, which is generated from three open-source 3D movies. The advantages of utilizing open 3D movies make the components in the film (e.g., background compared to CreativeFlow+[18]) more complicated than the previous dataset. The dataset includes pixel- and region-wise correspondence labels to support some applications like frame interpolation and automatic colorization.  * This paper contributes a new dataset specific for visual correspondence tasks in 2D animation. Besides the open dataset, the authors also perform experiments using the new dataset for a benchmark purpose. This motivation and contribution are very fit for this track. * The dataset contains much more complicated details than the previous dataset for this task. This can be checked in the video, which is a nice add-on.* Both the pixel-wise and region-wise labels are provided. The experiments also show the effectiveness of both tasks.* Sufficient details are provided in the main text, there are some minor concerns that I will discuss below. * In the experiment, which test set is used to evaluate metrics in Table. 2 and Table. 3? It may be unfair for these comparisons if only evaluated on the test set of AnimeRun, because the boost in the results may come from the less domain gap between train/test data, other than the quality and diversity of the training dataset for better generalization.Some minor concerns:* L140: What does it mean to turn off all illuminating objects? Just remove these objects or set the intensity of light to zero? Why may the illuminating objects (e.g., lamps) disappear in the optical flow?* Fig.4: Why is there no curve of MPI-Sintel in (a) and ATD-12K in (c)?* L110: "randomly assign representative colors". This part may need more details. For example, how to select representative colors, and how many colors are selected? Any comparison on the statistics before/after this augmentation?* Why does the FlyingThings3D dataset need to be mixed with the proposed dataset for fine-tuning?* The optical flow in the video (e.g., about 3:36) changes severely, especially in the background, which seems there are not many changes in the color frame.* It may be better to perform experiments and provide the evidence for improving the real-world application like frame interpolation to show the potential power of this dataset more clearly.  |||| rating: 7: Good paper, accept |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	This paper introduces AnimeRun, the first practical dataset for 2D animation correspondence. Unlike existing datasets, it is suitable for full-scene 2D animation. It contains pixel-wise correspondence labels for optical flow estimation and region-wise correspondence labels for segment matching. Using the dataset, the authors establish benchmarks for the two types of correspondence. The results show the effectiveness of AnimeRun and the limitations of existing methods. 1. This paper tackles a less-explored but valuable task. 2D animation production is labor-intensive and requires a large number of manpower for the processes of frame interpolation and colorization. The dataset facilitates automation of those processes, leading to the well-being of workers and the preservation of 2D animation culture.2. This paper introduces the first practical dataset for 2D animation correspondence. Existing datasets with real hand-drawn 2D animations (e.g., ATD-12K, [Zunda Horizon dataset](https://greenfunding.jp/pub/projects/1852), [TRIGGER dataset](https://www.nii.ac.jp/news/release/2022/0315.html)) do not have ground truth labels for correspondence. MPI-Sintel and CreativeFlow+ are not suitable for 2D animation (most animations draw full scene, not a single model like CreativeFlow+).3. The effectiveness of the dataset is demonstrated from various perspectives. As shown in the statistics (Figure 4) and the quantitative results (Table 2, Table 3), the advantages of the proposed dataset over previous studies are clear. The analyses with visualization results in Sec. 4.1 and 4.2 are convincing. 1. Some parts of the manuscript are difficult to understand at a glance.- Sec. 1: The real-world applications of the dataset are not so intuitive. They are described in text（L.13-17) without figures. It would be helpful to add a figure of the animation production process and explain that the two types of correspondence labels in the dataset are useful for frame interpolation and colorization. It would be good if the introduction is easy for anyone to understand (e.g., optical flow researchers not familiar with 2D animation, people involved in animation production who are not familiar with computer vision).- Figure 2: Readers should be able to understand the advantages of this study at a glance without having to read the caption or main text. For example, with red and green textcolor,```  (a) MPI-Sintel      (b) CreativeFlow+  (c) AnimeRun (ours)  Full scene for 3D   One model for 2D   Full scene for 2D```- Table 2 etc.: Dataset names are too abbreviated. I think it fits in the table without abbreviating anything other than FlyingThings3D. The comparison of Sintel, CreativeFlow+, and AnimeRun, which is important for this paper, should be understood by readers at a glance.2. There is room for improvement in dataset creation.- There is a large gap with real hand-drawn 2D animations (e.g., Figure 3 of ATD-12K [19]). Specifically, AnimeRun lacks shade and background texture. Animations and games with toon-shaded 3D models look more like real hand-drawn 2D animations (e.g., [D4DJ](https://www.youtube.com/watch?v=u5_dOnl-UJE), [Expelled from Paradise](https://www.youtube.com/watch?v=89heoIzvtUw), [Umamusume](https://www.youtube.com/watch?v=cmuTy73jzSs), [Genshin Impact](https://www.youtube.com/watch?v=PXp8qKZeHeY)).- The dataset is made from only three videos, and all three were produced by Blender Studio.- L.104-109: AnimeRun neglects the diverse styles of 2D cartoons and focuses on typical styles. Unlike CreativeFlow+, it lacks diversity in line drawing styles. Although the stylistic diversity of CreativeFlow+ looks excessive, it would be possible for the authors to make an effort to get closer to the distribution of real animation styles.- If the dataset only aims for applications to midway processes of animation production (e.g., frame interpolation and colorization) and there is no need to resemble finished products, the authors should state this clearly. In that case, "finished animation" (L.40) should be modified.- If the authors do not address the above limitations, they could be described in Supplementary File.  |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This submission introduces a new dataset and showcases its use for pixel-wise (optical flow) correspondence and region-wise (segment matching) correspondence tasks.Authors achieve this by applying custom pre-processing on open-source 3D animation to transform it into 2D cartoons. This article's clear strengths are the ability to use open-source animations and 3D to 2D processing workflows to obtain data for optical flow and segment matching tasks. While this article is a valuable dataset introduction, there seem to be some key issues that should be discussed.Mentioning a few practical aspects of the state-of-the-art methods and the datasets used might have been beneficial. Such information reduces the barrier to entry for experimentation.There seems to be a lack of explicit Limitations section in this work, and parts of the writing seem incorrect.  |||| rating: 5: Marginally below acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct
6hzH8pohyPY	Batch-Size Independent Regret Bounds for Combinatorial Semi-Bandits with Probabilistically Triggered Arms or Independent Arms	long	This paper considers the combinatorial semi-bandits problem under two different settings: (1) probabilistically triggered arms setting where the set of played arms can trigger reward on other arms, (2) the non-triggering setting with independent arms. For both these settings the authors improve batch-size dependence in regret as compared to previous work by considering a new variance-based condition on the underlying reward distributions. The authors show that this condition is satisfied in many applications such as cascading bandits, influence maximization on DAGs etc, and leads to significant improvements in the regret achievable for these applications.  Strengths: The paper provides a good contribution to the literature on combinatorial semi-bandits as it improves the dependence on batch size from linear to logarithmic in many practical applications. The paper is very well-written and provides clear intuition behind various ideas/assumptions. The comparison with prior work is also adequate. I do not foresee any negative societal impact.   |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	This paper proposes new smoothness conditions for the well-known combinatorial semi-bandit problem with probabilistically triggered arms. It is shown that well-known combinatorial learning problems of interest, including combinatorial cascading, influence maximization, and probabilistic maximum coverage bandits, satisfy these conditions. With the so-called triggering probability and variance modulated smoothness condition, the authors significantly improved the O(K) factor that appears in the previous regret bounds, where K represents the batch size. The authors also propose a non-triggering version of the smoothness condition and show that dependency on K can be completely removed for the non-triggering combinatorial semi-bandit. In order to achieve the improved regret bounds for the triggering version of the problem, the paper relies on empirical Bernstein inequality to construct upper confidence bounds of base arms. For the non-triggering version, batch size independence is achieved by constructing sub-exponential concentrated confidence intervals.  Strengths: Finding general enough smoothness conditions under which the effect of batch size on the regret can be significantly improved is an important problem. This paper intuitively develops new smoothness conditions and algorithms that use tailored confidence intervals to address this problem. Computationally efficient algorithms are proposed for triggering and non-triggering cases. Writing is clear. Theorems are well organized. Weakness:True batch size independence comes with an algorithm that requires enumeration of all possible actions. I wonder if any computationally efficient algorithm can achieve batch size independence.  Limitations are adequately discussed. This work is theoretical in nature and does not have a societal impact.   |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	Remark: Throughout my review, [n] refers to the n-th reference in the full paper (with appendix) from the supplementary material. (I mention this because the reference numbers differ in the 9-page submission.)This paper studies Combinatorial Multi-Armed Bandit (CMAB) problems with probabilistically-Triggered arms (CMAB-T). In essence, CMAB is a variant of the standard bandit setting where the learner chooses a subset of arms (a.k.a. a “super arm” or “action”) and the mean reward is a function of the chosen subset and of its component arms’ mean rewards, and for CMAB-T the learner only observes feedback on a random subset of the chosen arms. This formulation generalizes problems including cascading bandits for ranking search results, online influence maximization, etc.Prior work [27] introduced a smoothness condition (with respect to the function that maps the component arms’ mean rewards to the subset’s mean reward) that improved existing regret bounds [7] by a factor of $1/p^*$ (see Line 36 for details). This work provides a refined smoothness condition that involves the variance of the arms’ reward distribution, i.e., the condition is less restrictive when the variances are smaller (simply because the mean rewards are easier to learn in such cases). Provided this condition holds, the authors develop a variance-aware UCB-style algorithm based on the empirical Bernstein inequality [1] and show its regret improves existing work with respect to the “batch size” K, which is the maximum of arms that can be triggered at each round (e.g., the number of search results that a cascading bandit algorithm chooses).In addition, the authors study non-triggered arms (Section 4) and specialize their results to various application settings (Section 5). STRENGTHS:In my opinion, the strengths of the paper are (1) a novel and fairly natural smoothness condition that incorporates variance information, (2) algorithms that exploit low variance problem instances via empirical Bernstein confidence sets (instead of “variance-unaware” Hoeffding-based confidence sets), and (3) analysis that shows exploiting variance in this manner can dramatically reduce regret in terms of $K$ (e.g., from $K$ to $log K$ in some settings).At a higher level, I feel the main strength of this paper is to show that variance-aware algorithms lead to polynomial improvement in terms of K, and to do so in a fairly general setting (CMAB-T). More specifically, paper A (see below) recently proved something similar for cascading bandits (a special case of CMAB-T), although it focuses on gap-free bounds so is complementary to the current work (which focuses on gap-dependent bounds). More broadly, a number of recent papers have demonstrated similar (in spirit) polynomial improvements resulting from variance-aware algorithms in various bandit and RL settings (e.g., paper C below achieves a polynomial improvement in terms of the horizon for finite-horizon RL; see also the references therein and in paper B below). Thus, the current paper seems rather timely given the similar flavor to these works.(To be clear, I’m not demanding these papers be cited — reference A is a very recent preprint and the others are on different topics — but rather, emphasizing the timeliness of the current work).(A) Vial, Sanghavi, Shakkottai, Srikant, “Minimax Regret for Cascading Bandits”, arXiv preprint(B) Zhang, Yang, Ji, Du, “Improved Variance-Aware Confidence Sets for Linear Bandits and Linear Mixture MDP”, arXiv preprint(C) Zhou, Gu, Szepesvari, “Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov Decision Processes”, COLT 2021WEAKNESSES:(1) While mostly well-written, the paper is very dense, particularly the first few technical sections (i.e., starting from Section 2). I imagine this is mostly a side effect of packing many technical definitions and results into a short page limit, but it was hard to follow at times. For example, Section 2 would have been easier to understand if the authors had used a simple special case of the CMAB-T model (e.g., cascading bandits) as a running example to help illustrate the technical definitions (of which there are many in Section 2).(2) Along these lines, I did not find the proof sketches very illuminating, and it was difficult to glean much intuition from the full proofs in the appendix given their length and density. In other words, I would have preferred an intermediate explanation -- high-level like the proof sketches, but more detailed like the actual proofs -- to illustrate the key ideas of the analysis (again, perhaps in a simpler special case of CMAB-T).~~(3) Unless I'm mistaken (in which case I'll gladly update during the discussion period), the improvement over prior work in the disjunctive cascading bandit setting is oversold. (See "Questions" section for details.)~~Update post-rebuttal: Weakness (3) has been satisfactorily addressed and I remain in support of acceptance. Overall, I feel the limitations were acknowledged -- for example, the assumptions are clearly stated in each of the theorems, Remark 3 acknowledges that additional assumptions are needed to ensure the proposed Condition 3 implies the existing Condition 2, etc. In terms of negative societal impact, the authors simply state (in the checklist) that there is no foreseeable impact. Personally, I feel this view is somewhat narrow -- while the paper is theoretical, the algorithms could obviously have real-world impact if deployed -- but I understand the authors' point and view this more as a difference of opinion than an objective weakness.  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
cUY5OkP3VR	Hyperparameter Sensitivity in Deep Outlier Detection: Analysis and a Scalable Hyper-Ensemble Solution	long	This paper studies hyperparameter (HP) sensitivity for the outlier detection (OD) task, which is a fundamental challenge in the unsupervised setting. The authors conducted an extensive empirical study of several deep OD models' performance with various HP choices and demonstrated these SOTA OD models are very sensitive to HP selection. A deep auto-encoder ensemble model is proposed to reduce the dependency on HP and achieve better OD performance with the help of the ensemble. In order to prevent long running time for training each individual model in the ensemble, model parameters are shared among multiple models through three techniques: 1) skip link for depth ensembling; 2) batch ensemble for width ensembling; and 3) sub-sampling. Several experiments have been performed to demonstrate the effectiveness of the proposed ensemble method and the efficiency of the model training. * Strengths  1. This paper raises an essential question for the outlier detection task. Under an unsupervised setting, the model performance is agnostic before actual prediction/testing due to the lack of ground-truth label information, resulting in no clear HP selection strategy, compared to the supervised counterparts (validation).  1. The reviewer appreciates the time and efforts put into the experimental study of the existing OD models in Section 3. This study clearly points out that most deep OD models are sensitive to HP selection, and the final performance of models may suffer from potentially-large variation.  1. The parameter-sharing AE model and the three related ensemble techniques proposed in this paper may help reduce the training time for large deep ensembles.* Weakness  1. Although rarely used in deep OD models (majorly due to long training time), the ensemble ideas are widely used in OD tasks. For example, the Isolation Forest (IF) is an ensemble tree method. The ensemble ideas inside this paper simply aggregate the outlier scores from models inside the ensemble, which is the common practice.  1. The paper doesn't actually answer **RQ1) how to design an unsupervised deep OD model that is robust to its HPs**. Note that **RQ1** is different from **Q1** and **Q2** given in Section 5. Although the ensemble method reduces the HP dependency for each individual sub-model inside the ensemble, the HP for the entire ensemble is not studied. For each sub-model, the HP may be model depth, layer width, learning rate, etc. The HP for the entire ensemble may be the number of sub-models, sub-model HP selection range, etc. The first paragraph in Section 4.2.3 gives an example of the sub-model HP selection range. The current ensemble presented in the paper (Table 9 in the appendix) can only be considered as a single HP setting for the entire ensemble. It is unclear how the method performs if different values are given for Table 9. Therefore, we cannot tell if the proposed ensemble is robust to its HPs or not (RQ1).  The main limitation of this paper is mentioned as the second weakness previously. If the major issue (HP robustness of the ensemble ) is not answered, the use of the ensemble method cannot be justified, which also weakens the training speed-up claim for the ensemble method.  |||| rating: 5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	The papers provide 2 main contributions:1. The paper performs a large-scale study of how different model hyperparameters (e.g., number of layers, number of training iterations, learning rate, etc.) affect the performance of several existing deep learning-based outlier detection models (vanilla autoencoder, RDA, DeepSVDD, GANomaly, and RandNet). The results show that these models are quite sensitive to the hyperparameter choices, which are often not explored in the original papers. Furthermore, ensembling the outputs of these models (across different hyperparameter runs) generally yields higher outlier-detection performance than methods for choosing a single hyperparameter.2. The paper proposes an "ensemble" of autoencoders of varying depths and widths called RobOD, which is used for outlier detection. I put "ensemble" in quotes, because RobOD is not an ensemble of independently trained models, but rather an "ensemble" in a similar sense to using Monte-Carlo dropout to generate a large set of models from a single trained model. Empirical results demonstrate how RobOD is competitive at outlier detection against other models, while being significantly more robust to the hyperparameter choices. (An alternative interpretation is that RobOD eliminates having to choose between depth and width hyperparameters, so there are simply fewer hyperparameters to deal with.) ## Strengths**S1. Large-scale experiments**\Through a large set of experiments, the authors conclusively demonstrate the importance and influence of hyperparameters on current deep learning-based outlier detection models.**S2: RobOD demonstrates a method for creating an "ensemble"-like autoencoder model"** \By enforcing weight-sharing and adding reconstruction losses at varying depths, RobOD only requires a single autoencoder model that can "simulate" multiple autoencoders of varying depths and widths.## Weaknesses**W1. Missing details on how autoencoder is used for OD**\Perhaps this comes from my ignorance about OD models in general, but in the paper I didn't see any mention of how an autoencoder model is actually used for outlier detection. Suppose I have trained an autoencoder in an unsupervised manner by minimizing reconstruction loss. Then what? How is it used for outlier detection? For example, I can envision several ways of using a trained autoencoder for OD. If the autoencoder is a variational autoencoder (VAE), then I can calculate an estimated density of a given example under the training data distribution. If the autoencoder is deterministic, then I could threshold the reconstruction loss: if an example has high reconstruction loss, then I deem it an outlier. Are you using either of these methods? Or something totally different?**W2. Missing analysis of RobOD ensembling procedure**\One thing I'd like to see is whether the multi-depth / multi-width autoencoder setup improves the performance of autoencoders in general. For example, did this ensembling reduce the autoencoder's reconstruction loss?**W3. Does not separate variation in hyperparameters from variation in initialization**\Maybe I missed this point somewhere in the text, but I struggled to separate the two main sources of variation in model output. First, there are many hyperparameter choices. Second, there are many random initializations. How are the authors certain that the benefit from "ensembling" was due to ensembling across hyperparameters, instead of ensembling across many random initializations? If you took a vanilla autoencoder with the same width and depth as the widest/deepest RobOD constituent, then used many random initializations, how does that compare against RobOD?**W4. Missing contextualization with key related works**\To me, the multi-width ensembling adopted in RobOD is quite similar to Monte-Carlo dropout, except that the dropping-out is not random but rather follows a particular deterministic pattern. If my understanding of RobOD is incorrect, please correct me! However, if my understanding is correct, then I think it's worth addressing this similarity. Likewise, the multi-depth ensembling in RobOD is quite similar to stochastic-depth models, except again the dropping-out of layers is not random but follows a particular deterministic pattern. Furthermore, the RobOD autoencoder also somewhat resembles a "U-net," except that the loss is reconstruction instead of segmentation. Making these connections to existing literature would be helpful to understanding RobOD. The authors briefly discuss that their method is costlier than training a single model, and they do not list any negative societal impacts.  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	This paper studies the influence of hyperparameter for deep OD, as well as the strategy that alleviates the hyperparameter tuning issue during the training process. First, the authors soundly identifies the problem of hyperparameter selection in deep OD by extensive experiments with various AE-based deep OD methods and benchmarks. Then, with a general idea to avoid hyperparameter tuning via hyper-ensemble, the authors propose to leverage skip connection and batch ensemble technique to speed up the hyper-ensemble calculation. The results justify the effectiveness of the proposed strategy. Strengths:1. This is a well-motivated paper that tries to address a real problem that is often neglected. Hyperparameter selection is an important issue for almost all machine learning tasks, but it is especially challenging for unsupervised tasks like deep OD, as the outliers are often not accessible for validation. I appreciate the authors' effort to bring this topic up. Besides, the authors provides an extensive review on existing deep OD methods and a thorough analysis on them, which makes the discussion more convincing.2. The presentation of the paper is good. I do enjoy reading this paper, as it follows a very clear pipeline: identifying a valuable problem-giving a possible solution-tailoring the solution for improvement, which is a good example that exhibits a complete process of research. Besides, all concepts are clarified and used consistently across the entire paper. 3. The proposed ROBOD seems to be simple and easy-to-implement, but enjoys both good effectiveness and efficiency against other AE-based baselines.Weaknesses:1.  The largest limit of this work is that the discussion seems to be limited to AE-based deep OD methods. I do understand that AE plays a center role in deep OD, but its performance is also shown by numerous works to be less satisfactory, especially on relatively complex datasets. By contrast, recent self-supervised deep OD methods (e.g. transformation based methods, CSI) achieve significantly better performance in many cases, and the authors did not discuss them as examples, which is a clear weakness to me.2. The extendibility of the proposed ROBOD is a problem. For me, it is not straightforward to extend the skip connection based strategy to non-AE based deep OD methods that do not use a symmetric DNN architecture. In the meantime, ROBOD only considers the acceleration of two hyperparameters (though they are two important ones). For now, it is hard to call ROBOD a generally applicable framework for deep OD.  Despite that this is a good work in general, authors should add more discussion on the limit of their work. No negative social impact.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.
snUOkDdJypm	Finite-Time Last-Iterate Convergence for Learning in Multi-Player Games	long	This paper studies the last iterate rate of convergence of extra gradient and optimistic gradient algorithms in smooth monotone games with continuous convex action sets. More specifically, the authors prove that if players employ those algorithms with a constant step size they achieve last iterate convergence rate $O(1/\sqrt{T})$. This paper is well written and clear. The technique used for finding the potential function needed to prove the result is novel and interesting. I consider the contribution of this paper clear and quite important, since it complements and extends existing results from the unconstrained to the constrained setting.Minor comments: In line 231 there is a typo - hence (an) it suffices.Line 276: action -> action. This paper has no negative societal impact.   |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	This paper studies the last-iterate convergence rate to a Nash equilibrium in monotone games (with gradient feedback). It extends previous work to analyze the convergence of no-regret learning algorithm with constant step size in (possibly) constrained games. The authors propose a novel notion, namely the tangent residual, and use it to discover the potential function to show that the extragradient algorithm and the optimistic gradient algorithm have tight last-iterate convergence rates.  Strengths: The paper is well-written. The achieved results are quite interesting and relevant. The proofs and appendix are very detailed and provide strong supports for the claimed results. The tangent residual notion and the proposed computer-aided proof technique are, as far as I know, novel in this research context; moreover, they might be useful beyond the scope of this paper.Weaknesses: no major weaknesses. Optimally tuning parameters of the algorithms (and explicitly determining the constants in convergence rates) might make the results clearer.  the authors adequately addressed the limitations and potential societal impact of their work  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	The paper focuses on proving last iterate convergence for both Extra Gradient and Optimistic Gradient Descent methods for monotone games in both constrained and unconstrained settings (these setting captures convex-concave min-max optimization for example where the operator is monotone). The authors show tight last-iterate convergence of $O(1/\sqrt{T})$ without any uniqueness assumptions that prior works had consider (without proving rates). Please note that there were works that established last iterate convergence that the dependence on the parameters of the game were not clear (it might be exponential for example in the size of the game). The authors show dependence that is polynomial in the parameters of the game (Lipschitz constant and diameter of the domain for constrained). The idea is first to establish best iterate convergence and then show that the tangent residual (effectively the Hamiltonian) is a potential function.   Pros:1) The paper is well written and the proof is elegant. 2) The problem was an open-question that researchers were actively working on including the reviewer.3) The result does not hide any exponential dependence on the size of the game, it is very clean with normal assumptions.Cons:1) The idea of using sum of squares was already in the literature to construct/prove potential functions. This fact though does not decrease the significance of this work. The work is theoretical.  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper provides the first last-iterate convergence rates of extragradient algorithm (EG) or the optimistic gradient algorithm (OG) to Nash equilibrium of constrained smooth monotone games in terms of the gap function and Lipschitzs and monotone variational inequality, which match the existing lower bounds $\mathcal{O}(T^{-1/2})$. Tangent residual is proposed as a new proximity measure to a Nash equilibrium and used for convergence analysis. Originality: The algorithms and problem settings exist in the literature, but the non-asymptotic convergence rate in constrained smooth monotone games is new and tight, and the notion of tangent residual and the choice of potential functions for convergence analysis are novel. Hence, I think the originality is sufficient.Quality: This is a complete piece of work with both strengths and weaknesses. The claims are well supported by the theoretical analysis which looks reasonable, elaborate, and sophisticated (I have read the Lemmas and theorems but have not checked the proof). The experiments look reasonable to me, but it seems that the numerical evidence can be stronger if the authors compute for more iterations in the experiments and plot the potential function value and the performance measures in Appendix J. (see my final question for detail). Clarity: The submission is very clear and well organized. The intuition behind some notions could be clarified as elaborated in my questions.   Significance: The results look important to me since it provides the first non-asymptotic convergence rate in constrained smooth monotone games, which also matches the lower bound.  In the checklist, item 1b said "Did you describe the limitations of your work? [Yes] Our results are theoretical. We state the assumptions of our results in the statements of the lemmas." Do you consider the assumptions as limitations? If yes, do you intend to weaken them in future works? The end of the introduction also proposes a future direction to apply the techniques of this paper to stochastic gradient feedback, which implies a limitation and its solution.   |||| rating: 8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations. |||| confidence: 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
SL4SwMNnwIe	Acceleration in Distributed Sparse Regression	long	The paper proposes a method for accelerating distributed high-dimensional regression under sparsity constraints. To do so the authors adapt Nesterov's proximal gradient method to the setting by combining it with a consensus step based on the connectivity of the graph. Experiments on synthetic data and a couple of public datasets demonstrates superiority validate the analytical claims. Strengths:1. The paper clearly improves over prior work in theory and in practice.2. The application of Nesterov's method to this setting is clear and intuitive and overall the authors demonstrate its impact on multiple problem settings.Weaknesses:1. The explanation of the method is a bit dense and may not make complete sense to readers unfamiliar with all the technical prerequisites. Specifically, the logic behind equation (13) was not clear to me. It appears to rely on prior work ([7],[30],[19]) but as it is the key step in the algorithm, I would strongly recommend explaining it in a self-contained manner to make the picture clearer. 2. The authors acknowledge that when the graph is not connected enough (for e.g. when condition (18) is not satisfied) then multiple rounds of communication/iteration may be needed to facilitate convergence. It is not clear what that exactly means. I am assuming it means multiple rounds of S.1 (consensus and gradient tracking). However I do not see clearly how that will overcome the lack of connectivity. This is another place where some intuition would be helpful. Also won't it affect the statistical and computational guarantees in (5) since now each iteration will have higher cost due to multiple rounds of communication?3. The paper ends abruptly with no conclusion section or a discussion of limitations/future work.  N/A  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	This manuscript studies the problem of sparse regression in a decentralized setting. Specifically, an accelerated algorithm is proposed, and its convergence and statistical properties are proven. Building upon recent advances in high-dimensional statistics, the authors take advance of the restricted strong convexity and restricted smoothness properties to develop their convergence analysis. Decomposable regularizers are also assumed. The authors show the performance of the proposed algorithm for the studied problems  Strengths- The manuscript clearly states the proposed problem and compares it with existing literature. It is clear that the proposed method extends the existing literature on the topic- The manuscript is well written and easy to follow. The authors make an effort to explain the details in sufficient detail. Weaknesses- While there is sufficient contribution, many of the core technical ideas build upon [1] and classical approaches to decentralized optimization.- "almost" strongly convex or smooth is strange. The way the authors handle inexactness and approximation terms like that causes confusion. For example, the authors mention trajectories (approximately) traveling in the "good" portion of the landscape. However, if the geometric properties of the function only hold in the appropriate subspaces, there is no reason to believe that they will hold in the approximate trajectories also. - The authors mention and work with l0 in the sparse regression setting and numerics but it looks like l1 is used instead. - The selected simulations do not clearly show the asymptotic behavior studied.  No societal impact limitations.   |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper investigates distributed sparse regression in high-dimensions to speed up its computation, which combines Nesterov’s proximal gradient with consensus and gradient-tracking mechanisms. This method can estimate locally the gradient of the empirical loss while enforcing agreement on the local estimates. Based on the basic assumptions, the proposed algorithm globally converges at linear rate. The iteration complexity scales as \sqrt{k} while the communications per iteration are at most \logm/(1-\rho). The experiments on real-world datasets and simulation studies verify the effectiveness of the proposed algorithms. This paper has the following contributions: This paper proposes distributed algorithm for high-dimensional estimation to reduce the computational requirements, which decentralizes Nesterov’s accelerated proximal gradient via consensus dynamics and a gradient tracking mechanism to track locally the gradient of f. In theoretical analysis, under standard notion of restricted strong convexity and smoothness of the empirical loss, the iterates generated by the proposed scheme converge at linear rate to a limit point. The numerical experiments on simulated and real data show the effectiveness of the proposed algorithm. The computational complexity is also an important part in algorithms. In this paper, it will be better to analyze the complexity (time, space, communications) of the algorithm from theoretical analysis and experiments.The research field of this paper is significant. The organization and presentation of this paper can be further improved. This paper is quite hard to follow. There is no conclusion section and organization section. This paper describes the scope of application of the proposed methods in detail, and does not involve negative social impact.  |||| rating: 5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	The paper proposes an accelerated algorithm for distributed sparse regression in high dimensions. They combine accelerated Nesterov’s proximal gradient with consensus and gradient tracking mechanisms.They show it converges globally at a linear rate, achieving both optimal iteration complexity and communication complexity under the first-order oracle. Originality: The problem setting is quite different from many previous results. The paper considers the high-dimension setting where the ambient dimension $d$ could be much larger than the number of samples $N$, failing many previous algorithms. Though the proposed solution is a combination of two popular techniques, the analysis is new. The analysis setting follows from [1] but has been modified to adapt to the distributed setting.Quality: The paper is technically sound with all theoretical claims supported. The experiment setting is clearly described.Clarity: The paper is overall well-written and easy to follow. Significance: The paper answers an important question in distributed learning and fills the blank of methods and theories in sparse distributed learning in high dimensions. It might be important. See the questions.  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
lkQ7meEa-qv	Learning Neural Acoustic Fields	long	Modeling room and scene acoustics is vital for AR and VR problems. Realistic rendering of spatialized audio is important. This paper builds a representation learning model/schema for parametrizing such acoustic responses via a encoder-decoder style deep network interpolation machine. This is a neat idea. The paper is written well and the benefits of the proposal are explored.  Strengths: 1. Clearly motivated for NN modeling. Well written and presented. 2. Reasonable to expect the benefits of NeRF to extend to acoustic fields. Main weakness: Room and Scene acoustics is a well studied area in Audio. In particular, interpolation of room responses with and without geometry modeling including accounting for diffraction and dispersion patterns around object boundaries is being actively studied in signal processing and acoustics. Although the authors do build the related works, there are still relevant baselines that are missing (both from discussions and from evaluations perspective) like https://ieeexplore.ieee.org/document/7987742 and others. This is not to say that the evaluations are invalid, but more to understand what the network is really learning compared to classical approaches. Additionally a bulk of the content in Section 3 introing the acoustic functions can be reduced and cited to appropriate references. Saving much real-estate.  Yes. This was addressed.   |||| rating: 5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	Disclosure: I reviewed a previous version of this paper submitted to another conference earlier this year. This paper introduces the concept of Neural Acoustic Fields, inspired by the recent developments in Neural Radiance Fields (NeRF) in the context of visual scene generations from a few still images. The idea is, for a given complex acoustic environment, to learn using a neural network a function that maps a pair of 3D (emitter, receiver) locations, a receiver orientation, a receiver channel (left or right) and a time-frequency point (t,f) to the corresponding log-magnitude and phase spectrogram of the room impulse response (RIR) from that emitter to that receiver at that (t,f) point. The function is learned using a balanced least square loss in the log-magnitude and phase domains and positions are represented by a sinusoidal embedding as well as learned geometrical feature maps. ## Strength- The proposed approach is sound both in terms of the underlying acoustical model and in terms of the neural network design and training process, and the general idea of NAFs is novel, interesting, and potentially impactful.- The ability of the method to correctly estimate log-magnitude time-frequency responses and reverberation times at any point in different complex environments across a synthetic and a real dataset is convincingly demonstrated.- A number of interesting experiments reveal that the proposed model learns geometrical structure that could be used for other purposes.## Weakness- An important weakness of the paper is the evaluation (See "limitations" for more details).- The paper lacks clarity as well as many important details to make it self-contained and reproducible (See "limitations" for more details). ## Phase and binaural renderingThe main limitation of the paper is the evaluation. Indeed, the only two objective metrics used to evaluate generate RIRs are insufficient to validate the central claim made by the authors in the abstract, namely: "continuous nature of NAFs enables us to *render spatial acoustics* for a listener at arbitrary locations".The first metric is the RT60 error, which is a single scalar parameters that globally captures the reverberation time but ignores fine properties of the reverberation which typically vary across frequency. The second metric used is the mean squared error on the log-magnitude spectrogram of the RIR. This metric completely ignores the phase reconstruction. Though, correctly reconstructing the phase is known to be of crucial importance for rendering reverberation, and even more so for rendering spatial effects, in binaural settings. Indeed, the primary cue for sound source localization are phase differences across frequencies, which need to be very accurately rendered. The authors claim that their model is able to render multiple orientations of a binaural listener. However, the main notable effect of rotating a listener on the spot for a fixed source in a fixed environment will be on the phase differences between the two channels. The authors to propose to predict the phase via an instantaneous frequency representation, but the reconstructed phases and phase differences are never evaluation.In short, the evaluation is currently insufficient to support the implicit strong claim that the proposed method is able to "render spatial acoustics" in the binaural settings. They merely convince that the method is able to accurately render *monaural* reverberation, which is a significantly easier (though highly non-trivial) task. This strong limitation is not properly addressed anywhere in the manuscript.## Clarity and precisionThe writing is unprecise and handwavy at a number of places. It gives the general impression that the main background of the authors is in NeRF rather than in acoustics/audio signal processing.- L44: "an intractable number of rays are necessary"  The term "intractable" is not precise enough here. Ray tracing is a widely used technique for acoustic simulation, for instance it was used to generate the SoundScape dataset [25] used by the authors themselves for training their method. In that paper, 200 rays are emitted from each source location. Is that considered an intractable number?- L47 "NAFs encode and represent an impulse-response in the Fourier frequency domain." -> actually, in the time-frequency domain, which is quite different.- L60: "The first approach encodes the sound field at a user-centric location by capturing the sound from spatially distributed sources [8,9,10,11]". It is not clear what the authors mean here by "capturing the sound from spatially distributed sources": capturing in what sense? How do those methods specifically differ from the proposed approach? Curiously, the 4 references given seem completely unrelated to each other and span 50 years of literature. The most recent one [11], from 2021, actually seems highly related to what the authors propose, and some more in-depth comparison with this work should be made in the paper.- L113 "Directly outputting the impulse-response v magnitude" -> this seems incorrect, the term "magnitude" should probably be removed.- L119 "due to the smooth nature of the frequency space" -> the authors probably mean "time-frequency space".- L122 "For phase, we use the instantaneous frequency (IF) representation proposed in [29]." : more details on the phase representation used should be provided to make the paper self-contained / reproducible. In particular, one cannot directly reconstruct the phase from the instantaneous frequency, since it is by definition the time derivative of the phase. Hence, a global phase is missing at each frequency. It is not clear how this is handled by the authors.- L147-150 on local geometric features: It is hard to understand what the authors are talking about here without reading in details the supplementary material. Would be good to make this part more self-explanatory.- The authors compare an approach using a "shared" feature grid vs. a "dual" feature grid, the former outperforming the latter. However, in the supplementary material (fig. A4), only the network architecture for a dual grid is presented, which is confusing.- Section 4.1: when presenting the two datasets, the authors omit to discuss their limitations, although both are in fact quite limited and do not allow testing the proposed approach in its full generality. The first one only employs 2D grids while the second one is monaural.- L217: "The bitrates are chosen to approach the storage costs for our NAF": This is not precise enough to understand how the baselines were implemented exactly. The authors should report results as a function of bitrates for the different methods to get a clearer picture of the benefit of the approach compared to standard techniques. Moreover, Table 3 seems to contradict this statement: here we see that the space consumption of NAF is vastly inferior to that of AAC and Opus.## Typos- L44: "an intractable number of rays are necessary to" -> is necessary.- L179 "on 6 representative scenes. Where" -> remove full stop- L212 "as well as using have" -> incorrect phrasing.  |||| rating: 8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations. |||| confidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.	This paper describes a NeRF-like representation for binaural room impulse responses in a given acoustic space. It is an implicit representation of listener position, orientation, and ear, as well as source position, time, and frequency that outputs a scalar magnitude and phase. Experiments on the Soundspaces and MeshRIR datasets show that it is able to reproduce realistic effects in loudness maps like sonic shadows from doorways for irregularly-shaped rooms that would otherwise require expensive ray tracing for impulse response simulation. The model's predictions are compared to those of Opus and AAC and shown to be both smaller in space as well as more accurate in terms of T60 estimation and log spectrogram MSE. An ablation shows that a geometric encoding layer that is shared between listener and emitter works better than having separate or no encoding layer. Use of the proposed representation with NeRF for multi-modal representation improves the visual representation when training images are sparse. A further experiment shows that the distance to the nearest wall can be predicted more accurately from the learned representation than from MFCCs by a linear layer. Strengths:* Novelty and timeliness: NeRF is very popular and while several papers have conditioned a visual NeRF model on audio of various kinds, this is the first, to my knowledge, that provides a meaningful application of the same idea of implicit scene encoding in an audio context.* High quality experiments: The experiments are well conducted and thorough. Useful ablations were conducted which provide non-trivial insights, i.e., that a shared geometric encoding works better than having separate encodings for source and listener.* Clarity: The paper is very well written and easy to follow* Supplementary material: The supplementary material is useful and informative without being overwhelming.Weaknesses:* A few missing details: the paper makes very efficient use of its space, but seems to leave out a few crucial details: the actual architecture of the model being trained, how exactly AAC and Opus were used to encode these libraries of impulse responses, and how the loudness maps are calculated. The limitations section captures salient limitations of the approach and experiments.  |||| rating: 8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper proposed a model for learning to interpolate room impulse responses given room geometry and emitter/listening position.Experiments on two datasets demonstrate that the proposed model can encode room geometry information, and that the learned representation can assist in cross-modal learning. Strengths:The paper is clear and well-written.  The method appears to work well.  Weaknesses:The quantitative evaluations (eg Table 1) look good, but there is no meaningful qualitative evaluation of the method's ability to interpolate RIRs.The raw numbers are difficult to interpret, even for knowledgable practitioners, let alone the broader machine learning audience. There is no mention of societal impact.  The checklist response claims this will be included in a supplementary document.Technical limitations are addressed.  |||| rating: 8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
-YCAwPdyPKw	A Bayesian-Symbolic Approach to Learning and Reasoning for Intuitive Physics	long	The paper addresses the problem of sample-efficient inference for symbolic physical rules. In the literature, there exists neural-network based models for learning a physical engine which have good predictive accuracy but poor sample efficiency, as well as symbolic models which are highly sensitive to deviations from their fixed physics engine. To be able to overcome issues as such, authors propose a generative model along with a symbolic regression framework, in which forces are produced from a probabilistic context free grammar that is designed to mimic simple Newtonian physics. This particular grammar is parameterized by a few latent variables related to unobserved properties of the physical environment, such as mass and charge. Finally, they develop an Expectation-Maximization algorithm, in order for estimating these latent variables as well as inferring the underlying physical laws of the system.The methodology described in the paper enables prior physical knowledge to be incorporated into the statistical machine learning models in the form of a probabilistic context free grammar, which in return makes inference via EM applicable. The paper is well written in general, i.e. the main idea of the paper is easy to grasp and all the related technical concepts are explained in a very simple way. Mathematical notation is clear and consistent. Proposed methodology is novel and sound. Claims of the authors are supported by the experimental results on simulated datasets, and there seems to be no fallacy in empirical evaluations. Nevertheless, experimental section can be extended by large-scale applications or real world datasets. In other words, learning the underlying physical rules from a data set which is collected through real-world sensors would provide further evidence to the impact of such a framework. In addition, I think it’s necessary to include more results from symbolic and neural-network based learned models as a reference. Although relation to prior work is clearly addressed in the paper, authors compare their results to only OGN. In my opinion, learning symbolic rules for physical reasoning and prediction in a sample-efficient manner will certainly be of interest to the ICLR community. I believe that the proposed methodology is clearly worth exploring, and the presented experimental results are promising. On the other hand, it is no surprise that a carefully constructed grammar for physics is sample-efficient. Authors chose to restrict the grammar with a small set of production rules, in order to narrow down the search space for deriving force terms. However, this can be also seen as a compromise from the generality of the model, since such a predefined grammar might be too restrictive and it might necessitate adding new rules for each new physical phenomena. For instance, in the fields related to optics or thermodynamics, to be able to model phenomena such as refraction-diffraction of light or heat transfer, one might need to modify the grammar manually. Another drawback of the methodology could be enforcing position and velocity vectors to reside in the Cartesian coordinate system. Such a choice limits the expressiveness of the model, for instance allowing a general coordinate system could make it easier to model harmonic movement via angular coordinates. In short, my major concern is that enforcing too many constraints inspired by the physics rules we already know introduces a bias, which in return affects the generalization of the model. My final remark is about the EM algorithm in the paper. I think that the details about the EM should be written more clearly and thoroughly. I recommend writing the generative model explicitly which could improve the clarity of the paper in general, as well as allowing different inference algorithms to be applicable. |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The paper proposes a fully Bayesian approach to symbolic intuitive physics that, by combining symbolic learning of physical force laws and statistical learning of unobserved properties of objects. The paper proposes an EM-based method where in E-step object properties distribution are sampled using the current best estimated force laws and in M-step symbolic regression is used to update those laws.The paper is clearly-written with clear explanation of the proposed EM-style method. However, one of the main claim that the method "enjoys the sample efficiency of symbolic methods with the ac- curacy and generalization of data-driven learned approaches" is not well-supported in the experiments. While in Sec. 4.1 it shows the proposed method is more data efficient than the neural baseline, it's not clear how the method generalized to complete different scenarios. Ideally the symbolic physical laws are universal thus can be applied to all physical interactions.Some other questions / comments:- One of the contribution states "Through empirical evaluations, we demonstrate that the BSP approach reaches human-like sample efficiency, often just requiring 1 to 5 observations to learn the exact force laws – usually more than 10x fewer than that of the closest neural alternatives." This (10x more data-efficient) is hard to tell from figure 5 as it doesn't show when the neural baseline reaches the same performance.- The error bars in figure 5 is somewhat not very indicative of the stability of the method and some of them for the neural baseline are extremely large. It states that  the values are "out of five experiments with different shuffling of the training set". How many different shuffling is there?- Sec 3.3.1 "In practice, as the cross-entropy method itself is sensitive to random initializations, in order to ro- bustify the M-step, we repeat it for r runs and pick the best optimizer." Is 'm' supposed to be r in Algorithm 2? - For figure 6, if it converges after iteration 3, can we show the expression tree for F3 instead F5? How much do they differ?  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	This paper proposes a fully Bayesian approach to learn an intuitive physics model by combining symbolic regression and statistical learning (MCMC). Pros:+ Personally, I like this paper as it approaches the learning of the intuitive physics model in the right direction, highlighted in figure 1. It is a nice combination of symbolic and learning-based approach.Cons:- The related work section is too brief to see the main difference between the present work and prior work. The current related work section only focuses on the machine learning-based intuitive physics model but does not cover symbolic regression in general. Using symbolic regression has a long history, especially in material science, soft robotics, and machine learning in general. Prior work has demonstrated that SR can indeed learn the physical law in a much more complex setting [1-2].- For researchers who are opposed to the idea of intuitive physics, they would ask where the prior knowledge comes from. Some cognitive research has shown that they are innate for humans and some animals. But this rule cannot be applied for a machine: If we give them all the rules, why not just directly use the physics engine? In the experiments, the results demonstrated here are too simple so that most of the naive physics-based simulator can produce similar results at a probably faster speed. The question is, what the benefit of learning? One may argue that if I know some properties individually, I might be able to transfer this knowledge to unseen scenarios. The authors do demonstrate some capability in 3.3.2 and 4.2, but it seems to come naturally with SR based on the given prior, not something new or surprise. Such capabilities have been demonstrated in [1-2].- I would like to see if the authors would be able to demonstrate the learned physics can be generalized to scenarios beyond the training datasets. For instance, can the learned model from MAT dataset be transferred to simple scenarios created by bullet-like engine with a similar friction-based interaction, but not identical? The learned physical knowledge should be general enough. Even not as the perfectly correct Newtonian physics, it should be able to generalize to unseen scenarios.[1] Distilling free-form natural laws from experimental data, Science 2009[2] AI Feynman: A physics-inspired method for symbolic regression, Science Advance 2020 |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The paper proposes an Bayesian-symbolic physics (BSP), an intuitive physics model that jointly infers symbolic force laws and object properties (mass, friction coefficient). The inductive bias is force summation, F=ma, and a grammar of force laws to express object interactions. The inference is done via an EM method that alternates between object property estimation (E-step) and force law induction (M-step), using techniques like symbolic regression and Hamiltonian Monte Carlo (HMC). Some preliminary experiments are shown for the method's effectiveness and data efficiency. **Strength**: The paper fills an important missing position in the spectrum of intuitive physics models, as Figure 1 argues. The force law grammar, to my knowledge, is something novel in this area, and represents a reasonable inductive bias that balances expressivity and physical plausibility (with two further physical constraints: dimensional analysis and reference invariance). The grammar also helps improve data efficiency. From the inference side, the proposed EM approach is also reasonable. Symbolic regression for force law inference can be interesting for future research.**Weakness**:Experiments seem a bit weak for now, and I have some technical concerns about the inference. **Questions/suggestions**:1. Learning with unobserved properties (Figure 6) is the key experiment setting according to the paper's selling point, but is obviously lacking. It'd be great to see more scenarios (than graviton) with more diverse settings (e.g. object mass, initial position), some quantitive numbers, and comparison with some baseline (if possible). 2. For some qualitative sense, would also be nice to show true trajectories vs. predicted trajectories in the main paper, or if the learned symbolic law force is correct (for Figure 5). Would be interesting to see if any symbolic laws are predicted wrong and how they look like.3. For object property inference, why use Monte Carlo methods over variational/gradient-based methods? Also, is it possible to use MCMC for formula inference? Some explanations or experiments would solidify design choices for the inference part.4. Is there an ambiguity when force constants and object properties are jointly inferred? How tricky is designing priors for these? (I see some tricks are used in the paper to handle small constants, for example)5. The paper currently only compares with one side of the spectrum, i.e. more neural approaches. Comparing BSP with more symbolic approaches like (Smith et al., 2019), the strength is supposed to be the ability to handle non-standard object interactions outside fixed physics engine but within the force grammar. Is it possible to generate some random force laws and show BSP still works while baselines from BOTH sides of the spectrum may fail? It could be the most powerful experiment to support this position in the spectrum.In general I like the ideas and hope to see more experiments and justifications for design choices. |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
ajH17-Pb43A	AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning	long	This paper evaluates a gradient-based subset selection method in hyperparameter optimization and shows that the method can speed up the hyperparameter optimization process without significant performance deterioration compared to when using the entire dataset. The proposed method, AUTOMATA, consists of three components: hyperparameter search algorithm, data subset selection method, and hyperparameter scheduler. Each component of the proposed method can be selected from existing algorithms. The effectiveness of AUTOMATA is experimentally verified on datasets of text, computer vision, and tabular domain. The performance of  AUTOMATA is experimentally verified. * Strengths: The effectiveness of the data subset selection method in hyperparameter optimization is experimentally validated, and the proposed AUTOMATA succeeds in speedups of 3 to 30 times for several datasets.* Weaknesses: The approach is somewhat straightforward. The novelty of this paper is using the gradient-based data subset selection method in hyperparameter optimization. It is quite natural that subset selection methods are also useful in hyperparameter optimization because existing data subset selection methods succeed in the cost reduction of a single training process. The limitations and potential negative societal impact are addressed.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper presents AUTOMATA, a gradient-based subset selection framework for hyper-parameter tuning. The authors evaluate the framework using various datasets and models. The experimental results show that the proposed framework can improve the speedup of parameter tuning significantly.   The method proposed by the authors are very interesting.Originality:The novelty of this work limited. The design is more about a combination of some existing techniques.  Quality:- The writing can be improved. There is no explanation and justification on some design decisions. Please refer to Questions section for more details. Clarity:- It will be helpful to provide more justification to the design. Why those three components are crucial? - The main text should be self-contained. - In Section 2.3.1, there is no theoretical proof or experiment to support idea of gradient selection. - Better to informative component names instead of Component-1, 2, and 3.Significance:- This speedup-accuracy trade-off can be useful for other follow up works.   The authors have addressed the limitations of their works.   |||| rating: 4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This work proposed a framework to speed up HPO by gradient based dataset subset selection. The methodology contribution lies on the gradient based per batch subset selection using OMP. It seems to be a modification of  GARD-MATCH [23]. The authors provided extensive empirical results on different HPO search algorithms and HPO schedule algorithms, and in combination of different core set selection methods. The results suggested that one could speed up HPO significantly with the framework while keeping the final solution quality (mild decreasing). The paper’s empirical work is very well done. It contains clear experiments setting, abundant real world examples, thoughtful baselines, detailed ablation studies and interesting analysis. The results are also encouraging. I like the presentation, the paper is well organized and easy to follow.On the weakness side, the methodology contribution seems limited and the main merits of the paper are the empirical results. Also, the trade off between speed up and performance loss is hard to quantify in advance and it will be hard to choose a subset size in practice without trying a few options. In the end, the results seem sensitive to the HPO optimizers (the performance loss could differ significantly). The authors have mentioned that choosing a subset size in advance can be difficult and it is hard to anticipate the loss of performance. I would also add how to make it more robust across different HPO optimizers.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	The paper presents AUTOMATA, a framework for hyper parameter tuning that relies on three main components for accelerating the search for the optimal hyper-parameter configuration. Automata was developed with the goal of accelerating the search for the optimal configuration in order to minimize the cost and environmental impact of the hyper-parameter tuning process. The key mechanism described by the authors as the main contributor to the savings and speed-ups achieved by Automata is DSS, a data subset selection approach that aims to find a subset of data with which to evaluate each configuration such that: i) it is faster to evaluate the quality of a configuration; ii) the performance order of the evaluated configurations when evaluated with the full training set is preserved. **Originality:** the tasks and the methods are not new but the work provides a combination of well-known techniques which, as far as I know, is novel. It is not clear how DSS differs from Grad-Match [23].**Quality:** The evaluation could be improved by providing more experiments on the order of the hyper-parameters. If DSS differs from Grad-Match, why is Grad-Match not a baseline?**Clarity:** Overall, the paper is clear and well written. Some parts could be improved. First, if Automata is a framework, then more emphasis should be given to its components and to its use. For example, if I wanted to use Automata for my work: i) which components could I reuse? ii) which components would I need to replace? iii) what should I be careful about, for instance regarding integration of the 3 main components (search algorithm, DSS, and scheduler)? Second, the meaning of "retaining the ordering of hyper-parameters" is not clearly explained anywhere in the paper and when the authors mention that final accuracy is not their main concern (line 74) confuses the reader, since in the hyper parameter tuning context, improving accuracy is a primary concern. Third, it was not clear to me if variable 'k' was the same throughout section 3 nor what its meaning was. Figure 2 currently does not contribute much to understanding the paper. Suggestions on how to improve this are below. Finally, section 3.2 would read much better if there were more paragraphs, perhaps split per domain, like in section 3.4 (text datasets, image datasets, tabular datasets).**Significance:** the idea of having a framework that contains different search algorithms, data subset selection strategies, and schedulers that can be mixed and matched to speed-up the hyper-parameter tuning for different applications and domains is interesting and I can see it being used by practitioners/researchers.    Limitations regarding any potential negative societal impact of the work were addressed.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
hUAmiQCeUGm	Improving Tail Label Prediction for Extreme Multi-label Learning	long	The paper presents a method for improving tail-label performance in extreme multi-label learning setup where the number of target labels can be extremely large. It is based on the finding that the distribution of the norms of the learnt weight vectors also follows a power-law as does the distribution of the samples among labels. The main contribution of the paper is proposing methods for re-ranking which encourages precedence of tail-labels and a data augmentation mechanism. It achieves improvements when applied to sota methods on relevant PSP metrics.Some of the concerns regarding the paper are :- The approach overall seems more like an ad-hoc post-processing step rather than a learning algorithm. It is possible that the impact of RankNet proposed in section 3.2 can be achieved in a more simple way of reranking scores. In the code provided, it was not clear where RankNet as described in section 3.2 was implemented. - The theorem 1 seems incorrect. The probability model is not completely specified as it is not clear what exactly is meant by the test point being randomly sampled. Is it uniformly at random (as seems to be from the proof) or from the distribution that is same as the training distribution (as the typical i.i.d assumption in ML). Also, it seems to compute expectation of some event {y_j \in \beta^{k}}, which is strange as expectations can be computed only of random variables. Overall, the statement of the theorem seems quite vague and imprecise. There are some notational issues also, the W, and w symbols in the theorem don't match the preceding text.- In terms of the experimental results, it is not clear what happens with vanilla p@k and nDCG@k. Even though it is mentioned on page 6 para2 that the these metrics are computed but these are not given anywhere. Also, the Table 4 does not seem to be of much consequence as the re-ranking method can be potentially be applied to all the competing methods.- Other minor comments - the references are improperly given. In some places abbreviations are used for conference names, and in others full names are given. In many places, arxiv versions of the papers are mentioned, even though the corresponding papers are published with conferences/journals. |||| rating: 4: Ok but not good enough - rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	Summary:=======In prediction problems with millions of labels also known as Extreme Multi-label Learning (XML) problems, e.g., recommender systems, the model predictions are not as good for the tail (rarer) labels. This paper proposes two models for this problem. The first model is re-ranking-based, that is, it reranks the prediction scores of a standard XML model. The second model tries to augment the rarer labels to reduce the skew in data. Results shown on several real-world datasets highlight the superior predictive ability of the proposed reranking model for tail labels compared to a host of competitive baselines.Comments:==========The paper solves an important problem which has several industrial applications of extreme multi-label learning. The proposed methods are novel, perhaps less so to someone who is an expert in XML. The experimental evaluation is highly impressive. Both the proposed methods outperform a host of highly competitive baselines on a variety of datasets by significant margins. However, I have a couple of concerns regarding the proposed methods:1). The RANKNET method which re-ranks the XML model's predictions needs to be compared against a baseline which also performs re-ranking for an apples-to-apples comparison in Table 2. Sure, the improvements due to reranking (vs no-reranking) are impressive, but how would a simple re-ranking approach which is not population-aware perform? How is the lambda chosen? By CV? Since you can stack RankNet modules to make it deep, how many were used for results in Table 2? How sensitive are the results to the number of modules?2). The data augmentation for the tail labels seems arbitrary. Why only Input dropout and Input swap? Also, it is unclear how one should split the data between head and tail labels? More importantly, how are the model scores for head and tail labels integrated to make a final prediction?    |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper considers the setting of extreme multi-label classification, where labels typically follow a power-law distribution with many infrequently-observed labels (so-called tail labels). In this setting it often happens that multi-label classifiers more often predict frequent labels as positive than infrequent labels. In practical applications this is not always wanted, and the authors present a new algorithm that favors tail labels over frequent labels. To this end, a specific ranking-based loss function that consists of two parts is minimized. The first part of the loss ranks positive tail labels higher than positive frequent labels. The second part is more standard, and ranks positive labels higher than negative labels. Improving predictions for tail labels is an interesting research goal that has not been thoroughly addressed in the literature, but I am not convinced of the theoretical results and the introduced algorithm. Theorem 1 does not hold because an important condition is missing. The theorem would only hold if w_j^T x > 0 for all x. However, in practice, such a condition cannot be guaranteed. The formulation of the theorem is more difficult than needed, but what the authors want to say is the following: "P(y_j|x) is a monotonically increasing function of the norm of w_j". The proof that is found in the appendix cannot be correct because one can easily construct a counterexample when w_j^T x < 0, and the proof is also more complicated than needed. In fact P(y_j|x) is just a transformation of w_j^T x via a monotone function g with [0,1] as codomain. Useful choices for g are the logit or probit link, but not an exponential function (as stated in the proof). With this insight one can easily see that, when w_j^T x < 0, the probability P(y_j|x) will decrease when for example all coefficient in w are multiplied with a factor two. In that case the norm of w_j all increases, and we have a counterexample for the theorem. To my opinion, the proof makes a few very strange constructions, but I cannot immediately see where the mistake is. I also do not understand why the link function is only introduced in the appendix, because it is a key concept to link w_j^T x and P(y_j|x). To increase readability, I would advise to discuss this early in Section 2. I also do not understand what w_j represents in the case of tree-based models. More discussion is needed. For tree-based models, one doesn't have a weight vector per class, isn't it? I am also not convinced of the algorithm that is introduced in Section 3.2. The method is very ad-hoc, without any theoretical justification. As a result of pairwise terms, it might also be computationally challenging to optimize the proposed loss for extreme multi-label datasets. Isn't there a much simpler solution? Using the terminology of Section 2.1, one could simply improve the performance for tail labels by adjusting the threshold t for such labels only. Has such a simple solution been considered in literature? In that way one could fit standard probabilistic classifiers during training, following by a reasoning on probabilities in a post-training procedure. Similar to the approach of the authors, one could take label frequencies into account during this post-training procedure, resulting in a threshold t that depends on label frequency.  In the experiments it is not clear to me why only four XML datasets are used. Why were the other datasets in the XML repository not analyzed? Please provide a good motivation or analyze all datasets.  |||| rating: 3: Clear rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
fGF8qAqpXXG	Vector-output ReLU Neural Network Problems are Copositive Programs: Convex Analysis of Two Layer Networks and Polynomial-time Algorithms	long	The draft is a vector extension of [1] on studying how to approximately solve the global optima of a two-layered Relu network. The key of the analysis is to enumerate all possible sign patterns of the ReLU unit generating from specific data. Once we have the enumeration, we can also enumerate the linear area separated by ReLU, and the whole optimization problem will become a non-convex quadratic optimization problem. The non-convex quadratic can be approximately solved with its convex dual (or exactly under some conditions), or we can relax it to a copositive program (which might still be NP-hard to solve). With some assumption on the data, the sign pattern of the ReLU is a singleton, then we will have efficient algorithms to exactly recover the global optima of the two-layered network.I like the idea of linking 2-layer NN with copositive programs, the writing is good, and the proofs seem to be correct.However, I feel that the study on the whiten case with a singleton ReLU sign pattern is really not useful. When all the sign patterns of the ReLU are pinned, the representation power of a two-layered network reduces to a linear model (in a polyhedron). And if the representation power is equivalent to a linear model, why not reparameterize it linearly? The specific case lost all the existential meaning for a ReLU unit. (prove me wrong by showing it is better than linear in experiment.) Overall, this is a good extension of [1], but the improvement in complexity only works in trivial case.Minor issues:1. (Algorithm 1, (b)) The uj gj is undefined. The sj^(k) is not used anywhere.2. (equation at the bottom of p.18) Misisng sqrt{}3. Compleity below Section 4.2: enumeration takes O(n^r), but solving a copositive program is still NP.4. (Algorithm 1) It takes me a while to realize how the Frank-Wolfe algorithm works for the $\sum_i|Vi|\leq t$ constraint. Maybe it can be more clear.5. (Writing) There are many reference to appendix without specific section number... Please add them.[1] Neural Networks are Convex Regularizers: Exact Polynomial-time Convex Optimization Formulations for Two-layer Networks. Mert Pilanci and Tolga Ergen |||| rating: 7: Good paper, accept |||| confidence: 2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper	## SummaryThe paper proposes a convex formulation for shallow neural networks with one hidden layer and vectorial outputs. This is an extension on a line of previous works (Ergen & Pilanci, 2020a) and (Ergen & Pilanci, 2020b) where similar results have been established for the case of scalar outputs. A Frank-Wolfe algorithm for finding the global optimum of the resulting convex program is proposed and evaluated on smaller datasets. ## Explanation of RatingOverall, the paper tackles an important problem, is technically sound and well-written. I recommend acceptance of this paper, but have some smaller doubts which are mainly due to the incremental nature of the results (see detailed comment #1) and the strong assumptions for the proposed relaxation to be exact (see comment #2).## Detailed Comments1. My main concern with the paper is, that it is a direct extension of the results from (Ergen & Pilanci, 2020a) and (Ergen & Pilanci, 2020b) to the vectorial setting and therefore lacks novelty. Therefore, I feel the results presented in this paper might be better suited for a journal version of these previous works. 2. The tightness guarantee for the relaxation requires the number of hidden neurons to be larger than the number of data points. In practice, this assumption is often not satisfied. The paper would be stronger if there was some theory (e.g. a duality gap analysis) that quantifies how the quality of the relaxation degrades as the number of data points is increased.   |||| rating: 7: Good paper, accept |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	Summary: This paper generalizes the results of Pilanci and Ergen (2020) showing that the non-convex optimization problem corresponding to the training of a one-hidden-layer muti-output ReLU neworks can be solved using convex programming. In particular they show that the problem has (I) a finite convex bidual that can be solved efficiently using some variant of the Frank-Wolfe algorithm and (II) a convex strong dual given by a copositive program. Unfortunately in the general case the complexity is exponential in the rank of the data matrix. A spike-free assumption is introduced that facilitates things and allows polynomial time algorithms, however this assumption is pretty strong and I doubt it is useful in practice as it makes the training 'almost' equivalent to training a linear classifier. Some references are missing as the problem is related to the low-rank matrix factorization problem. The notation in some critical parts of the paper is not clear and makes reading difficult.I would like to start saying that I am open to increasing my score (**Update: score updated after author feedback**) if the authors are able to clarify notation and add references/discussion, because I think that the essential contributions of this paper are importantPros:1. Quality/Significance: The paper continues the vein of previous convex reformulations of the training of ReLU networks of Pilanci-Ergen (2020), extending the results of the single output case to multi-output which is arguably the interesting case in contemporary applications of Deep Learning. It opens the way to certified global optimality of solutions via convex programming of ReLU networks, and provides insight into the fundamental complexity of the optimization problem in nonconvex (factored) vsconvex form.2. Originality: There are few papers trying to understand the connections between the nonconvex formulation of the training of ReLU networks with convex formulations that provide certified optimality. For this reason I find the paper original although it builds upon previous work exploring this idea. Most papers deal with heuristic or ad-hoc arguments of convergence of GD/SGD in the non-convex formulation with many assumptions that might not hold in practice or that are cumbersome.3. Clarity: The "text" part of the paper is clearly written and the pace seems good. However there are crucial points where I could not understand the notation (see cons).Cons:1. **This has been fixed in an updated version and is now not a problem** Clarity: I could not really understand the optimization problem (9) and (11) because the variable $i$ appears as an optimization variable $\min_{i \in [P]}$ but it does not appear in the optimization objective? In the objective the letter $i$ is used but as an index in the summation $\sum_{i=1}^P$ which makes things really confusing. It appears that $i$ is not really an optimization variable and perhaps what the authors meant is that the constraint in (9) and (11) should read $V_i \in K_i \forall i \in [P]$? at least this is what I would find most natural. This should be clarified and corrected if needed.  2. **This has been fixed in an updated version and is now not a problem** Clarity: It is not so clear how Algorithm 1 follows the Frank-Wolfe template. As I understand an initial value of $t$ is chosen and then the inner minimization problem is solved. Following this a new value of t is chosen and so on. So in the inner minimization problem the constraint set (assuming my understanding in the previous point) is $\sum_{i=1}^P ||V_i|| \leq t$ . Then the step (a) looks like the LMO but it is not clear where is the constraint \sum_{i=1}^P \| V_i \|_ enforced? It looks like the FW update necessarily modifies only one $V_i$ and that the solution can be obtained from the LMO corresponding to only one constraint ||V_i||_ \leq 1 is this the case? I think it is enforced through the constraint $\|u\|_2 \leq 1$.It would be great if the authors can confirm and flesh out the intermediate steps in the appendix, which does not explain much besides the algorithm in the main text. This would make it more accesible.3. **The authors argue in rebuttal that ReLU networks on spike-free data is still different from a linear classifier, some experiments were added** Significance: The Spike-free assumption seems to simplify things but it might be unrealistic. what is even more evident is thatit is somehow equivalent to learning a linear classifier as it implies that $(XU)_+ = XU$ so the network is actually $XUV^T$ so it is equivalent to $XA$ with $A=UV^T$. Under this identification it is well known that the variational formulation of the nuclear normimplies that the regularizer $\|U\|_F^2 + \|V\|_F^2$ is equivalent to the nuclear norm of the unfactored matrix $A$. There are multiple works studying this that should be mentioned and it should be acknowledge that the spike-free assumption is more convenient for simplifying the analysis, rather than realistic (or it should be argued why it is ok to do the assumption).  Also it is not clear in the statement of the theorem what is the relation to spike-free. Does whitened $X$ imply spike-free $X$?4. Significance: Theorem 2 is a simple consequence of this "spike-free" simplified setting and corresponds to the solution of the proximal operator of the nuclear norm.5. Significance: the previous two points suggest that the important results here are those corresponding to general $X$ matrix. Unfortunately in that case the complexity is exponential in the rank of the data, which I guess is just the "real" dimensionality of the data? for example if data is collinear this would be 1 and it is not surprising that the problem would become easier. It looks like the proposed algorithms are currently only of theoretical interest.6. **Authors have answered this in the revision** Experiments: exp 5.1: Perhaps add here the corresponding plots for the 0-1 error? it would be great to understand if the solution obtained with convex programming translates to better misclassification error compared to sgd. Currently how much time does the convex program take to solve? compared to SGDdo you think there is any benefit given that SGD seems to find good solutions in the overparametrized case? 7. **Authors have answered this in the revision** Experiments: exp. 5.2. same as before: what happens with the misclassification error? what stops you from doing the computation on all the data? memory?Does the red cross mean that the soft thresholded SVD is solved in less than one second? References:1. Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix DecompositionRicardo Cabral, Fernando De La Torre, Joao P. Costeira, Alexandre Bernardino; Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2013, pp. 2488-24952. Geometry of Factored Nuclear Norm Regularization. Qiuwei Li, Zhihui Zhu, Gongguo Tang3. Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization. Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo+ many references thereinoverall I would be happy to recommend acceptance if the previous issues were adressed in a succint way.**Update** After author feedback many of my concerns have been addressed, in particular the optimization problemtemplates are much easier to understand now as well as the derivation of the algorithm. Also some important differencesbetween linear classifiers vs ReLU networks on 'Spike-free' data have been clarified. These were my main concerns and thus I am inclined to raise my score.  |||| rating: 7: Good paper, accept |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature	This paper showed that a two-layer vector-output ReLU neural network training problem is equivalent to a finite-dimensional convex copositive program. Based on this connection, the authors gave the first algorithm that finds the global min of the network training problem, which has running time polynomial in the number of samples but exponential in the data matrix. For CNN, the running time is only exponential in the filter size, which is usually a constant. The authors also described circumstances in which the global min can be efficiently found by soft-thresholded SVD; provided a copositive relaxation that is exact for certain cases. The effectiveness of the proposed algorithms is verified in experiments.I think this is a good theory paper that shows an interesting connection between two layer network training and copositive program. My questions/concerns are as follows:1. What’s the role of the regularizer in the current analysis? Does the result extend to the setting where the loss function does not have a regularizer? 2. The current analysis seems to be restricted to two-layer neural networks. I understand the analysis of multiple layer neural networks can be potentially much more difficult. It would be good if the authors can add some discussion on the possibility of extending the analysis to more general multiple-layer nets. 3. The current paper focuses on minimizing the training loss. In practice, people usually use over-parameterized neural networks which has an infinite number of global min for training loss. It’s believed that SGD works well because SGD has an implicit bias which tends to converge to a global min that also generalizes well. So I think it would be good if the authors can check the generalization performance of the proposed new algorithm. 4. Since the algorithm in this paper is limited to a two-layer ReLU neural network, which is rarely used in practice. So I wonder what’s the general implication/message of this paper to the practical ML community.5. A closely related paper [Ge et al 2018] also studies two-layer ReLU neural networks with vector-output. Under the assumption of symmetric distribution and more output dimension than hidden neurons, they gave a tensor decomposition algorithm that can recover the ground truth parameters in polynomial time. It might be good to compare the current paper and their result.https://arxiv.org/abs/1810.06793 |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
eE-S1U5GG94	Principle Components Analysis based frameworks for efficient missing data imputation algorithms	long	This paper proposes a framework based on principle components analysis (PCA) to speed up the missing data imputation. It divides the feature sets into two partitions -- the fully observed one and the one that contains missing values. The proposed method applies PCA to the fully observed partition to do dimensionality reduction, followed by the existing imputation methods. The authors further propose to apply PCA to the imputed data to speed up the downstream classification task. Strengths:- The paper is overall organized and readers can easily follow.Weaknesses:- The major weakness is that the methodological contribution is quite limited. Projecting data into lower dimensional spaces to speed up downstream tasks is not new. In fact, it has been widely regarded as the go-to tool to handle large datasets. Limitations are not discussed in this paper.  |||| rating: 2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	The authors presented two novel frameworks, PCAI and PIC, that allow for more efficient handling of missing data. There are two use cases, fashion MNIST (imaging data) and Parkinson (voice recording) dataset.  Strength: easy to understand and implement. Weakness: The main assumption is that introducing random missing is not acceptable without sufficient evidence and justification.  The main assumption is that introducing random missing is not acceptable unless the author can show that such data suffers from random missing. For instance, clinical data has missing that is not completely random. Please see the paper published in nature digital medicine on this topic. https://www.nature.com/articles/s41746-021-00518-0 Similarly, voice recordings and imaging data suffer likely from non at random missingness due to the nature of the data.   |||| rating: 4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	The authors present a PCA based framework for missing data imputation specially efficient for high dimensional data. The experiments indicate that the imputation presents low MSE and, when applied to classification tasks, results in similar or better accuracy values. The main motivation is relevant and promising. The related work section indicates a well explored field. Nevertheless, the authors should mention and include in the experimental section at least some of the many PCA based approaches to data imputation. See for instance [1,2,3,4] and the references within. Without those comparisons, it is difficult to state the originality of the work.Since the proposed approach relies on classic PCA and can be coupled with any imputation strategy, it seems to be readily available for practitioners. This is a welcomed advantage of the proposal.The writing is mostly sound and the manuscript is well organized. However, some steps of the proposed algorithms are difficult to follow due to the lack of details. I will list them in the next part.References[1] Qu, Li, et al. "PPCA-based missing data imputation for traffic flow volume: A systematical approach." IEEE Transactions on intelligent transportation systems 10.3 (2009): 512-522.[2] Folch-Fortuny, Abel, Francisco Arteaga, and Alberto Ferrer. "PCA model building with missing data: New proposals and a comparative study." Chemometrics and Intelligent Laboratory Systems 146 (2015): 77-88.[3] Folch‐Fortuny, Abel, Francisco Arteaga, and Alberto Ferrer. "Assessment of maximum likelihood PCA missing data imputation." Journal of Chemometrics 30.7 (2016): 386-393.[4] D’Enza, Alfonso Iodice, Francesco Palumbo, and Angelos Markos. "Single Imputation Via Chunk-Wise PCA." Data Analysis and Rationality in a Complex World (2021): 75. The manuscript states sufficiently its limitations.  |||| rating: 4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	The authors propose we propose a framework based on PCA to speed up the imputation process of many available imputation techniques. The proposed methods are named PCAI and PCA-PIC. Experiments are provided on various datasets. Weaknesses:1) The authors only compared PCAI strategy with the traditional strategy.2) "Principle Component Analysis" should be "Principal Component Analysis"Strengths:The proposed methods are named PCAI and PCA-PI are novel. The authors addressee adequately the limitaions of  PCAI and PCA-PI.  |||| rating: 5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
gZAu2NR7X	Self-Supervised Learning with an Information Maximization Criterion	long	This paper presents a self-supervised learning with an information maximization criterion among alternative latent representations of the same input that naturally prevents dimensional collapse. It considers a second-order-statistics based mutual information measure, the log-determinant mutual information (LDMI), which is equivalent to Shannon mutual information under Gaussian distribution. A further first-order approximation to the log-determinant of the sum of two matrices is used to simplify the final objective to a Euclidean distance-based objective function regularized by the log-determinant of the feature covariance matrix. Consequently it  avoids the collapse problem establishes relevance among alternative representations by increasing the linear dependence among them.  Experiments on 4 image datasets show that the proposed approach gives better results than contrastive and non-contrastive methods.   The paper is well written and easy to follow. However, there are several issues I'd like to address. The revised version has cleared my concerns.1. This following statement is misleading: "A common self-supervision task is to match the latent representations that come from the distortions of the same input." It's true in computer vision, but not true in speech or natural language processing. 2. The following statement is not quite right: "maximizing SMI between the representations of the same input is a challenging task whose implementation would require relatively large sample sizes [11, 12]." [11] demonstrates that when mutual information is large,  existing variational lower bounds degrade and exhibits either high bias or high variance. In fact McAllester and Stratos prove that serious statistical limitations are inherent to any lower bound method of measuring mutual information. More specifically, any distribution-free high-confidence lower bound on mutual information estimated from N samples cannot be larger than O(ln N). 3. Collapse is a central concern for SSL with the same input in computer vision.Reference:David McAllester and Karl Stratos, Formal Limitations on the Measurement of Mutual Information, The Twenty Third International Conference on Artificial Intelligence and Statistics, 108:875-884, 2020. Yes  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper extensively addresses the collapse problem by proposing second-order statistics-based mutual information measure that reflects the level of correlation among the inputs. It claims that maximizing this correlative information measure between alternative representations of the same input serves two purposes: (1) it avoids the collapse problem by generating feature vectors with non-degenerate covariances; (2) it establishes relevance among alternative representations by increasing the linear dependence among them. All these are proved to simplify as a regularization term which acts as a natural barrier against feature space degeneracy. In general, the paper is well written and reasonably understandable. I found the underlying theory to be very strong, however the presented experiments aren't sufficient to show the strength of the work. **Strengths**(1) The paper is theoretically grounded on correlative information measure of representation. Additionally, it is very well reasoned, I like the way it is presented.(2) On classification downstream task, the proposed methods has obtained state-of-the-art results one some datasets. Although, I found that the results are not robust and the comparison is not complete. Please also see my experiment related comments in the weaknesses section.**Weaknesses**(1) The experiments shown are bit limited and not very robust. Currently the experiments are only done on one downstream task i.e. classification. More experiments on other tasks, such as object detection, semantic segmentation, few-shot learning, representation transferability etc could have been interesting and could have strengthen the method. Moreover, ablation of the model on different hyperparameters should have been studied to understand the strength of the model. Furthermore, standard experimental strategies are not followed, as an example, for classification tasks, self-supervised models follow "linear" and "fine-tuning" strategies, which is not followed in this work.(2) I wonder if there is any way to estimate the amount of mutual information maximized via the proposed CorInfoMax approach. It would be interesting to see some intermediate results or plots showing the evolution of estimated mutual information as the training progresses. Also it would have been interesting to see analysis on the performance behaviour of the model with evolution of mutual information. Does higher mutual information always guarantee a better trained model? The authors have not mentioned about the limitation of the work in the paper. A possible limitation could be the consideration of only the global information and not considering the local information, which might make this method only applicable for global task, such as classification. I suspect this type of model won't work very well on the task like involving fine-grained feature understanding.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	The paper introduced CorInfoMax, a second order statistics based mutual information method to avoid collapse problem in self supervised learning (SSL). It used log-determinant mutual information (LDMI) as the measure between data pairs with different augmentations. Mathematically, the paper gave step-by-step formulation to derivate the final form of the objective via its first-order Taylor series approximationExperimental results on CIFAR10/CIFAR100/ImageNet gives competitive results. Strengths- This work is novel and well intuited. Collapse issue is a well-known problem in SSL, traditionally Shannon Mutual Information (SMI) is applied to mitigate the collapse issue with several drawbacks, such as large batch requirement. The log-determinant mutual information (LDMI) naturally enforces latent distributions have non-degenerate distributions which can avoid collapse. Therefore, this method is well motivated and makes sense- Clean and neat formula. Step by step formula gives clean loss format in Eq (8). It clearly indicated data under different augmentation should be closed to each other and log determinant formula enforces the model from model collapse. The result is elegent.- This result shows state-of-the-art SSL performance. Weakness- Mathematical formula is too heavy in the paper. It is a bit hard for audience to follow. I would strongly recommend a more detailed inference in any appendix to show how they are coming from- More though experiments should be provided to demonstrate the effectiveness of the approach. For example, the paper argues the drawbacks for SMI and introduce LDMI measure as a better solution to avoid collapse. However, no evidence shown hot does it better avoids collapse.  Yes  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper presents a new self-supervised learning method based on the log-determinant mutual information proposed earlier. Experiments on three small size datasets and one medium size dataset show the effectiveness of the proposed method. Strengths:1. The experimental results are good.2. Overall, the article is easy to read.Weaknesses:1. The experimental part is relatively weak. 2. The novel of this paper is limited. The main contribution of this paper is to directly generalize the earlier proposed log-determinant mutual information to the field of self-supervised learning.3. Section 3 is superfluous. The authors have adequately addressed the limitations and potential negative societal impact of their work.  |||| rating: 4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly. |||| confidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.
BJfRpoA9YX	Adversarial Information Factorization	long	In this paper, the authors introduce a neural network architecture that has three components.First a VAE is used to encode images in to two latent states \hat{y} and \hat{z}, with \hat{z}intended to be class (e.g. face attribute) agnostic. The decoder reconstructs images from \hat{y}and \hat{z} concatenated together. A GAN style discriminator attempts to distinguish the decoded image from the original input image as real or fake, allowing the decoder to produce higher quality decoded images. An auxiliary network A attempts to classify the face attribute yfrom the class agnostic features \hat{z}, with the idea being that the encoder should try to produce \hat{z} vectors from which the class cannot be predicted. An additional classifier is trainedusing a classification loss \hat{L}_{class} on the encoded reconstructed image, the use of which I don't understand.I think additional work on section 2.5 through section 3 would be helpful to improve clarity.As one example, "y" is unnecessarily overloaded: y denotes a specific attribute, \hat{y}denotes a latent vector that is intended to not be class agnostic, \tilde{y} denotes theprediction of an auxiliary network on an intended class-agnostic latent vector \hat{z} ofthe presence of the original attribute y, and \hat{\hat{y}} denotes the non agnostic latentvector achieved by passing the decoded image back through the encoder.This notational complexity is compounded by the fact that a number of steps in the method arenot well motivated in the text, and left to the reader to understand their purpose. For example,the authors state that "we incorporate a classification model into the encoder so that our model mayeasily be used to perform classification tasks." What does this mean? In the diagram (Figure 1),where is this classification model? Why in the GAN loss is there a term that compares thefake loss with the result of classifying a decoded z vector? Is this z \hat{z}, or a latent vectordrawn from a distribution p(z)? If it is the former, how does this term differ from the secondterm in the GAN loss. If it is the latter, then shouldn't it be concatenated with some y in order tobe used as input to the decoder D_{\theta}?Why is it important to extract \hat{\hat{y}} from \hat{x}? In the paper you state that the loss"provides a gradient containing label information to the decoder," but why can't we use the known label yof the original input x to ensure that the encoder and decoder preserve this information if it is used as \hat{y}?Later in the paper, you explicitly state that \hat{\mathcal{L}_{class}} "does not provide any clear benefit."If that is the case, then you should ideally include it neither in the model nor in the paper. If it wasincluded primarily because previous models included it, then I would recommend you introduce its usein a background section on Bao et al., 2017 rather than including it in your model description with anexplanation like "so that our model may easily be used to perform classification tasks."Ultimately, this last point brings us to a good summary of my concerns with the model: the inclusionof too many moving parts, some of which the authors explicitly say later on provide no benefit.Moving on to experimental results, I think this is another area where I have a few concerns. First, inFigure 2, the authors argue that your model is "better for 6 out of 10 attributes" and comparable results for most others. The authors include a gap of 0.1 in the "Gray_hair" category as "better" but label a gap of 0.5in the Black hair category as "comparable." I think results in several of the categories are sufficiently closethat error bars would be necessary to draw actual conclusions. If "better" were to mean "better by 0.5" for example,then the authors method is better on 4 tasks (smiling, blonde hair, heavy makeup, mustache) and worse on 3 (black hair, brown hair, wavy hair).With respect to the actual attribute editing, my main concern here is a lack of comparison to models other than Bao et al., despite the fact that face attribute changing is an exhaustively studied task. A number of papers like Perarnau et al., 2016, Upchurch et al., 2017, Lample et al., 2017 and others study this task from machine learning perspectives, and in some cases can perform photorealistic image attribute editing without complicated machinery on megapixel faceimages. At least the images in Figure 3 and 4 are substantially downsampled from the typical resolution found in the Celeba dataset, suggesting that there was some failure mode on full resolution images.----Edit: I've reviewed the authors' addressing my concerns in their paper and am happy to increase my rating as a result. |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	Summary: This paper builds upon the work of Boa et al (2017 ) (Conditional VAE GAN) to allow attribute manipulation in the synthesis process. In order to disentangle the identity information from the attributes the paper proposes adversarial information factorization : let z be the latent code and y be the attribute the paper proposes to have p(y) =  p(y|z= E_phi(x)), i.e to have z independent of y.  This disentanglement is implemented through a GAN on the variable y  min _phi Distance (p(y), p(y|z)), the distance is defined via a discriminator on y.  Experiments are presented on celeba dataset,  1) on attribute manipulation from smiling to non smiling for example, on 2) attribute classification results are presented , 3) ablation studies are given to study the effect of each component of the model highlighting the effect of the adversarial information factorization. Originality Novelty: There is a large body of work on disentanglement that the paper does not cite or compare to for instance, InfoGAN,  Beta- VAE https://openreview.net/pdf?id=Sy2fzU9gl and disentangled latent concepts https://arxiv.org/pdf/1711.00848.pdfNote that for example that in beta- VAE it is a similar idea where but it is on z and z|x and the distance used is KL (since it is has closed form with gaussian) , min_phi Loss+ beta KL (p(z), p(z|x)), a discussion of the previous related work in the paper is necessary.  The work is also related to MINE https://arxiv.org/pdf/1801.04062.pdf where one would like to minimize the mutual information I(z;y)  this mutual information is estimated through a min/max game. Questions: -  why is RMSprop used for optimization, your model and the Bao et al baseline might benefit from the use of Adam?- (Table 3 in appendix ) Have you tried higher values of alpha the weight of KL, with the model of Bao et al (it is recommended in beta VAE to have high value of what you call alpha)?Overall assessment: The paper novelty is using min/max game to estimate the mutual information between y (attribute) and z (identity code). Disentanglement and use of min/max games for estimating mutual information has been explored before.  Further discussion and comparaison to previous work is needed.   |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper proposed a generative model to learn the representation which can separates the identity of an object from an attribute. Authors extended the autoencoder adversarial by adding an auxiliary network. StrengthThe motivation of adding this auxiliary network, which is to distinguish the information between latent code z and attribute vector y, is clean and clear.Experiments illustrate the advantage of using auxiliary network and demonstrating the role of classify. Experimental results also show the proposed model learning to factor attributes from identity on the face dataset.Weakness The proposed model seem to be unnecessarily complex. For example, the loss of  in (6) actually includes 6 components (5 are from L_enc) and 4~5 tuning hyper-parameters. The L_gan also includes 3 parts. The reason of adding gan loss lacks either theoretical or empirical analysis. So as L_KL. In addition, the second term in L_gan is unnecessary since you already have a reconstruction loss. It also make it to be unclear what we obtain if the equilibrium of the GAN objective achieved.The written of this paper can be improved to make it more clear. It looks \hat_y and \tilde_y are same thing. How do you get \hat_z? Do you assume the posterior distribution is Gaussian and use the reparameterization trick? What are \hat_y and \hat_\hat_y? Are they binary or a scalar between 0 and 1?  How do you generate \hat_x? When generating \hat_x, do you sample \hat_z and \hat_y? If so, how do treat the variance problem of \hat_y?  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
pY9MHwmrymR	An Extensible Benchmark Suite for Learning to Simulate Physical Systems	long	The authors introduce a physical systems evaluation dataset framework that focuses on evaluating machine learning algorithms for simulation problems. They provide four systems of increasing difficulty to evaluate baseline methods (spring, wave, Naive-Strokes and spring mesh), explore the trade-offs between derivative-based prediction and step prediction and advocate the usage of K-nearest-neighbors to better understand the complexity of different simulation tasks. The paper is very concise, easy-to-follow and well-illustrated.The authors do a great job motivating the four representative benchmark physical systems, and provide a comprehensive array of baseline data-driven methods. While neither of these are exhaustive, the flexibility of their framework allows for the integration of other learning tasks or machine learning methods. All in all, their contribution promisingly lays the groundwork for future research in the field of scientific computing While the graphs are typically easy-to-follow,  I was slightly confused by repeated colors in figures 3 and 4. My understanding is that same-color marks are different architectures of the baseline methods. However, different architectures could have significantly different strengths and weaknesses. For instance, shallow MLPs are typically more robust to noisy datasets than deep MLPs. Do you believe that readers would benefit from having more fine-grained labels for the methods (e.g. shallow vs deep MLP/CNN) in figures 3 and 4?Further, due to the non-deterministic nature of some NN-based approaches, it would make sense to average results over multiple runs. In the main paper there doesn't seem to be any indication of this. Were the results presented as averages over multiple runs?  |||| rating: 8: Top 50% of accepted papers, clear accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The present work is motivated by the need for a) thorough evaluation of data-driven approaches in scientific computing pipelines and b) the lack of standardized benchmarks in the literature.   The authors focus on physical simulation benchmarks that a) map a high-dimensional state space into another high-dimensional space (as in temporal integration schemes, mapping the state of the system at one time step to the next, or b) from a high-dimensional input space to a lower-dimensional output (as in surrogate models, mapping the initial conditions to a functional of the solution.  In addition the paper addresses the narrow data regime, where initial conditions are sampled from a low-dimensional manifold (even within a high-dimensional state space), and the wide regime, where initial conditions span a truly high-dimensional space. The main contribution of the paper is the presentation of a suite of simple, representative physics problems along with reference numerical solutions for traditional time integration schemes to benchmark data-driven methods (MLPS, CNNs, kernel machines, Nearest neighbors).  The key conclusion of the paper is that, even in the simplest physical models, currentdata-driven pipelines, while providing qualitatively acceptable solutions, are quantitatively far fromdirectly numerically integrating physical models, and this performance gap appears unfeasible toclose by merely scaling up the models and/or the dataset size.   Another finding is that a simple L2-based nearest neighbor regressor outperforms most deep learning models in the narrow regime for complex systems such as incompressible Navier-Stokes systems.  -- Provides a standard benchmark for contrasting traditional solutions for numerical computation with physics based models against data-driven methods.-- I like the progression of complexity of the benchmarks and the exploration of narrow and wide data regimes. Such a standard benchmark allows for systematic evaluation and tradeoff analysis.-- The experimental results provide sufficient insights -- The code and data is made accessible and is extensible-- No ethical/social implications exist -- Apart from highlighting the deviations in results between data driven and traditional methods, It is not absolutely clear how the benchmark results points to ML algorithm improvements.  (The authors state this as a point of strength of the paper, but it is not obvious what they imply by this).-- While I like the organization of the benchmark in the paper, I would have assumed that a more simpler differential system with well known closed form solution for the result may be an interesting option as a baseline.  -- The paper will be strengthened significantly if other aspects not covered by the authors (described in the limitation sections) are included including the missing timing analysis.    |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper introduced a benchmark for the methods of data-driven-based physical system simulation. They provided a dataset of four physical systems (spring, wave, spring mesh, navier-stokes) including data, data generation methods, codes, and documents. Then, they conducted a variety of experiments using knn, nn kernel, MLPs, and CNNs. This work is interesting, and important for learning physical systems. The descriptions are also detailed. However, the value of this benchmark needs to be more explained, please see Weaknesses. - This is hard work (850h on GPU + 2180h on CPU). It is really difficult to process such a large amount of data and keep it unified and correct.- The data, source codes, and documents are all well organized.- The generation methods, documents, and experiments are described in detail. For a paper of dataset and benchmark, the contributions should be novel/originality data, comparison and analysis of state-of-the-art methods, and the impact on the future of related fields. I hope the authors could give more explanation about these points.- The paper claimed that the proposed benchmark takes a step towards unified evaluation protocols and metrics, but it is unclear.- Data. The complex system is more important for simulation problems. The value of simple system data needs to be explained. The data of four systems are also generated in the existing paper, such as spring in [7], Navier-Stokes in [48]. What is the difference between the provided data and the previous data?- Benchmarks. Only simple baseline models are tested (except for u-net on naiver-stokes), and the paper claim that these models could not solve the problem well. However, advanced models have been proposed and compared with the baseline models in the existing papers. SOTA models should be tested and analyzed.  |||| rating: 7: Good paper, accept |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct
TaARsI_Iio	A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction	long	This paper introduces a Korean language legal NLP dataset, LBOX OPEN, and a Korean legal language model trained on LBOX OPEN, LCUBE, a decoder only model based on a GPT-2 architecture. The dataset includes a legal precedent corpus, two multi-class classification tasks, two legal judgment prediction tasks (with quantized ground-truth predictions) covering criminal and civil cases, and one legal summarization task.The authors show that pretraining on a domain specific corpus result in performance gains across several of the tasks and comparable performance to a model with more capacity. They find that the domain specific pretraining is less helpful for the legal summarization task. These results are consistent with existing results on English language legal datasets and pretraining corpora. The originality comes primarily from the creation and evaluation of legal datasets and pretraining corpora in Korean. There are few pretraining corpora / datasets and legal-domain specific language models for Korean law. This paper introduces a large-scale Korean language legal corpus, different downstream NLP tasks relevant to the legal domain, such as legal judgment prediction, and a pretrained legal-domain language model, which I believe will be valuable in providing more multi-lingual NLP resources for the legal domain. Additionally, the dataset includes tasks over a diverse range of downstream tasks that are of interest to the legal NLP community, including more difficult tasks such as summarization that are often expensive to collect ground-truth labels for.The authors put a significant amount of engineering effort and human validation into extracting structure data from raw data, since most Korean legal documents are available in either non-digitized document image or PDF formats.Details were provided that would make it possible to reproduce pretraining for LCUBE-base and the fine-tuning procedure used for downstream tasks. I thought the experiments removing the facts from input were useful in showing the importance of the facts in the model reasoning about judgment predictions.The data and language model are accessible through HuggingFace. See additional comments in Documentation section on potential improvements to accessibility and accountability. I would have liked to see a little more justification about some of the quantization choices to form classification tasks out of the two legal judgment prediction tasks. It seemed like some different choices were made about maintaining class balance in quantization for the criminal vs. civil legal judgment prediction tasks and it was not entirely clear to me from the experiments whether the results would be affected by the somewhat subjective quantization decisions.It might be helpful to discuss potentially reasons that the summarization task does not see improvements from legal domain-specific pretraining.There is limited discussion of the social and ethical impacts of the dataset. I think two specific points that should be included are (1) the dataset’s license and whether any of the incorporated data from other sources is under other licenses, (2) whether there is potentially personal identifiable information in the legal case corpus and the norms / privacy protections governing Korean legal data released by the government and legal system.  |||| rating: 7: Good paper, accept |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	The authors present a new benchmark for three different types of legal tasks (classification, judgement prediction, and summarization) in Korean. Specifically, they introduce six datasets: (1) a legal precedent corpus consisting of 150k precedents; (2) a multi-class legal text classification dataset consisting of 10,000 facts-category pairs with 100 different case categories; (3) another multi-class legal text classification dataset consisting of 2,760 facts-statutes pairs; (4) a legal judgement prediction dataset consisting of 11,000 cases from 7 categories; (5) another legal judgement prediction dataset consisting of 4,678 pairs of facts and degree of money claim acceptance; and (6) a summarization dataset consisting of 20,000 precedent-summary pairs. They also released a GPT2-like model for the Korean legal language and fine-tuned it on the different datasets. First such large-scale domain-specific benchmark in Korean with good documentation. Existing legal tasks are largely English-oriented. Having legal NLP tasks in a diverse set of languages is important. * Apart from the specificity of the language, the motivation for this new benchmark is not explicit. The authors propose several legal tasks based on the data they were able to collect but do not emphasize the importance of these tasks in the real world and how they could benefit society (apart from the summarization task, perhaps).* As the authors state in Section 3.1, structuring raw data that contains a mix of plain text, tables, and images is a non-trivial task and represents an important part of the work and value provided in the paper. However, little detail is given regarding the different components of the data engineering pipeline (no information about the specific checkpoints or model parameters used for the layout classifier and parser, the OCR framework, and the model used to correct OCR errors). Additionally, code for extracting quality data from the raw documents does not seem publicly released (unlike code for experiments), which is a pity as it would provide excellent value for the research community.* Authors claim that LCube is a legal language model. However, it turns out that the legal corpora only represent 5.5% of the whole pre-training corpus, whereas news and books represent 90% of the pre-training data. I wonder why the authors did not start from a pre-trained KoGPT-2 checkpoint and further pre-trained on their legal corpus only, which has been shown to lead to similar or even better results than pre-training from scratch for a portion of the computing resources — see ClinicalBERT (Huang et al., 2019), BioBERT (Lee et al., 2020), SciBERT (Beltagy et al., 2019).Other nits:* In the Abstract and Introduction, the authors mention some approximative dataset sizes that exceed the actual sizes of the datasets (e.g., 3k pairs for Statute while 8% smaller in reality, 11k pairs for LJP-Criminal while 4.5% smaller in reality, 5k pairs for LJP-Civil while 6.4% smaller in reality). Authors should prefix these approximations with "around"/"~" in order not to mislead the reader about the actual sizes of the provided datasets.* For LJP-Criminal, the authors state that unavailable data (such as the age of defendants) makes the regression task too difficult and therefore formulate it as a classification task by discretizing the outputs. However, there does not seem to have been a preliminary investigation that supports these claims. Also, the authors explain that the boundaries for quantization are chosen in part based on Korean legal aspects and give the example of 1M won as the lower bound amount where public officials can lose their position if found guilty. This would be interesting to not only have one example of such legal aspects but a clear description of all of them for each bound they correspond to. Additionally, authors should show the data distribution related to those boundaries (to make sure that the models do not simply learn to predict the over-represented classes). Same remark for LJP-Civil.* The Results section feels like it was written relatively quickly: it lacks detailed analysis and is poorly organized. The authors performed good control experiments that deserve their own subsection with a deeper analysis of the results. Similarly, a more detailed comparison between KoGPT-2 and LCube is missing.  |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This work proposes a dataset (LBox-Open) containing 150k Korean legal precedents, and proposes two classification tasks, two legal judgment prediction tasks, and a summary generation task based on part of the data in this dataset. And a GPT2 model LCUBE-base is pre-trained using three datasets including LBox-Open. Both the model and the data are open source and can be easily loaded and used through HuggingFace.**Datasets**  Their proposed dataset is the first large-scale Korean-language legal-orientation dataset that combines scale and diversity. Contains 63k cases sentenced in the past four years and 96k cases from the first and second level courts in the fact-checking stage, of which a total of 80k are from LAW OPEN DATA searches and 70k are from their own database, covering a total of 1,160 case categories. They have customized a set of data engines to normalize the data format, and uniformly process the data in the original format of pdf and pictures into the format of text.**Tasks**  Based on a 150k formatted dataset, they propose 5 tasks. All are based on actual business, and four of them are classification tasks, (1). Predict the type of cases based on facts, (2). Predict violations of laws  based on facts, (3). Predict fines, prison months, and labor imprisonment months based on facts. (4) Predict claims acceptance based on facts and gist of claim. One is the generation task, which is to predict summaries based on ruling and reasoning section.**Model**  Based on the three datasets of Wiki, Modu and LBox-Open, they pre-trained a GPT-2 with 124M parameters and compared it with KoGPT-2-base and MT5-small on the above five tasks, providing a task baseline. 1. The dataset covers enough case categories, and the data structure is clear and easy to use.2. The proposed tasks have real business value and solve real business problems.3. This work has a certain role in promoting the application of AI technology in the legal direction. 1. I feel a little weak in the experimental part. It is recommended to add a pre-training of the encoder-decoder language model for comparison.2. On the summary task, the reason why LCUBE is significantly lower than other models can be further analyzed, or some other evaluation metrics of the generation tasks or even some artificial evaluation metrics can be introduced.  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	The authors propose a new legal corpus for the Korean language, with a total of 5 associated tasks: two classification tasks, two legal judgment prediction, and one summarization task. They furthermore release a new language model based on GPT-2 architecture but for Korean legal language.   The corpus released as well as the processing into the various tasks is a new addition and composed of marginally more data than the previous dataset for Korean law from AI-hub, ranging from 11K to 150K data points. The new language model for Korean legal language is also of practical interest.  The whole pipeline, from the preprocessing to the evaluation, lacks rigors and makes me doubt the pertinence of the contribution. More specifically:- The extraction from raw data is produced by ML, with an interconnection of systems and some manually labelled pages. There is no mention of how well each of these systems perform, and no verification on how well they perform together (for example, verification on a 100 randomly selected examples that the extraction was correct). It would also be good to have information such as the threshold for manual labelling, as well as the reasoning behind the choice of that particular threshold.- For the tasks, only a limited range of examples are included ("100 most frequent case categories", "46 frequent case categories", etc). No justification for these choices are provided; these limitations are not needed since all tasks are modelled as a language modelling task which can be trained with any number of categories; the limitations diminishes greatly the interest for the dataset outside academic research; the task doesn't reflect the performance it would have would it be used in practice.- The language model LCUBE-base is evaluated and compare to KoGPT2-base on the new tasks, which are part of the corpus used to train LCUBE-base. Given this, it would be surprising if LCUBE wouldn't outperform KoGPT2-base. While limited, an already better evaluation would be to evaluate on the dataset of AI-hub which should be a legal task but from a different corpus.- One of the argument for this paper is that the dataset from AI-hub is limited in data given that "1.5 million Korean precedents are generated per year". However, the dataset provided is composed of 150K precedents, which doesn't even amount to one year of Korean precedent. The question naturally arises as to why not have a bigger corpus if the data is here and manual labelling is limited in the creation process?- There is no analysis provided which would help underline the performance of the system and how it could be used.   |||| rating: 4: Ok but not good enough - rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The authors present a multi-task legal service benchmark in Korean for language understanding measures over judgment prediction and many other tasks. The authors also build baselines using existing mono-lingual models as well as multi-lingual models and show enough headroom for future research. The main contribution is that the authors contribute a large open-source LBOX OPEN dataset in Korean for evaluating language understanding for legal documents. This is valuable given that legal documents are hard to harvest and annotate by humans. It also contributes to multi-lingual studies over language understanding for legal documents. - The main contribution is that the authors contribute a large open-source LBOX OPEN dataset in Korean for evaluating language understanding for legal documents. - This is valuable given that legal documents are hard to harvest and annotate by humans. - It also contributes to multi-lingual studies over language understanding for legal documents. - Baselines still suggest that a multi-lingual model such as MT5 performs better than in-distribution pretrained LCUBE-base. It is worth exploring even large models such as GPT-2 or GPT-3, if not MT5-base or large, to benchmark existing multi-lingual models with this benchmark. - It would be interesting to see how an English pre-trained language model performs by directly asking it to transfer to do legal documents understanding in another language (assuming we have some hacks in tokenization).- It would be interesting to see more error analyses over these baselines. For instance, what are the cases that MT5 gets wrong in terms of CASE NAME classification? Is this because the document is too long? Or it contain complex descriptions in Korean?  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	Large-scale corpora, benchmark datasets, and language models in the legal domain are lacking. This paper remedies this by introducing this for Korean Law. It introduces Korean legal AI benchmark datasets that consist of six datasets: one large-scale legal precedent corpus, two classification tasks, two legal judgement prediction tasks, and one summarization task. In addition, the a large-scale Korean legal language model is released. As part of the (pre)processing, the authors processed document images and PDFs. The pipeline is presented and explained. The paper contribution consists of the introduction of a large-scale corpora, benchmark datasets, and language models in the domain of Korean Law. Legal datasets are scarce, particularly in different languages than the English language. This dataset offers an important resource for empirical legal researchers and computational researchers who wish to analyze Korean case law. The six datasets that are introduced (and benchmarked) have the potential to provide for a significant breakthrough. The same applies to the Korean legal language model that is released. To my knowledge, it is the first of its sort. It was quite the endeavor to compose the datasets and the model, which makes the construction a significant effort that is unlikely to be done by others.One could argue that a Korean legal dataset has limited relevance to the broader research community. By that standard, the vast majority of non-English datasets can be excluded. As the authors point out, there is a knowledge gap with respect to NLP models (and data) in languages that are not English.The dataset is made available on Github. Although I have not ran the code myself, the documentation seems sufficiently clear and transparent in order to access the datasets. The authors speak about precedents, but it does not become entirely clear what is meant with precedents. As the authors rightfully point out, precedents are cases from the past that have produced a rule that can be used to decide subsequent cases. Many cases are not considered precedents, although on a generic level every court decision can be seen as a precedent. It seems that the author follow the latter approach, essentially equating precedents with court cases. This is fine, but it would be good if the authors would explain this. This is a minor comment.The paper speaks about 6k precedents that were collected as part of approximately 1.5 million Korean precedents in the total population. Maybe I missed it, but I did not encounter possible explanations for the discrepancy. For instance, could selection bias be involved? It remains difficult to determine the generalizability of the dataset.I am not sure what is measured in the 'prediction' part. What is predicted, is the outcome of the decision based on the text that the court produced. This is more likely to be a prediction of how judges motivate their decisions in case of a certain outcome rather than predicting outcomes based on 'the facts'. The results in the 'LCUBE-base + reason (oracle)' model are surprising. As previous research shows, one would expect that the outcomes of court cases can be predicted with high levels of accuracy when including the reasoning of judges. Or did I not correctly understand what was included in the 'reason (oracle)' here?  |||| rating: 9: Top 15% of accepted papers, strong accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
bBff294gqLp	NAS-Bench-Graph: Benchmarking Graph Neural Architecture Search	long	In this paper, the authors propose a benchmark for graph neural architecture search. Based on a tailored search space, they evaluate different search algorithms of existing methods on graph neural architecture search. They further give analyzes of the benchmark in terms of different aspects.This is the first work to design a benchmark for graph neural architecture search and extensive experiments are conducted and comprehensive analysis is given, which can help us better understand the progress of this topic. Further, it can provide a database of GNN architectures for quick look-up, which can accelerate the following-up research of graph neural architecture. Please check the above contribution part. 1. The search space is kind of smaller, and the datasets are only for node-level tasks, thus may limit the usage of the benchmark.2. In Section 4, it will be more interesting if the authors can analyze the influences of differentiable search algorithms beyond RL and EA.3. Some related works are missing, e.g., ### Refs1. Qin et al., Graph Neural Architecture Search Under Distribution Shifts. ICML 20222. Wei et al., Designing the topology of graph neural networks: a novel feature fusion perspective. WebConf 20223. Cai et al., Rethinking graph neural architecture search from message-passing. CVPR 20214. Zhao et al., Search to aggregate neighborhood for graph neural network. ICDE 20215. Want et al., AutoGEL: An Automated Graph Neural Network with Explicit Link Information. NeurIPS 2021  |||| rating: 7: Good paper, accept |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature	This article presents a benchmark for Neural Architecture Search aiming at giving a baseline protocol to efficiently compare different architecture, in a reproducible manner, and a easily usable database for any practician, mimicking the progresses made in other NAS domains.It presents its way of searching an efficient architecture using 7 base and typical Graph Neural Network (GNN) layer, and tests them on 9 open datasets, also well-known and relevant in GNN research, using different approaches (namely evolutionary algorithms and reinforcement learning).Another part presents insights on the distributions for the models, with their performance, how much each was used. Some additional information is provided on the correlation between datasets.  - Following a procedure that has worked in other fields is very sound and good practice - Can be very useful for many practical purposes  - Not completely clear/insisting on the main point of the article which seems to be the benchmarking of different methods, as it seems that there is not much novelty on the different methods of NAS  |||| rating: 7: Good paper, accept |||| confidence: 2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper	The authors propose NAS-Bench-Graph, a tailored benchmark for GraphNAS. The authors trained and evaluated different GNN architectures on nine graph datasets. The authors showed that the proposed benchmark is compatible with existing GraphNAS libraries such as AutoGL and NNI. The studied problem is novel. The authors tried to benchmark GraphNAS algorithms, which haven’t been studied by anyone else.The proposed benchmark is compatible with existing GraphNAS libraries such as AutoGL and NNI. The studied problem is novel, but not important/interesting. Different NAS algorithms might have different performances on different problems/tasks/datasets. Considering the NAS is usually used to find the best model structure for a specific problem/task/dataset, what are the advantages of benchmarking different NAS?The author should compare with GraphGym [1], considering this is one of the most important and most common-used GraphNAS libraries, and a lot of designs in this paper are inspired by GraphGym, e.g., the search space.The authors only conducted experiments on node classification tasks. Link prediction and graph classification are also important tasks in graph learning.The authors just generally compared the “evolutionary algorithm” and “RL algorithm” in the paper. However, there are a lot of specific GraphNAS baselines that the authors ignored and didn’t compare [2, 3, 4].Lack of analysis and insights from the experiments. What are the take-aways from benchmarking different NAS algorithms?[1] You, Jiaxuan, Zhitao Ying, and Jure Leskovec. "Design space for graph neural networks." Advances in Neural Information Processing Systems 33 (2020): 17009-17021.[2] Gao, Yang, et al. "Graphnas: Graph neural architecture search with reinforcement learning." arXiv preprint arXiv:1904.09981 (2019).[3] Zhao, Huan, Quanming Yao, and Weiwei Tu. "Search to aggregate neighborhood for graph neural network." arXiv preprint arXiv:2104.06608 (2021).[4] Shi, Min, et al. "Evolutionary architecture search for graph neural networks." arXiv preprint arXiv:2009.10199 (2020).[5] Zhou, Kaixiong, et al. "Auto-gnn: Neural architecture search of graph neural networks." arXiv preprint arXiv:1909.03184 (2019).  |||| rating: 4: Ok but not good enough - rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	**Edit 08-25**: I increase my score from 7 to 8 as the authors addressed all my concerns and acted upon my suggestions for improvements. I  am also happy about several improvements in response to other reviewers / the AC, especially including a discussion on limitations in the main text. Concerns raised from other reviewers are either not convincing to me or adequately addressed by the authors.---The paper presents a tabular NAS benchmark for GNN architectures spanning nine graph datasets. The tabular data includes several metrics and will allow future research to study NAS for GNNs in an efficient and principled fashion. The authors also use their benchmark data to derive insights such as different graph datasets requiring different architectural choices for best performance. 1. **Impact and relevance to NeurIPS community**: NAS researchers in general will find the benchmark useful and build on it. NAS for GNNs in particular has seen several works in NeurIPS and other top conferences, so NAS-Bench-Graph is highly relevant to the NeurIPS community.2. **Originality**: NAS-Bench-Graph follows many previous tabular benchmarks for NAS, it is, however, the first such benchmark for NAS for GNNs and the first NAS benchmark for the graph modality. The benchmark features a macro and micro component for the search space, which not many benchmarks do.3. **Accessibility and accountability**: The main benchmark data is hosted on github, with the complete data available on Figshare with its maintenance guarantees etc. The authors provide code to read the data in the common data format of a python dictionary. Further, some documentation is provided in the form of a README that shows the basic usage of the benchmark. License for code and data is given. I did not try running the code.4. **Miscellaneous**: NAS-Bench-Graph features a large number of (varied) datasets (9) compared to many other NAS benchmarks, which allows for more comprehensive benchmarking. Performance metrics are recorded for all epochs, allowing, e.g., also multi-fidelity algorithms to be benchmarked with NAS-Bench-Graph. The paper includes several insightful analysis that provide a good intuition of the benchmark characteristics and, e.g., add evidence for the surprisingly strong performance of random search. The authors include a detailed explanation for the systematic setting of hyperparameters. 1. **Accessibility and accountability**: The code that was used to create the benchmark data is not open sourced. The same goes for the experiments and empirical analysis. The python dependencies for the benchmark are not specified. Ideally, NAS-Bench-Graph could be installed via pip from github or pypi. Installation instructions in general are not provided.2. **Empirical protocol**: The empirical evaluation does not quantify meassurement error, e.g., by providing error bounds, making it hard to assess how statistically sound their analysis is. This is acknowledged in the checklist which I appreciate. Further, the empirical evaluation only concerns final performance after ~200 evaluations and not full optimization curves.  |||| rating: 8: Top 50% of accepted papers, clear accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	To address two key challenges in GraphNAS 1) unified and reproducible experiments 2) excessive computation requirements. The authors propose NAS-Bench-Graph, a benchmark for graph neural architecture search (GraphNAS) including data points from 26K GNN architectures on nine prevalent graph datasets. In addition, the authors conducted detailed analysis such as the visualized distribution of accuracy, latency, the number of parameters, and the compatibility of NAS-Bench-Graph to existing NAS algorithms.  The authors also demonstrate the effectiveness of NAS-Bench-Graph by searching GNN architecture with open-sourced libraries on NAS-Bench-Graph and report the performance of the searched network. +Clear GraphNAS search space abstraction and benchmark construction procedure.+Detailed and intuitive analysis of the architecture space. The authors said NAS-Bench-Graph contains 26,206 different architectures, and a detailed explanation of how the number is derived would be helpful.Typo in line 30: “...key challenges that seriously hind the”, hind->hinder.  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	The paper presents a NAS for GNN benchmark, that is fair, comprehensive, and reproducible. The authors provide a framework for GNN NAS unifying the codebase hyperparameters, datasets, and evaluation protocols. They propose a new search space and conduct analysis and visualization on the searched architectures and their transferability. + The motivation of this benchmark is good, which calls for a unified and fair benchmark for GNN NAS algorithms. + The proposed benchmark is first presented in GNN NAS domain, to the reviewer’s best knowledge.+ The visualization Fig. 2 and 4 provide good insights to analyze the latency-performance trade-off and cross-dataset architecture transferability. - Most of the experiments are done on their proposed search space, which may vary across different methods. This seems to be a mismatch with the goal of a benchmark, which is often designed to compare different methods under a fair experimental setting.- The chosen datasets are not diverse enought. Only homophily data are considered. It would be better to consider heterophily data.- The benchmark paper does not compare with enough baseline methods, such as SANE, GASSO, AutoCoG.  |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
eAhc2CNAl	XTC: Extreme Compression for Pre-trained Transformers Made Simple and Efficient	long	In this work, authors start from a point where they perform a systematic study to measure the impact of many key hyperparameters and training strategies from previous work, such as distillation, DA, etc. Then, they find out that previous baselines for ultra-low bit precision quantization are significantly under-trained. Based on the study, they propose a new compression pipeline named XtrmC. The focus of this paper is mainly on BERT. Strengths:- informative and deep study on the existing work. The key factors evaluated by the paper may inspire following research- Good presentation of the experiment results - Clear statements on the limitation of the current work and show how the future could be.Weaknesses:- The overall discussion in the main draft focus on the study, but not the new method. More dicussion on the new compression method may benefit the overall presentation logic since the title is about the extreme compression, not just a study on existing methods.  Yes  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	The paper presents an empirical study of the role of various stages in recent extreme compression of Transformer architectures for NLP tasks. Based on this study, the paper presents a simplified architecture and training process that significantly reduces model size. Provides a thorough empirical study of how the # of training iterations affects heavily quantized network compression techniques. This shows that the accuracy of binarized networks is improved merely by training longer combined with data augmentation.This study also shows that complicated multi-stage distillation is not required to maintain accuracy of heavily quantized networks.Based on the empirical study, the paper presents a simplified model which consists of layer reduction (structured pruning) and a single stage of distillation. The experimental results show that this simplified model works well compared to the more complex baselines.As the proposed method requires more training iterations, it may be more computationally complex than competing methods, even if those methods require more complex distillation stages. The paper lacks a detailed analyses of the computational complexity of the various methods.The contribution of the paper, while effective, is ad-hoc. It would be nice to have some theoretical discussion of "why" increased training times helps with heavy quantization, and why additional distillation stages are not needed. At least the weight distribution histogram could be presented as a way of gauging the effect of training in the low- versus high-training time regimes.Another weakness is that much of the details of the study is included in the supplementary material, and not in the main text. A neurips paper should be self contained, without requiring reference to supplementary material to gain important details. The authors do mention some limitations. They do not discuss any potential negative societal impacts, although they could perhaps mention positive social impacts arising from greatly reduced inference time computational loads, as well as reduced training times, both of which would lower the carbon footprint of neural networks.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper provides an empirical study on the compression of BERT via distillation and quantization. By fine-tuning a large amount of compressed BERT models, the authors find that longer training iterations with small learning rate can help quantization, without techniques like multi-state quantization or weights split. Based on a series of empirical observations, the authors propose XtrmC that jointly quantizes and distills an uncompressed BERT, and achieves acceptable performance with 50x model size reduction. Strengths : 1. The experimental results in this paper are abundant. The authors report a series of results under different settings, and present the corresponding findings in a Q&A fashion.2. The newly found observation helps to build an efficient compressed BERT without a complicated and ad hoc training strategy.Weaknesses: 1. The novelty of this paper is incremental since it mainly discusses the effectiveness of the strategies proposed in the TinyBERT, TenaryBERT and BinaryBERT. In other words, it is more an engineering or application note, rather than a work of research innovation.2. Finding 3 is similar to the discussions already provided in TinyBERT and has been verified in its subsequent works.3. Lack of the discussion on other BERT variants like RoBERTa and generative models like GPT. The paper contains valuable engineering insights, yet its presentation is largely hampered by the careless writing and lack of basic proofreading (e.g. where is the mentioned Fig. 4?), as shown in my editorial comments below.Editorial:1. In abstract, resource-constraint should be resource-constrained?2. P.2, bottom, celebrating should be celebrated.3. Line 51, accuracy “los”?4. Line 73, extend should be extent5. Line 144, the 1-norm argument should be w or w_i?6. Line 150, bucket-A?7. The positioning of Tables 1 & 2 should be swapped to be in order.8. Line 178, add “multi-stage knowledge distillation” after “claim”.9. Line 184, x2.5 should be 2.5x10. Number of brackets in (1) doesn’t agree!11. Line 196, Fig. 4 is referenced in text but it is missing!12. Line 210, Row should be Rows13. It is clumsy in having the Best (above) rows in Table 3, why don’t you just bold the winners?14. P.7, middle paragraph, I am confused by BERT-base and BERT_{base} notations.15. What is Row 0 in Table 5?16. Line 280, what is “better a trained”?17. Line 317, “when binarization”?   |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	The paper provides a thorough understanding of the low-bit (1-bit and 2-bit) quantization for pre-trained BERT. Specifically, the authors discover four characteristics: 1) longer training and proper hyperparameter reduce the gap, 2) 1-stage knowledge distillation is okay, 3) data augmentation is essential, and 4) pre-training distillation for model compression may not be always helpful. From the findings, the paper proposes XtrmC, a model compression technique that integrates the abovementioned points. XtrmC includes a lightweight layer reduction (based on simple rules) followed by long QAT with 1-step KD and DA. The experiments show that the compression ratio of XtrmC is much higher than the previous works, such as TernaryBERT and BinaryBERT. Overall, the paper is clearly written and well organized. Previous works are sufficiently compared, and the motivation is reasonable. The paper is supported by extensive experiments. Considering that there is no unified comparison of the various methods for low-bit BERT quantization, this paper is very timely. Thank you for your hard work!Especially, it is interesting that pre-training distillation may not be necessary (Finding 4), which can save huge training costs to obtain the final model. It is also good news that Skip-BERT shows comparable and even better results.However, the findings are somewhat empirical; It would increase the value of the paper if you could provide some intuitions about why the phenomenon occurs for each finding. For example, for Finding 1, how could longer training help to avoid sharp accuracy drop (or, falling into early local minima)? For Finding 3, why DA is especially important for small tasks such as CoLA and RTE? The authors well addressed the limitations and future research directions.  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
H1V4QhAqYQ	Augment your batch: better training with larger batches	long	This paper tested a very simple idea: when we do large batch training, instead of sampling more training data for each minibatch, we use data augmentation techniques to generate training data from a small minibatch. The authors claim the proposed method has better generalization performance. I think it is an interesting idea, but the current draft does not provide sufficient support.1. The proposed method is very simple. In this case, I would expect the authors provide more intuitive explanations. It looks to me the better generalization comes from more complicated data augmentation, not from the proposed large batch training.  2. It is unclear to me what is the benefit of the proposed method. Even provided more computing resources, the proposed method is not faster than small batch training. The improvement on test errors does not look significant. If given more computing resources, and under same timing constraint, we have many other methods to improve performance. For example, a simple thing to do is t0 separately train networks with standard setting and then ensemble trained networks. Or apply distributed knowledge distillation like in (Anil 2018 Large scale distributed neural network training through online distillation)3. The experiments are not strong. The largest batch considered is 64*32, which is relatively small. In figure 1 (b), the results of M=4,8,16,32 are very similar, and it looks unstable. It is unclear what is the default batchsize for Imagenet. In Table 1, the proposed method tuned M as a hyperparameter. The baselines are fairly weak, the authors did not compare with any other method. I would expect at least the following baselines:i)  use normal large batch training and complicated data augmentation, train the model for same number of epochsii) use normal large batch training and complicated data augmentation, train the model for same number of iterationsii) use normal large batch training and complicated data augmentation, scale the learning rate up as in Goyal et al. 20174. For theorem 1, it is hard to say how much the theoretical analysis based on linear approximation near global minimizer would help understand the behavior of SGD. I fail to understand the the authors’ augmentation. Following the author’s logic, normal large batch training decrease the variability of <H>_k and \lambda_max, which converges to ‘’flat’’ minima. It contradicts with the authors’ other explanation. 5. In section 4.2, I fail to understand why the proposed method can affect the norm of gradient. 6. Related works:Smith et al. 2018 Don't Decay the Learning Rate, Increase the Batch Size. =============== after rebuttal ====================I appreciate the authors' response, but I do not think the rebuttal addressed my concerns. I will keep my score and argue for the rejection of this paper. My main concern is that the benefit of this method is unclear. The main baseline  that has been compared is the standard small-batch training. However, the proposed method use a N times larger batch and same number of iterations, and hence N times more computation resources. Moreover, the proposed method also use N times more augmented samples. Like the authors said, they did not propose new data augmentation method, and their contribution is how to combine data augmentation with large-batch training. However, I am not convinced by the experiments that the good performance is from the proposed method, not from the N times more augmented samples. I have suggested the authors to compare with stronger baselines to demonstrate the benefits. However, the authors quote a previous paper that use different data augmentation and (potentially) other experimental settings. The proposed method looks unstable. Moreover, instead of showing the consistent benefits of large batch, the authors tune the batchsize as a hyperparameter for different experiments. Regarding the theoretical part, I still do not follow the authors' explanation. I think it could at least be improved for clarity.  |||| rating: 4: Ok but not good enough - rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper describes a new method for data augmentation which is called batch augmentation. The idea is very simple -- include in your batch M augmentations of the each training sample, effectively this will increase the size of the batch by M. I have not seen a similar idea to this proposed before. As the authors show this simple technique has the potential to increase training convergence and final accuracy. Several experiments support the paper's claims illustrating the effectiveness of the technique on a variety of datasets (e.g. CIFAR, ImageNet, PTB) and architectures (ResNet, Wide-ResNet, DenseNet, MobileNets). Following that there's a more theoretical section which provides some analysis on why the method works, and seems also reasonable. Overall simple idea, well written-paper with clear practical application and of potential great interest to many researchers |||| rating: 8: Top 50% of accepted papers, clear accept |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	The paper shows that training with large batch size (e.g., with MxB samples) serves as an effective regularization method for deep networks, thus improving the convergence and generalization accuracy of the models. The enlarged batch of MxB consists of multiple (i.e., B) transforms of each of the M samples from the given batch; the transform is executed by a data augmentation method such as Cutout or Dropout. The authors also provide a theoretical explanation for the working of the method, suggesting that the enlarged batch training decreases the gradient variance during the training of the networks.The paper is well written and easy to follow. Also, some interesting results are experimentally obtained such as the figures presented in Figure4. Nevertheless, the experimental studies are not very satisfactory in its current form. Major remarks:1.In terms of regularization with transformed data in a given batch, the proposed method is related to MixUp (Zhang et al., mixup: Beyond empirical risk minimization), AdaMixUp (Guo et al., MixUp as Locally Linear Out-Of-Manifold Regularization), Manifold Mixup (Verma et al., Manifold Mixup: Learning Better Representations by Interpolating Hidden States), and AgrLearn (Guo et al. Aggregated Learning: A Vector Quantization Approach to Learning with Neural Networks). It would be useful for the authors to discuss how the proposed strategy differs from them or empirically show how the proposed regularization method compares to them in terms of regularization effect.  For example, in MixUp, AdaMixup and Manifold Mixup, the samples in a given batch will be linearly interpolated with randomly reshuffled samples of the same batch. In these sense, using them as baselines would make the contribution of the proposed method much significant. 2.In the experiments, it seems the authors use different data augmentation methods for different datasets  (except for Cifar10 and Cifar100), it would be useful to stick with a particular data augmentation method for all the datasets, for example, it would be interesting to see the performance of also using Cutout for the MobileNet and ResNet50 on the ImageNet data set. 3.Regarding the experimental study, I wonder if it would be beneficial to include three variations of the proposed method. First, use baseline with the same batch size, namely BxM, but with sampling with replacement. That is, using the same batchsize as that in Batch Augmentation but with repeated samples. In this way, the contribution of the data augmentation in the proposed method would be much clearer. Second, as suggested from the results in the PTB data in Table1, using only Dropout obtains very minor improvement over the baseline method. In this sense, using other data augmentation methods instead of Cutout for the image tasks would make the contribution of the paper much clear. Third, training the networks with the batchsize of BxM, but excluding the original data samples in the given batch would be another interesting experiment. That is, all samples of the batch in the batch augmentation are synthetic samples. Minor remarks:1.Is the regularized model robust to adversarial attacks as suggested in Mixup and Manifold Mixup?2.Would it be beneficial to include various data augmentation methods for the same batch? That is, each transformed sample may come from a different data augmentation strategy.==========after rebuttal===========My main concern is that the paper did not clearly show where the performance improvement comes from. It may simply come from the larger batch size instead of the added augmented samples as claimed by the paper. I think the current comparison baseline in the paper is insufficient. I did propose three comparison baselines in my initial review, but I am not satisfied with the authors' rebuttal on that.   |||| rating: 4: Ok but not good enough - rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
JRAlT8ZstmH	Latency-aware Spatial-wise Dynamic Networks	long	Paper tackles the problem of dynamic inference where the forward graph will depend on the input data. It focuses on real speedups instead  of theoretical one, this makes the work more valuable. There is a latency prediction model that can estimate the latency by considering the algorithms, scheduling and hardware properties. Experiments are performed on image classification and demonstrate latency reduction of 23 to 45% depending on the hardware superiority.  Strengths:+ Dynamic inference is a field of great interest. Naturally, people spent different effort to perform vision tasks so should deep network. + The method considers the real latency and not FLOPs. Additionally, demonstrating real speed-up is greatly appreciated. + Multiple paths are considered during training, and a single path is executed during inference.  + Pare is well written and the content comes smoothly. Weaknesses:- Using distillation during fine-tuning will increase accuracy and might be not fair comparing to the original model.- When compared to the previous work like Conv-AIG, the training recipes might be different (authors start from trained model and do 100 more epochs). The over training pipeline is not described as well, things like augmentations, learning rate scheduler etc should be mentioned. If those are not the same (as number of epochs) then authors should perform a fait comparison with those being the same. - There are questions in the section before that are not clear. Understanding them will help to evaluate paper better.  - Limitations are not listed by authors- There are seem to be a possible a set of hyper-parameters that need to be tuned like S etc. - Some details are missing and clarifying them will be helpful.- It is not clear how different convolution implementations (like Winograd) will benefit from the work.  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper presents a dynamic inference paradigm based on selective inference of convolutions on the spatial dimension. Specifically, for each convolutional block, it uses a masker layer to predict the masked region of the feature map and only conduct convolution on the masked region. Such convolution has less computation than a full convolution and improves efficiency of the model. Moreover, it utilizes a latency prediction model to estimate the latency of different mask operations. With a differentiable training framework, the model can be optimized according to configurations different hardware platform. Strength:-Although masked inference is not a new idea, this work targets at a more practical inference speed optimization compared to existing dynamic-masking approaches. Several critical modifications for the real efficiency of this approach are adopted in the inference pipeline: fused operations, granularity choices of the mask etc.-Experiments with ResNet and RegNet validates the effectiveness of this method on several tasks: ImageNet classification and object detection. It reduces inference time by 23% and 45% on V100 and TX2 platform respectively without significant drop on accuracy.Weaknesses:-The framework can only be applied to models with ResNet block whose major computations are located in the 3x3 conv of the network blocks. I don’t think it work well for other efficient models such as MobileNet and ShuffleNet that does not have heavy 3x3 convs.-Only GPU platforms are tested. Does this approach generalize to other types of hardware such as CPU? -For the speed comparison, what kind of cuda runtime environment are you using for the experiments? What are the cuda version, cudnn version? are you using TensorRT? These details are not given in the main text. Yes  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This submission introduces a spatially dynamic neural network approach, developed in a latency-aware manner that can generate realistic speed-ups during inference. The proposed methodology allows to dynamically skip computation at different spatial regions of the feature maps at a coarser granularity (of blocks of pixels), directly generalising the widely studied pixel-level approaches from the literature. This leads to more regular computation pattern and reduces the overheads, to maximise the attainable inference speed gains. Additionally, the granularity of the pixel blocks is tuned in a latency-aware manner to balance the speed-accuracy trade-off. Results indicate that the proposed approach is effectively improving efficiency, across hardware platforms and models for image classification and object detection tasks. Strengths:- The paper addresses a very important real-world issue, which is well-motivated and demonstrated experimentally.- The proposed approach, although rather simple, is able to generate realistic speed-ups with no effect in accuracy, showcasing improved efficiency compared to baselines.- Useful insights are provided in the experiments analysis.Weaknesses (see questions for more details):- The novelty of the proposed approach is limited, as it directly generalises a widely studied problem, going from pixel-level to block-level. Similar generalisation in the context of early-exit networks has recently been studied in the literature:  - Liu, Zhuang, et al. "Anytime Dense Prediction with Confidence Adaptivity." International Conference on Learning Representations. 2022.-  The description of the latency prediction model (Sec 3.3) is very high-level, making it hard to justify its contribution . - Some interesting aspects of this work (eg the generic formulation combining spatial computation skipping and layer skipping - Sec 3.2) are only theoretically mentioned, but not experimentally studied.- Some of the claimed contributions and ablation studies are mostly comprise technical/implementation aspects (Sec 3.4).- Evaluation is focused on Classification/detection, but it is unclear how the proposed methodology can be applied to more dense CV tasks, such as instance/semantic segmentation. The reviewer believes that this mapping won't be intuitive and this should be discussed in the limitations of the proposed approach.Presentation- Syntax/grammatical errors exist sporadically across the manuscript, mostly in Sec1, 3.3 and 4.5- Some aspects of the proposed methodology are described in rather high-level, making it impossible to reproduce. Some limitations are discussed in the conclusion section. The authors are encouraged to enhance this discussion.  |||| rating: 5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
jM76BCb6F9m	LEAF: A Learnable Frontend for Audio Classification	long	The paper proposes a learnable frontend for classification tasks on audio signals. The proposed learnable audio frontend (LEAF) is a generalization of a mel filterbank, used commonly in machine audition.LEAF consists of a Gabor filterbank, magnitude-squared nonlinearity, Gaussian lowpass filter and previously-proposed per-channel energy normalization. Learnable parameters of LEAF are the center frequencies and bandwidths of Gabor filters, bandwith of lowpass Gaussian filters and smoothing coefficients of sPCEN. The proposed LEAF matches the performance of competing frontends in most of the cases and leads to some improvements in some cases.In general, I like the paper and the proposed frontend is very sensible from perspective of audio-related applications.However, I believe there’s some exaggeration in terms of impact of the proposed frontend based on the results in this paper. The proposed system is not actually challenging status quo, as many learnable frontends have been proposed in the literature in the past (as also listed in the references).The paper is easy to read and authors communicate their contribution clearly.However, I believe that the title may be somewhat too general: LEAF is evaluated only on classification tasks, and IMHO that should be indicated in the title as well. There are other tasks, such as speech enhancement, where LEAF-like frontend may work well, but that is out of scope of this paper.Experimental results show that LEAF is performing well in the considered tasks. However, it would be interesting to understand the differences in performance when the encoder & head are changed and/or increased.More specifically, the current results are obtained using a lightweight EfficientNet and linear heads. Is there any particular reason for this setup? Would the conclusions change with a different encoder/head?Also, understanding of the influence of the filterbank setup (number of channels, window length, stride) would be beneficial.Details:(1) Title should reflect the fact that LEAF has been evaluated only on classification tasks(2) Abstract: “over a wide range of audio domains” —> It would be more appropriate to talk about a range of applications in audio domain.(3) Abstract: “unprecedented” -> I believe this is a bit exaggerated(4) Introduction: “this might not be the optimal approach for non-human sounds” —> This is a strange argument, and I believe the authors are confusing sound perception and sound production. Human sound perception works quite well for recognition on non-human sounds. The authors imply that a system which replicates a system mimicking human perception is suboptimal for non-human sounds. However, human auditory system is not designed for processing of only human-made sounds. Furthermore, optimality depends on the application, so stating that something is not optimal for a class of sounds makes no sense without the defined application, which in this case could be recognition of “acoustic events or animal vocalizations”(5) Introduction, last paragraph: “wide and diverse range of tasks, including speech, music, audio events, and animal vocalizations” —> Signals, such as speech or music are not tasks. A task can be, e.g., speech recognition.(6) Conclusion: Stating there’s a “historical statu quo of using hand-crafted mel-filterbanks” with so many end-to-end systems giving the best performance in different applications is a bit too much.======= Review edit after authors' revisions ====== Most of my concerns have been resolved in the significantly-improved revised version of the paper. |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The paper shows a detailed interpretation on the relationship between each component of hand-crafted audio front-ends (such as mel-spectrograms) and learnable counterparts. To do that, they followed the narratives presented from the previous works such as SincNet and improved the model by changing the several components of it. The authors grouped the audio front-ends into mainly three parts which are filtering, pooling, and compression. And, the contributions were made at each stage. For filtering stage, instead of learning all the parameters of the convolution layer, they let the model to learn only center frequency and bandwidth of the filterbanks that are initially assigned with Gabor filters. For pooling stage, instead of using simple average or max poolings, they let the model to learn low pass filtering with small parameters. For compression, instead of using log based dynamic compression, they extended Per-Channel Energy Normalization by replacing a fixed smoothing factor to learnable parameters and named it to sPCEN.To evaluate the proposed approach, they evaluated the models on 8 audio classification tasks which might have diverse audio and label characteristics (such as acoustic scene sound, animal sound, music, speech). The compared models are mainly mel-spectrogram and SincNet. The results shows that the proposed model outperforms the comparisons for most tasks. Then, they further ran a multi-task classification experiment to obtain universal audio front-ends. And, the results show the proposed learnable front-ends is showing some generalization ability on most tasks. Finally, they evaluated the proposed model on large-scale audio classification dataset (AudioSet) and verified that the proposed front-ends is also showing the good performance on it.In page 4, the authors mentioned that the l2 normalization helps distinguishing the role of filtering and compression. I think this contribution is not trivial, so if the authors can add more experiments (or plots) to show the difference between models with and without l2 normalization, then it would be helpful.The backbone model used in the paper is fixed, and showing that the proposed audio front-ends shows similar trends with multiple backend models can verify better generalization ability of the proposed approach. So, if the authors can add additional experiments with multiple backends, then it would be helpful. |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper presents a new learnable representation fo audio signal classification and compares it to the classical mel-filterbanks representation and two other learnable representations on a broad range of audio classification tasks, from birdsongs to pitch, instrument, language or emotion recognition. The proposed representation combines several parameterized representation techniques from the recent litterature. It is reported to yield on par or better classification results than the other methods on several of these tasks using single- or multi-task learning.Pros:- Learning an ultimate, universal, generic representation for all audio signals that renders the 80 years old mel-frequency scale obsolete is certainly an attractive goal- The proposed representation carefully and elegantly combines the best parts of several recently proposed parameterized representations and enjoys a nice interpretability while requiring few parameters to learn.- Comparing different audio representations on such a broad range of audio classification task is a welcome and unmatched effort, to the best of my knowledge.Cons:- The paper lacks humility in its story-telling and its style. It employs formulations such as "lived through the history of audio", or "challenging the historical statu quo" when refereing to mel-frequency representations, although by the authors' own admition in the paper, a large amount of research effort has already been given in recent years towards learnable audio representations (the authors cite a dozen papers but there are more). Hence, this paper is not a first attempt. And despite the pompous use of "universal" in the title, I believe it is not a last attempt either. The authors claim that the proposed representation "outperform mel-filterbanks over several tasks with a unique parametrization" but this is far from clear when looking at the results carefully. In the majority of the tasks, the representation performs either slightly worse, equal, or about 0.5% better than Mel-filterbanks. It is not clear whether such improvement is significant, since no error bar or standard deviation is provided in the results (a sadly common habit in the audio litterature). The only tasks where a truly significant improvement is reported are language identification and emotion recognition, which are also the tasks where all the methods perform the poorest. It looks like any significant difference between the 4 compared approaches would vanish if these two tasks were omitted. The reason why the proposed representation performs well on these two very specific tasks is not clear and not discussed.- More generally, the paper would be much more valuable if it gave a sense of WHAT is actually learned by the proposed method. Is the final representation significantly different from a mel-filterbank? Given how close to mel most reported results are, this is doubtful. In fact, Fig. A.1. strongly suggests that LEAF just re-learned mel, but strangely this figure is never commented. Some comments on the learned compression-parameters would also be appreciated.- At least one important comparison point is clearly missing in the reported results: STFT + PCEN or mel-filterbanks + PCEN, e.g., Wang et al. (2017) or Schlüter & Lehner (2018) [note that the latter already uses sPCEN rather than PCEN, contrary to the authors' claim] . Omitting this from the comparisons prevents one from knowing whether the proposed parameterized Gabor filterbank brings any advantage over another time-frequency representation like STFT or mel-filterbanks. Less critically, another missing comparison point is LEAF + CNN14, in Table 4.- What the authors refer to as "audio" in the title and throughout the paper is in fact much more narrow, namely "audio classification". Learnable audio representations have been studied in a broader context in recent years, e.g., speech enhancement, source separation, dereverberation, sound localization or audio (re-)synthesis. In fact, one of the important breakthroughs recently brought by learnable audio frontends was in source separation with the paper TasNet (Luo et al. 2018) which is not cited by the authors. In the same context, (Ditter and Gerkmann 2020) presented a learnable gammatone-like filterbank and showed that fully-parameterized learned filterbanks tended to have logarithmic spread in frequencies. Moreover, the use of learnable analytical filterbanks/Hilbert pairs due to their envelop extraction/shift invariant properties was already discussed in depth in (Pariente et al. 2019) [cited in the paper]. Overall, while comparing different learnable audio representations on a broad range of audio classification tasks is a timely and worthwhile topic, and while the proposed representation elegantly combines several recent ideas in this area, the general presentation and angle of the paper strongly lacks humility. Instead of the proposed title, something like "Benchmark of learnable audio representations on a broad range of classification tasks" would be more truthful to the work. To make the investigation more worthwhile and insightful, additional comparison points (STFT + PCEN, mel-frequency + PCEN, Gabor + log, etc.) as well as an analysis of what the model has actually learned would be needed.======= Review edit after authors' revisions ======The changes made by the authors in the title, abstract, introduction and conclusion to narrow the scope of the paper, better contextualize it, and make it more humble and truthful are very welcome. The extra experiments, figures, addition of error bars and new statistical tests are also a real plus. In doing so, the authors addressed all of my major concerns.For these reasons, changed my evaluation score from 5 to 8. |||| rating: 8: Top 50% of accepted papers, clear accept |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature	In this work, the authors introduce a learnable front-end (LEAF) for audio. The paper evaluates it on several tasks in the audio domain such as acoustic event classification, speaker identification, keyword spotting, language identification, music classification etc. The study reports that proposed features outperform or perform similar to Mel filterbanks for many tasks. This paper is well written and easy to follow. The references are excellent as well. I have summarized my comments below which will help in improving the quality of this manuscript: 1. While the authors evaluate the proposed front-end on several tasks, the evaluation protocols are missing. Especially, the training partition and test partition and number of segments in each class. For example, did they calculate accuracy using balanced classes? Was classification done at the frame level or the utterance level? I would highly recommend authors to add the experiment protocol to the manuscript as Table 1A does not provide enough information. 2. The authors evaluate LEAF on several tasks however the model architecture for classification models remains constant. Typically Mel FBs are utilized across architectures and one of the major shortcomings of the proposed work is the integration of LEAF with state-of-the-art models (for example x-vectors for SID). This would give more understanding about the generalization capability of the LEAF.3. As pointed out by authors in the introduction, Mel FBs are invariant to deformation and noise-robust. All the audio problems addressed in this work encounter varying levels of noise in the real-world. It is extremely important to assess the noise robustness of the proposed front-end. I would recommend the authors test LEAF with different levels of noise.4. For results in Table 1 the statistical significance should be computed. For the acoustic scene task, the difference in performance is very less and this dataset only has 810 samples overall for testing. Therefore, without confidence intervals, it is very hard to conclude the performance of LEAF. Minor comments:1. Please add this missing reference: Davis, S. and Mermelstein, P., 1980. Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences. IEEE transactions on acoustics, speech, and signal processing, 28(4), pp.357-366.2. In Table 1, Mel and LEAF has the same accuracy for Music instrument and needs to be bolded. 3. mel ---> Mel 4. In Fig 2, you may improve the font size of x-axis labels.  |||| rating: 4: Ok but not good enough - rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
a4zg0jiuVi	Anytime-Valid Inference For Multinomial Count Data	long	This paper introduces two online sequential tests of equality and contrasts, for Bernoulli and Poisson process, based on multinomial hypothesis. In simulated studies, the new tests show the speed up in decision making and reduce the opportunity cost in data collection process. Strength:- sequential testing is an important problem and authors demonstrated an effect approach- some theoretical results are provided on the tests- multiple applications are testedWeakness:- the results are not surprising. For example, theorem 4.1  and Lemma A.3 seems known/straightforward. The resulting test, which is just a ratio test, is not so difficult to come up with.- no comparison with baselines are conducted in the experiments. It didn't demonstrate the efficiency gain of the proposed approach against potential baselines.  adequately, in fact the discussion is thorough.   |||| rating: 4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	This paper proposes a sequential multinomial test where Type I error is controlled through the martingale properties and the test is consistent. Then, the paper show how to apply the proposed test to test the data generated from the Poisson counting process and the Bernoulli process. == After rebuttal ==\I want to thank the authors for their efforts to clarify my misunderstanding. I have more appreciation of this work now and meanwhile still have some concerns remaining. The pros of this work after the clarification: controlling the Type I of this proposed multinomial test naturally comes from the martingale properties of a variant of the sequential probability ratio test (SPRT), and also the consistency of the proposed test is also based on the nice consistent property of the SPRT. The Dirichlet prior with $\alpha_{0,i}=k\theta_{0,i}$ is a reasonable set-up to simulate an alternative where there is only a small departure from the null. Some concerns after the clarification: the authors point out that the comparison with the $\chi^2$ test is shown in A9 Figure 4, but from what I read, isn't this a Type I error rate comparison? Does this work have the power (Type II) comparison between the proposed test and the $\chi^2$ test? Besides, one of the biggest benefits of a sequential test is the early (optional) stopping, but the drawback is that, with the same sample size allowed, its power will be attenuated compared with a fixed-size hypothesis testing. I tried to find through the appendix to see the experimental evidence that the stopping time increases with the increasing difficulties of the alternative hypothesis, but I didn't see that in the appendix. Lastly, a sequential test usually wouldn't enjoy too much benefit when the considered alternative hypothesis only has a minimum departure from the null, and this is the setting (small departure from the null) considered in Section 2.1. I wonder does the proposed test still can reject the alternative fast in this setting. If so, what's the intuition of that? I would love to kindly ask the authors to remind me if I miss something here. Thank you. === correction ===In the end of the comments above, I was asking can the test reject the null fast under the alternative. ===final score ===The authors did a great job to largely relieve my concerns and help me understand their work more. Thus I am willing to increase my score to 6. Strengths: The design of the test is technically solid. The Type I error is controlled and the test is a consistent test.Weakness: (1) The assumption on the parameters (line 52 to line 56) about the alternative may not be accurate, but the consistency of the test heavily depends on such an assumption. (2) The motivation for developing the test instead of using some two-sample tests such as a T-test is missing. At least the author should show the experimental comparisons with some classical A/B tests. See above.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	This paper proposes new sequential hypothesis tests and corresponding confidence sets and intervals for multinomial data. These tests provide "always-valid" guarantees, i.e. controlled Type I error probability under optional stopping/continuation.First, the authors consider the direct application of detecting departures from a specified multinomial parameter, that is sample ratio mismatch tests.Second, they consider the problem of comparing the success rate of k arms corresponding to inhomogeneous Bernoulli processes. The parameters of the Bernoullis can vary with time (but in the same way for all arms). This problem is mapped to a multinomial one by considering the probability of each arm producing the next success conditioned on the event that there is a success.This point of view has several advantages:1. It allows using the sequential constructions for tests, confidence sets and intervals proposed in the first part.2. It only needs to count success observations.3. The time varying effect gets canceled out.Finally, the problem of comparing the intensity of inhomogeneous Poisson counting processes is also mapped to a multinomial problem by showing that the probability of the next observation belonging to the i-th process is a multinomial random variable. I enjoyed reading this paper that addresses difficult and interesting statistical problems.Although the paper is very well written, I found it a bit unfortunate that part of the introductory material had to go to the Appendix, but I understand it was necessary for space constraints.As explained in the paper, the construction of the sequential test for multinomial point hypotheses follows standard steps that are well known in the community and the theoretical results of this part are straightforward consequences of known results.From my point of view, the most interesting and novel contributions are those of Section 3 and 4, i.e. how to turn the problem of comparing time-inhomogeneous Bernoulli and Poisson counting processes into a multinomial problem.Moreover, these problems have important practical applications that are very relevant for the NeurIPS community. Yes  |||| rating: 8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	The paper introduces a sequential test for multinomial hypotheses using a martingale construction. It then uses this result to develop a sequential test of equality and contrasts in inhomogeneous Bernoulli processes and time-inhomogeneous Poisson counting processes. The strengths are (1) the authors use martingale inequalities to construct a test martingale thatcontrols the frequentist Type I probability below a desired level u. (2) they obtain a confidence with a coverage guarantee of at least1-u.The main assumption of the test is that "departures from the null to be small and encode this information into the Dirichlet prior". Null-hypotheses are never fully true and, therefore, the above assumption decreases the robustness of the test. I am not sure the result is really novel and in any case it seems to be an application of the results in Test Martingales, Bayes Factorsand p-Values by Shafer et al.. I believe a statistical conference/journal would be a more appropriate venue for this paper, where the contributions of this paper can be discussed with more details with respect to the state of the art. It is also not clear to me why the authors focus on Bayes factors and then derive results about frequentist Type I and II errors. Then why did they not consider a likelihood ratio test instead of a Bayes factor ? From a Neurips perspective, the authors are considering a simple problem (count data) and other approaches could be used. For instance, one could simply compute the posterior of theta and then use alpha-% credible intervals for drawing conclusions about theta_0. This approach would work quite well in practice considering the goal is to estimate theta from counts. The assumption "departures from the null to be small" is very strong.  |||| rating: 5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
rcEDhGX3AY	Weakly-Supervised Semantic Segmentation via Transformer Explainability	long	The paper implements the work proposed in Chafer et al CVPR 2021 and also incorporates ideas from another work Ahn et al. 1) It would have been better to present a more detailed discussion on hyper-parameter tuning. This is essential considering the work uses an already available implementation. 2)  It will be better to avoid terms like "....mind-blowing..." in a research article. |||| rating: 8: Top 50% of accepted papers, clear accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper does not focus on reproducing the results in a paper. It studies how to combine the approaches in two papers [1] and [2]. [2] is about weakly-supervised segmentation, which uses the attention analysis (of the classification networks) results (i.e., attention maps corresponding to the classes) as the pseudo label in the context of weakly supervised segmentation task without label groundtruth. [1] studies the attention analysis of the vision transformer (ViT) based classification networks. The authors use the ViT based method from [1] to replace the CNN based method in [2]. It is a well motivated study for weakly supervised segmentation - trying to use the ViT generated attention maps to replace the CNN generated ones. It is valuable that this paper reports the results observed during experiments. However, the authors paid few attention to studying the reproducibility of the original results/claims of [1] and [2]. Suppose we are at a viewpoint that the weakly supervised segmentation task can be a surrogate task to evaluate the reproducibility of the attention analysis method in [1] in different contexts. In that case, the paper still lacks discussion in this aspect. The authors may further analyze why the pseudo label generated by ViT is not good enough for segmentation. This can help to study whether the results/claim of [1] can be reproduced in a different context and may help improve the significance of this work as a technical innovation paper. [1] H. Chefer, S. Gur, and L. Wolf, “Transformer interpretability beyond attention visualization,” CoRR, vol. abs/2012.09838, 2020.[2] J. Ahn and S. Kwak, “Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4981–4990, 2018.  |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	**Scope of reproducibility:** This submission reproduces the work "Transformer interpretability beyond attention visualization” by Chefer et al. (2021). The original work of Chefer et al. proposes a mechanism for propagating relevancy through the layers of a transformer with the objective of interpreting the decisions of a transformer.  In this reproduction submission, the authors successfully reproduce the key experiments of Chefer et al. relating to vision transformers.  The authors then go beyond the work of Chefer et al. by integrating the resulting relevance scores into the pixel affinity propagation framework of Ahn and Kwak (2018). They show that while the pixel affinity propagation framework can be combined with ViT seeds and used to train a Hybrid-ViT architecture, this approach is less effective than the original CNN-based approach of Ahn and Kwak.**Code:** The authors made use of the code provided in the original works of Chefer et al. (2021) and Ahn and Kwak (2018). However, they replicated substantial parts of the pipelines. **Communication with original authors:** As far as I can tell, there was no communication with the original authors. However, given that the results of Chefer et al. (2021) were successfully reproduced, it seems likely that this was simply unnecessary. **Hyperparameter Search:** The authors did not conduct an extensive hyperparameter search for the work of Chefer et al. (2021). However, they instead extended the work to a new application (described below under "results beyond the paper").**Ablation Study:** The authors did not provide comprehensive ablation studies.**Discussion on results:** The authors provide a detailed discussion of the reproducibility of the paper, highlighting that most elements of the original work of Chefer et al. (2021) and Ahn and Kwak (2018) were simple to reproduce. They note, however, that it was somewhat unclear how to compute the AUC metrics for the perturbation experiments in Chefer et al. (2021).**Recommendations for reproducibility:** As noted above, the authors note one component of Chefer et al. (2021) where there is room for improved clarity to enable others to reproduce the work more easily.**Results beyond the paper:** The authors go considerably beyond the original work of Chefer et al. (2021) by integrating the resulting relevance scores into the Affinity Propagation framework of Ahn and Kwak. Although the experiments do not surpass the state-of-the-art, this experiment shows promising preliminary results. **Overall organization and clarity:** Overall, the paper is well organised and the writing is clear.**Extra comments:** I consider this to be a good reproduction study. By providing experiments beyond the original work of Chefer et al. (2021), it offers useful insights for the community. **References:** Ahn, J., & Kwak, S. (2018). Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4981-4990).Chefer, Hila, Shir Gur, and Lior Wolf. "Transformer interpretability beyond attention visualization." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021. |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
2aIbM23gW7	Learning Neuro-symbolic Programs for Language-Guided Robotic Manipulation	long	This work is concerned with developing a model that can produce a manipulation program in order for a robot to manipulate its surrounding to reach a goal state given an initial state. The work focuses here on a neruo-symbolic approach that includes several specialized submodules. Despite these differences all submodules can be trained end-2-end without intermediate supervision. The experimental results are promising.Overall this is a well written paper with a very relevant topic and proposed method for this workshop. I am hesitant on giving this paper the highest score due to two reasons that are somewhat correlated. The overall methods section in "Technical Approach" is quite minimilistic and scarce in details. E.g. particularly the description of the visual extractor is kept very short. Even if it is mainly based on previous work I believe it would be helpful to have some more information overall in this technical approach section. This leads to one of the more important contributions of the work, the single loss function (section 3.5), to be less obvious, i.e. how the gradient flows through to all initial modules. And particularly in my view this is the most interesting feat of the work, i.e. that training all submodules works via this one loss. Maybe adding more explicit mathematical notations in Section 3 could help for the comprehensability here.   |||| rating: 2: Good workshop paper, Accept |||| confidence: 2: The reviewer is confident that the evaluation is correct	## SummaryThe paper proposes a new neuro-symbolic framework, which can perform learning to translate instructions to grounded robot plans. The addressed problem is to solve planning problems where the input is given as a pair of a state as a depth image and natural language instruction. The task is difficult since the agent needs to understand both the visual state and the natural language instruction and to perform planning on top of them. To address the problem, the paper proposes a new framework, that consists of language reasoner, visual extractor, visual reasoner, and action simulator. Each component processes information so that the entire system can execute symbolic programs defined in the DSL in a differentiable manner to solve the visual planning problem. In the experiment, the proposed approach outperformed a neural baseline, and moreover, it showed strong generalization results in terms of the number of objects and the number of steps of the planning. ## Pros- The paper is very well written- The paper has a good technical quality, i.e., everything is formulated without technical flaws- The proposed approach is novel in the sense that it solves visual planning problems using the NS-CL approach beyond VQA tasks- The paper leads to many important applications## ConsThe paper is overall well-written, however, I noticed some minor points that can be addressed.- In Fig. 1, Visual Extractor has Relational embeddings, however, it is not explained in the main text. If not the case I'm missing something, please explain it properly somewhere in the paper.- In line 107, I think Action simulator should be capitalized as Action **S**imulator### Questions- The limitation of the proposed approach is not discussed in the paper. What would be the limitation?- Is the proposed framework capable of parallelized batch computation? The implementation of neural networks in general can process a given batch of examples in parallel on GPUs. For the proposed approach, if the user gives several examples as input, are they processed in parallel? If not, how long does it take to train the proposed model? Would it take longer compared to the neural-based baseline? |||| rating: 2: Good workshop paper, Accept |||| confidence: 2: The reviewer is confident that the evaluation is correct
nEJMdZd8cIi	projUNN: efficient method for training deep networks with unitary matrices	long	This paper proposes a method (projUNN) for efficiently training neural networks with parameters that are constrained to be unitary/orthogonal matrices. The main strategy is to obtain a low rank approximation of the gradient update and then use Riemannian gradient descent either via retraction (method projUNN-D) or exact computation of the exponential map (method projUNN-T). Overall, the paper is clear and well written; the appendix provides a nice mathematical background/review, many interesting remarks/extensions/experiments. The main strength of the paper arises from the observation that low rank matrices can accurately approximate gradients (where fidelity is measured by residual frobenius norm error). This approximation allows the proposed method to be essentially a factor of O(n) faster than its competitors.One potential downside is that the proposed method (projUNN) and some of the baselines use eigen decomposition/QR decompositions which suffer some overhead due to parallelization issues in the GPU, causing some methods to appear slower in the experiments. Authors mention that this issue could be potentially fixed in the future with updates to DL libraries. Yes  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	The paper studies the acceleration of unitary neural network training. By exploiting the low-rank structure of gradients (done with sampling), the authors design two efficient update methods of unitary matrices based on gradient descent: one that directly projects the updated weight to the unitary manifold, and one that projects the gradient to the tangent space followed by a rotation mapping. Experimental results on several RNN tasks show that the proposed algorithm is on par with existing unitary neural networks while achieving nearly optimal runtime complexity. The authors also perform some preliminary study on training orthogonal convolution layers.  The paper is well-motivated and clearly-written. The proposed methods are simple-yet-effective, and the theoretical results are strong (which I have checked in details). Comparison with prior works looks comprehensive. Due to my limited knowledge in related works, I will leave the evaluation of novelty as well as experimental results to other reviewers.  Yes. The authors discussed the numerical instability of the direct method as well as the memory concern when applied to orthogonal convolutions.  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	The paper present two efficient update rules for neural networks with unitary matrices parametrized as full n x n arrays. Unitary matrices have attractive properties for neural networks since they preserve the norm of vectors they are multiplied to, and unlike orthogonal matrices, they have differentiable parametrizations though the main drawback to their use is that full parametrizations and/or gradient updates are often inefficient ( O(n^3) as opposed to O(n^2) of arbitrary matrices ). This paper proposes efficient O(k n^2) gradient updates, where k is a gradient rank hyperparameter. The methods are based on the hypothesis that the training gradient w.r.t. the unitary matrix can be well approximated as a low-rank matrix. They propose to use existing randomized SVD methods to compute the low-rank approximationThe first method PROJUNN-D first computes a low-rank approximation of the gradient, then it applies a standard SGD step to the unitary matrix, and projects the result back to the unitary group, which can be done efficiently by exploiting the fact that the perturbation was low-rank. The author comments that this method can be subject to numeric instability.The second method PROJUNN-T first computes a low-rank approximation of the gradient, then it rotates the unitary matrix along a projection of the approximated gradient on the unitary Lie algebra.PROJUNN-D and PROJUNN-T are evaluated on learning a random unitary matrix and, as a URNN update rule, on various standard RNN benchmarks, with competitive results and a substantial reduction of runtime.An extension to convolution is also discussed, but it is not shown to be competitive to vanilla convolution. Strengths:- Very compelling result, might enable making URNNs practical- Very good exposition- Extensive experimentsWeaknesses:- Both methods seem fairly complicated to implement, full peudocode algorithm descriptions (including the low-rank gradient approximation) would be appreciated. Hopefully code will be released. None  |||| rating: 9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	The authors propose two optimization algorithms for training deep networks with unitary weight matrices. Then they extend their approach to unitary convolutional operators. Unlike existing approaches, they exploit the low-rank structure of gradients that is a consequence of the relatively small batch sizes used in SGD-like optimization. This way they achieve significant speedups over existing unitary training approaches. Strength- The authors propose two novel projected gradient step procedures for training constraint to the space of unitary matrices. In contrast to exisitng approaches, we achieve more computational efficiency by exploiting the low-rank structure of gradients that is present in minibatch training.- Experimental evaluations show that the low-rank approximation even allows ranks substantially smaller than the batch size ( times sequence length for RNN), increasing the computational complexity even more.- Discussion of related work places this work among existing work, clearly stating advances and differences.- The extension to convolutional layers by exploiting the relation between the Fourier transformation and the discrete convolution operation enables the use of unitary operators in the image domain. This may allow users more control about properties of the linear operators that have been linked to generalization in deep networks like spectral norm, stable rank or eigenvalue decay.- Source code for recurrent networks is available which facilitates widespread use. Unfortunately the CNN code was not available in time for the reviewWeaknesses- The experiments mostly show technical advantages like increases in computational performance, not benefits in applications where this technique is strictly required The authors have adequately addressed the limitations. I cannot think of any potential negative societal impact  |||| rating: 8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
eRBVi61Vct1	Robust Rent Division	long	This paper provides new algorithmic and complexity results for the problem of computing envy-free solutions to the fair division problem. Here, n agents must each be matched to one of n rooms given their valuations for the rooms and split the total rent for the n rooms among them, and the agents have quasilinear utilities, i.e., their utility for receiving a room is its value minus their payment which is their share of the rent. Previous work proves that an envy-free solution to the rent division problem with quasilinear utilities always exists and can be computed in polynomial time using a linear program to compute the prices that result in envy-freeness. However, existing approaches to compute such envy-free allocations are not robust to perturbations to agent's utilities (such as the Spliddit algorithm which is focused on maximizing the utility of the agent with lowest utility).The main conceptual contribution of this paper is a new objective: maximizing robustness to perturbations in agent utilities under the envy-freeness constraint. Two such objectives are considered under a Gaussian noise model, i.e., when agent's true valuations are assumed to be "drawn" from a distribution obtained by adding Gaussian noise to their reported valuations:- Maximizing the probability that the allocation is envy-free w.r.t. this distribution- Minimizing the expected envy, i.e., the sum total of envy experienced between every possible pair of agents, w.r.t. the distributionGiven a set of samples from this distribution, there are polynomial time algorithms that compute the optimal solution w.r.t. either of these objectives and the set of samples. The main technical contributions are:- Sample complexity results for the number of samples needed to provide a guarantee that the solution computed by a polynomial time algorithm is at least within some tolerance parameter of the optimal solution with high probability for either of the above objectives.- Computational hardness results for computing the optimal solutions w.r.t. the distribution of valuation profiles Strengths:- [Relevance] The paper uses several tools from learning theory to arrive at significant technical results.- [Novelty] The proposed objectives of maximizing robustness are novel to the best of my knowledge.- [Significance] The paper makes several important and interesting conceptual and technical contributions which I believe will be of interest to the large computation social choice community. The future directions pointed out are well chosen and I agree that they are interesting questions for future work (e.g. how to elicit uncertain valuations from users on platforms like Spliddit and their implications on computing robust solutions)- [Writing] The paper is well written and organized and I agree with the authors' choice of which proofs are included in the main paper or left to the appendix. The discussions of the motivation and related work are sufficient given the space constraints.- [Soundness] I was able to verify the main results and they seem sound. However, I was unable to check all of the results (e.g. the computational hardness results) thoroughly due to a lack of time.Weaknesses:- No significant weaknesses that must be addressed in a conference publication.- It would have been interesting to see at least experimental results with more noise models. However, this is a minor nitpick given the space constraints. I believe the authors have adequately addressed the limitations of this work and proposed meaningful avenues for future work.  |||| rating: 8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper focuses on the problem of fair rent division: how does one split a total cost among agents in a way that respects the individual agents' valuations and returns an allocation and a cost split that is envy free? This paper focuses on the case where there is uncertainty in the valuations reported by the agents. The paper therefore introduces what they call the lexi-slack solution which remains envy free with as large a radius as possible from the agent valuation reports. They also look at the case where we get samples from a valuation distribution and we want to maximize the probability of selecting an EF allocation with a small number of samples.  ++ The paper is extremely well written, it is easy to follow, and all problems and notation are clear. The discursive style is very nice and overall it was a very fun and interesting paper to read.++ The proofs and results seem correct to me and are written very clearly. While I did not check the proofs in detail I was able to follow most of the arguments and they seemed reasonable.++ I really enjoyed the sample / learning portion of the paper and found that interesting. One question/limitation I would have liked to see discussed is how this sampling would happen in the real world? Would one agent report their valuations multiple times??? I don't see how this would be practically applicable.-- I find the claim: "We end with some experiments on data taken from Spliddit. They suggest that our three new rules83 significantly outperform the Spliddit maximin rule on robustness metrics." a little disingenuous. Of course the method that returns an EF allocation that has as a secondary constraint these other robustness metrics is better, this is not surprising. What would greatly strengthen the claim is comparing to some other heuristically optimized robustness metric. While this is really not that necessary since lexislack is basically this (since it doesn't minimize expected envy which is NP-hard) but this claim could be written in a more even handed way and/or evaluated against other poly-time heuristics that get stacked on the maximin rule.-- None of the experiments contain error bars or confidence intervals for especially Figure 1. So it's basically impossible to tell what the difference are and whether or not they are significantly different. The empirical claims would be much improved with some error bars or empirical confidence intervals. One question / limitation that isn't discussed but might be interesting for future work is any notion of the mechanism being adversarial. The writing currently points to the fact that maximin can cause envy for small changes in valuations but what if the center is choosing these in a way to benefit one agent or another. This isn't a hard limitation but the discussion is a bit taught in part 7 so a bit more would be nice.Building on this, there is a discussion of the limitations of the L_1 distance metric, were other metrics used? Why L_1 and not others given that even the authors say that L_1 can over optimize for one agent... a bit more discussion would be nice.  |||| rating: 8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	The paper studies the problem of robust envy-free rent division.  In the conventional problem variant, given n individuals and their evaluations for n rooms, the goal is to find a rule to allocate the rooms and fairly split the rent such that everyone is satisfied with the room and the split payment (i.e., allocation is envy-free).  Prior literature assumes that the individuals' valuations are accurate. This paper proposes robust allocation rules such that the allocation remains envy-free (or maximizes the probability of being envy-free, or minimizes expected envy) even if the reported evaluations deviate from the true ones. Strengths: 1. The paper is very well-written, and the organization of the results make it easy to follow. The paper studies several notions of robustness against different uncertainty around the true evaluations.  The l1 notion seems quite natural, and yields a polynomial-time algorithm.  It seems that l_inf robustness would do similarly, although the paper doesn't state that explicitly (just that Prop 3.1 essentially goes through for l_inf robustness).  Distributional variations are interesting as well; although they yield NP-hard optimization problems (but poly sample complexity), the associated ILPs seem fast enough in practice.2. There is a particularly interesting sample complexity result for the distributional variant of this problem in which the authors derive VC dimension of the allocation "hypothesis space" (which is polynomial).  This nicely combines with relatively standard arguments to get polynomial sample complexity.Weakness: 1. I found the l_1 and l_inf models of robustness considerably more convincing than the distributional models, for a few reasons.  First, l_inf seems to address the main limitation of l_1 that the authors highlight in the introduction.  Second, l_p models capture a nice intuition related to incentive compatibility: that while people may misreport valuations, they will typically stick relatively close to true valuations (although admittedly here the Gaussian model also makes good sense).  This is not to say that the distributional models are bad; it's simply that they seem somewhat forced here, in the sense that added value seems low, and increased complexity therefore too weakly motivated.2. There isn't any discussion of incentive compatibility.  This is natural here, since it conflicts with envy-freeness (if I recall correctly), but it would still be useful to understand how the notions of robustness impact manipulability, at least one compared to the other.3. It would also be useful to include computational results for Lexislack, perhaps as a part of Fig 2 (if it makes sense visually), or separately. N/A  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper proposes three robust approaches for the envy-free rent division problem: the lexislack approach that looks for allocations that remain envy-free for a range of valuations around the input one, an approach that maximizes the probability of envy-freeness, and one that takes the magnitude of envy into consideration by minimizing it. The authors provide time complexities for obtaining solutions for all these approaches, as well as the probabilistic bounds that guarantee the quality of the solutions of the latter two approaches. The robustness and the computation times of the proposed approaches are demonstrated on user data collected from Spliddit. Strengths:The models proposed in this paper are very practical. My favourite part is the establishment of Theorem $4.1$, wherein the authors intelligently parallel a standard result in PAC learning to their framework and derive the probabilistic bound for their solution. Weaknesses:Some places can be better elaborated and several typos should be corrected. Line $108$, Page $3$ (Section $2$): "...each agent $i\in N$ and each room $r\in R$" should be "..each $(i,r)\in N\times R$"?Lines $137$ to $138$, Page $4$ (Section $3$): in the formulation of "${\rm slack}$", "$\Delta_{ir}$" should be "$\Delta_{ir}(\sigma, p)$"?Line $164$, Page $4$ (Section $3$): "$n^2$ values in $\Delta(\sigma,p)$" should be "$\{\Delta(\sigma,p)\}_{(\sigma,p)\in N^2}$".Line~$231$, Page $6$ (Section $4$): "pair $i,j\in N$" better to say "pair $(i,j)\in N \times N$"?There are several other typos or grammatical errors. I would suggest proofreading the paper carefully.  |||| rating: 8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
N6zHSyChCF2	Discrete Compositional Representations as an Abstraction for Goal Conditioned Reinforcement Learning	long	The paper proposes a learning framework for compositional representations of goals for goal-conditioned RL, and proposes to obtain a coarse specification of goals using discretization. The goals reside in a low-dimensional representation space that is obtained from high-dimensional sensory data,  Strengths1. The paper shows favorable performance over several baselines on multiple environments. Weaknesses1. Why is it necessary to use two different autoencoder architecture for environments with different difficulty, if the author claims that representation learning method for observations can be used as described in Section 3.1? 2. The method assumes that the observations and goals come from the same space, restricting the method from being used when goals are specified in a different modality. This assumption should be discussed in the limitations.3. The paper claims that the compositional representation helps encode goals into a semantically meaningful latent space (line 77), for example based on the example in Figure 1, I would expect some segments to correspond to goal colors and some segments to correspond to fruit types. However, HRL experiments in Section 5 do not provide evidence that the learnt goal representations are semantically interpretable.  The limitations and societal impact are not discussed. One limitation is that while a discretized goal representation enables composition, the composition is not necessarily interpretable, which is claimed in the introduction (line 78).   |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper proposes to use discretized self-supervised representations for goal encoding in goal-conditioned RL. Specifically, the paper proposes using a codebook of L discrete codes to encode each of G chunks of a state/goal representation vector. The paper evaluates this method on a variety of environments for goal-conditioned and hierarchical RL, and demonstrates improved learning efficiency consistently, and improved generalization in several cases. Strengths:* The challenge of representing goals for RL is important, and the paper does a good job of motivating the problem and the proposed solution.* The solution is relatively simple to implement, and the breadth of established evaluation environments and algorithms suggests that this approach will be generalizable to other similar settings* The baseline comparisons seem generally strong, as they are taken from established works in the area.Weaknesses:* Discretized representations—even compositional ones—do not always scale well to high-dimensional, complex spaces. While the paper emphasizes the fact that the goals and states come from noisy, high-dimensional sensory inputs, the underlying goal spaces in each case are effectively 2D (goal positions to navigate to, to push a puck to, etc.), and thus it is unsurprising that a discrete representation would be effective. The strength of the paper would therefore be improved by also evaluating performance on higher dimensional goal spaces; for example, positioning 5 or 10 different objects in distinct positions with the robot rather than just 1, or stacking objects in particular places and orders, making sandwiches with different ingredients, etc. These experiments might be unreasonable to demand in the present submission—as the authors note, they evaluate on many environments from prior related works—but they would make the paper much stronger. Otherwise, it would be worth discussing this as a possible limitation explicitly.* One aspect that seemed unclear in the current presentation is whether the goal reaching policy is trained according to the continuous goals produced by the higher-level policy or the discretized versions of them. For example, if the goal reaching policy is rewarded for getting within a Euclidean distance of 0.5 of the goal, is that based on the continuous or discrete representation? I assume the latter, but in that case the discretization process changes not only the way that the goal is represented to the agent, but the distribution of goals the agent is trained on. In particular, it seems like it might result in the goal reaching policy having a more consistent set of goals, which might explain some of the improved learning efficiency. These two factors could be disentangled in some ways, for example by discretizing the goal when inputting it to the agent, but still rewarding the agent based on the continuous representation. Exploring these issues would help to clarify the benefits of the method.* Some more ablations could be useful. For example, what if the L latent vectors are fixed to reasonable values (e.g. the final codeword values of a prior training run) rather than updated with an exponential moving average? This might relate to the prior point, and would help identify whether something about the updating process is playing a role in the benefits, or whether it’s just the discretization. * As far as I could find, the paper does not specify the codebook size L anywhere in the main text or the supplement. This detail is relevant to interpreting how discrete the encoding actually is, as well as for future researchers who want to use the method.* To my understanding, text that is reproduced from a prior paper should be clearly marked as such; a non-trivial amount of section 3.2 is copied verbatim from one of the references without clearly communicating that fact (although the reference is cited as a source for the method). The paper should be transparent about details that are reproduced from prior work. * A minor point, but the paper says “[26], which generalizes the vector quantization used in VQ-VAE [53] to a list of discretized codes instead of a single discretized code.”–the VQ-VAE paper also used arrays of codes rather than a single code in all applications; for example a grid of codes for images or an array of codes for sequences. It might be good to clarify this point. See above, but briefly:* It's unclear whether discretized methods would perform well in more complex settings; this should be explored and/or discussed.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	The paper presents an approach to improve the performance of learned goal-conditioned policies, by discretizing the goals using Vector Quantization. They call their framework Discrete Goal-Conditioned Reinforcement Learning (DGRL), and it consists of 3 stages.1) learn representations of the visual input using self-supervised learning, using either an autoencoder or an InfoMax objective.2) Process the continuous latent representations via a discrete codebook (generalisation of VQ-VAE to multiple codes) to obtain a discretized code.3) Use the pre-trained model on RL tasks, either by giving discrete goals and maximizing an intrinsic reward which is a function of the similarity between the given goal and the discretized embedding of the current observation, or in a HRL setting, where a high level controller is tasked with outputting goals, which are then discretized and used to condition the low level policy.The authors provide a theorem that the goal discretization improves the lower bound of the expected sum of rewards of unseen goals. They then provide a series of results comparing their method to a number of different baselines and different task domains. The task domains they chose were a number of navigation tasks in a gridworld environment with pixel observations, a set of exploration tasks in the MiniGrid environment, a maze navigation task in 2D, ant manipulation control and navigation domains, and the Sawyer Robotic Manipulation Task. In the majority of the presented tasks, variants of DGRL have shown to provide some benefit in either sample efficiency or higher final performance. ### Strengths1. The method is simple to implement in existing architectures and uses established approaches that have been developed for some time now.2. The authors did a comprehensive empirical study in many different domains and tasks. It is very useful to see how the method performs in all these domains.3. The paper is overall very well written, and the ideas and results are communicated clearly.### WeaknessesThe method does not provide a significant improvement in most environments, nor I consider to be necessarily original (see [1,2] for examples using a very similar idea). It is of course interesting to know the benefits of this addition, but as the results show, the results are comparable for the majority of tasks, except in some cases of gridworld or ant maze navigation and the sawyer arm tasks, where the DGRL seems to help with sample efficiency. For example, in Table 1, it is clear that some choices of G can even considerably harm performance compared to the baseline (see G=8). To me, this is not necessarily a reason to reject, as this idea is something that many others will think to try at some point. Publishing genuine empirical results showing the realistic improvements that should be expected by a simple addition to the method, such as the one presented in this work, should be encouraged in my opinion.In terms of presentation, I found it a bit odd that the related work was in the appendix. Perhaps a bit part of the introduction could have been removed or delegated to the appendix, as it refers to high level ideas about goal grounding and goal specification which are not directly addressed by the paper. I have a comment about the use of the term “compositionality” in the paper used to refer to the discrete representation, which I elaborate more on in the “Questions” section below. I would urge the authors to remove the term compositional from the title as well. Please see the questions section below.[1] Ozair, S., Li, Y., Razavi, A., Antonoglou, I., Van Den Oord, A. and Vinyals, O., 2021, July. Vector quantized models for planning. In International Conference on Machine Learning (pp. 8302-8313). PMLR.[2] Fernández, F., Borrajo, D. (2000). VQQL. Applying Vector Quantization to Reinforcement Learning. In: Veloso, M., Pagello, E., Kitano, H. (eds) RoboCup-99: Robot Soccer World Cup III. RoboCup 1999. Lecture Notes in Computer Science(), vol 1856. Springer, Berlin, Heidelberg. https://doi.org/10.1007/3-540-45327-X_24 I haven’t seen any critical assessment of limitations of the method, and would be very interested to hear the authors’ views on this. For example, how many different goals can we express in this method? How easily can this scale? How different are the resulting discrete codes between them?  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
HkNGYjR9FX	Learning Recurrent Binary/Ternary Weights	long	The paper proposes a method to achieve binary and ternary quantization for recurrent networks. The key contribution is applying batch normalization to both input matrix vector and hidden matrix vector products within recurrent layers in order to preserve accuracy. The authors demonstrate accuracy benefits on a variety of datasets including language modeling (character and word level), MNIST sequence, and question answering. A hardware implementation based on DaDianNao is provided as well.Strengths- The authors propose a relatively simple and easy to understand methodology for achieving aggressive binary and ternary quantization.- The authors present compelling accuracy benefits on a range of datasets.Weaknesses / Questions- While the application of batch normalization demonstrates good results, having more compelling results on why covariate shift is such a problem in LSTMs would be helpful. Is this methodology applicable to other recurrent layers like RNNs and GRUs? - Does applying batch normalization across layer boundaries or at the end of each time-step help? This may incur lower overhead during inference and training time compared to applying batch normalization to the output of each matrix vector product (inputs and hidden-states). - Does training with batch-normalization add additional complexity to the training process? I imagine current DL framework do not efficiently parallelize applying batch normalization on both input and hidden matrix vector products.- It would be nice to have more intuition on what execution time overheads batch-normalization applies during inference on a CPU or GPU. That is, without a hardware accelerator what are the run-time costs, if any.- The hardware implementation could have much more detail. First, where are the area and power savings coming from. It would be nice to have a breakdown of on-chip SRAM for weights and activations vs. required DRAM memory. Similarly having a breakdown of power in terms of on-chip memory, off-chip memory, and compute would be helpful. - The hardware accelerator baseline assumes a 12-bit weight and activation quantization. Is this the best that can be achieved without sacrificing accuracy compared to floating point representation? Does adding batch normalization to intermediate matrix-vector products increase the required bit width for activations to preserve accuracy?Other comments- Preceding section 3.2 there no real discussion on batch normalization and covariate shift which are central to the work’s contribution. It would be nice to include this in the introduction to guide the reader.- It is unclear why DaDianNao was chosen as the baseline hardware implementation as opposed to other hardware accelerator implementations such as TPU like dataflows or the open-source NVDLA.  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	This work proposes a method for reducing memory requirements in RNN models via binary / ternary quantisation. The authors argue that binarising RNNs is due to a covariate shift, and address it with stochastic quantised weights and batch normalisation.The proposed RNN is tested on 6 sequence modelling tasks/datasets and shows drastic memory improvements compared to full-precision RNNs, with almost no loss in test performance.Based on the more efficient RNN cell, the authors furthermore describe a more efficient hardware implementation, compared to an implementation of the full-precision RNN.The core message I took away from this work is: “One can get away with stochastic binarised weights in a forward pass by compensating for it with batch normalisation”.Strengths:- substantial number of experiments (6 datasets), different domains- surprisingly simple methodological fix - substantial literature review- it has been argued that char-level / pixel-level RNNs present somewhat artificial tasks — even better that the authors test for a more realistic RNN application (Reading Comprehension) with an actually previously published model.Weaknesses:- little understanding is provided into _why_ covariance shift occurs/ why batch normalisation is so useful. The method works, but the authors could elaborate more on this, given that this is the core argument motivating the chosen method.- some statements are too bold/vague , e.g. page 3: “a binary/ternary model that can perform all temporal tasks”- unclear: by adapting a probabilistic formulation / sampling quantised weights, some variance is introduced. Does it matter for predictions (which should now also be stochastic)? How large is this variance? Even if negligible, it is not obvious and should be addressed.Other Questions / Comments-  How dependent is the method on the batch size chosen? This is in particular relevant as smaller batches might yield poor empirical estimates for mean/var. What happens at batch size 1? Are predictions of poorer for smaller batches?- Section 2, second line — detail: case w_{i,j}=0 is not covered- equation (5): total probability mass does not add up to 1- a direct comparison with models from previous work would have been interesting, where these previous methods also rely on batch normalisation- as I understand, the main contribution is in the inference (forward pass), not in training. It is somewhat misleading when the authors speak about “the proposed training algorithm” or “we introduced a training algorithm”- unclear: last sentence before section 6. |||| rating: 8: Top 50% of accepted papers, clear accept |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	* SummaryThis paper proposes batch normalization for learning RNNs with binary or ternary weights instead of full-precision weights. Experiments are carried out on character-level and word-level language modeling, as well as sequential MNIST and question answering.* Strengths- I liked the variety of tasks used evaluations (sequential MNIST, language modeling, question answering).- Encouraging results on specialized hardware implementation.* Weaknesses- Using batch normalization on existing binarization/ternarization techniques is a bit of an incremental contribution.- All test perplexities for word-level language models in table 3 underperform compared to current vanilla LSTMs for that task (see Table 4 in https://arxiv.org/pdf/1707.05589.pdf), suggesting that the baseline LSTM used in this paper is not strong enough.- Results on question answering are not convincing -- BinaryConnect has the same size while achieving substantially higher accuracy (94.66% vs 40.78%). This is nowhere discussed and the paper's major claims "binaryconnect method fails" and "our method [...] outperforms all the existing quantization methods" seem unfounded (Section 5.5).- In the introduction, I am lacking a distinction between improvements w.r.t. training vs inference time. As far as I understand, quantization methods only help at reducing memory footprint or computation time during inference/test but not during training. This should be clarified.- In the introduction on page 2 is argued that the proposed method "eliminates the need for multiplications" -- I do not see how this is possible. Maybe what you meant is that it eliminates the need for full-precision multiplications by replacing them with multiplications with binary/ternary matrices? - The notation is quite confusing. For starters, in Section 2 you mention "a fixed scaling factor A" and I would encourage you to indicate scalars by lower-case letters, vectors by boldface lower-case letters and matrices by boldface upper-case letters. Moreover, it is unclear when calculations are approximate. For instance, in Eq. 1 I believe you need to replace "=" with "\approx". Likewise for the equation in the next to last line on page 2. Lastly, while Eq. 2 seems to be a common way to write down LSTM equations, it is abusive notation.* Minor Comments- Abstract: What is ASIC? It is not referenced in Section 6.- Introduction: What is the justification for calling RNNs over-parameterized? This seems to depend on the task. - Introduction; contributions: Here, I would like to see a distinction between gains during training vs test time.- Section 3.2 comes out of nowhere. You might want to already mention why are introducing batch normalization at this point.- The boldfacing in Table 1, 2 and 3 is misleading. I understand this is done to highlight the proposed method, but I think commonly boldfacing is used to highlight the best results.- Figure 2b. What is your hypothesis why BPC actually goes down the longer the sequence is?- Algorithm 1, line 14: Using the cross-entropy is a specific choice dependent on the task. My understanding is your approach can work with any differentiable downstream loss? |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
GXJPLbB5P-y	Simplifying Models with Unlabeled Output Data	long	This paper proposes a framework for problems where the output has some validity constraints, for e.g. the output must be a valid python program that must compile. These kind of problems arise naturally in settings such as pseudocode to program, and moreover there are many more unlabelled valid programs that are easily available (e.g. on Github) than there are labelled examples - i.e. paired pseudo-code, code examples. In this case, the authors propose the following framework of predict and de-noise: 1) train a de-noiser that learns to map synthetically noised versions of the un-labelled valid examples and 2) compose a predictor on the labelled examples with this de-noiser so that end predictions belong to the space of valid programs. The idea proposed in the paper is simple and intuitive, and the authors show that this approach leads to an improvement of 3-5% on the SPOC pseudo-code to code data-set. The authors also provide some theoretical justification why such a composition is the right thing to do.Overall I like the idea in the paper, and such an approach has been used in NLP for various problems such as spelling correction etc. Besides the predict and de-noise framework another approach that is used in machine translation is the idea of using back-translation, which in the context of pseudo-code to code would look something like this: 1) train a sequence model A such as transformer on code to pseudo-code, 2) use the trained model A to generate paired training data for the unlabelled code (obtained e.g, from Github) by out-putting pseudo-code for it, and 3) train a final model on original labelled pseudo-code to code data plus the artificially generated pseudo-code to code data. I wonder how this would compare to the proposed method?  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	The authors propose a more data-efficient way to train generative models with constraints on the output; specifically they evaluate on image generation and pseudocode-to-code (SPoC) tasks. They train two separate models, a “predictor” and a “denoiser”, which they then compose: the output from the “predictor” is further processed by the “denoiser”. For the SPoC task they show an improvement of 3-5% over a simple transformer baseline. The authors suggest a simple idea to make use of unlabelled data, should it be available. They use it to perturbate the unlabelled data and use the (perturbed example, example) pairs to train a denoising model. They argue that this should theoretically simplify the task of the predictor, and show improvements on several tasks. I believe that this is an interesting idea, and practically useful in the cases where data is sparse. However, the results that they demonstrate do not seem very strong, and I would have liked to see this technique demonstrated on more competitive tasks to better gauge how well it works. The improvement of 3-5% they state seems like a low gain over a simple baseline, that may also be achievable with other techniques. Clearly state your recommendation (accept or reject) with one or two key reasons for this choice. I do recommend this paper to be accepted, because it clearly presents an interesting idea. The recommendation is a “weak accept” though, because the experimental evidence for the technique is not convincing enough to me. I would have expected significant gains on a well understood task, clearly attributable to the technique. |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The paper introduces a “predict-and-denoise” model for structured prediction, specifically for tasks where the output has to adhere to some constraints e.g. natural language, code etc. This framework allows leveraging of unlabelled output data to train the denoiser, which consequently allows the base predictor to be of low complexity that can potentially generalize with relatively fewer labelled data. The authors theoretically back their arguments basing their theory on a 2 layer ReLU model. The paper demonstrates the performance of this model on two tasks - font image generation, and pseudocode-to-code translation and shows improvement in performance over previous works. +ves :- The paper is very well written and easy to follow. The motivation and the contributions are very clear, and the experimental section is also well detailed and organized. - To the best of my knowledge the framework of predict-and-denoise learned in a composed manner and using this framework to leverage unlabelled output data is a novel contribution of the paper. - The authors argue that this framework allows reduced complexity of the base predictor, backed theoretically for a 2 layer ReLU network. The authors have provided a detailed proof of their argument in the supplementary material, although I have not completely verified its correctness. Concerns :- I believe that the experimental section currently lacks fair comparisons, especially in the task of pseudocode-to-code translation. The authors compare their method with other methods for leveraging unlabelled data such as pre-training and back-translation. The authors show that predict-and-denoise framework can be applied on top of these existing approaches, and yields consistent improvement.  However, when comparing such combinations such as “pre-training/back-translation + composed” against pre-training/back-translation, the resulting performance is not  compared with an accordingly scaled base pre-training/back-translation model to have comparable number of parameters. With a very different number of parameters in the models being compared, it's hard to say where the performance benefit is coming from. - While the proposed method is complementary to approaches like pre-training and back translation, it will be helpful to also include comparisons such as “composed vs pre-training”, or “composed vs back-translation”. This will give an interesting comparison among these different ways of leveraging unlabelled output data. Again proper care needs to be taken about a comparable number of parameters. While I find the framework "predict-and-denoise" very interesting, I am not entirely convinced with its empirical performance reported in the current form and I have given my score accordingly. If the authors agree with my concerns, and can try to incorporate these changes during rebuttal, I will consider updating my score.   |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct
r2DdJQ9AJvI	TGEA 2.0: A Large-Scale Diagnostically Annotated Dataset with Benchmark Tasks for Text Generation of Pretrained Language Models	long	Recommendation: 2: Serious ethical issues that need to be addressed in the final versionEthics Review:The authors introduce the TGEA 2.0 dataset which is a Chinese dataset where the examples are generated by various pretrained language models. The dataset has been annotated such that the machine-authored texts can be assessed on various tasks within the broad categories of diagnosis tasks and pathology mitigation tasks.The main issue raised by reviewers is the risk of erasure and invisibility of linguistic variability in Chinese language training data. A recommendation was formulated in this regard.Ethics Documentation: To address the reviewers' concerns on erasure of specificities in the Chinese language, the authors offered to contact the developers of the publicly available models they are using with the idea of asking for information on the training data used for these models. The authors also offered to include data cards for th emodels and datasets, especially with respect to the varieties of Chinese. The goal is to identify varieties of Chinese other than Mandarin. More generally, authors propose to provide a clear description with respect to the variety of Chinese in the revised version of the paper. N/A  |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper presents a large-scale and curated dataset in Chinese along with two benchmarks for diagnosis (5 tasks) and pathology mitigation (2 tasks) to improve quality of generated texts from language models. The authors designed a thorough annotation process including data collection, training annotators in the pre-annotation phase and quality control by the feedback loop. The selected sentences for annotation cover 3 aspects: model, decoding strategy and prompt. They also provided a detailed analysis on the distributions of erroneous sentences produced by a variety of models with different sizes ranging from 110M to 2.6B parameters. The experimental results on the proposed benchmarks show that the diagnosis tasks are challenging and training a language model (here, GPT-2) on their dataset help reducing errors in the generated texts. 1. Data collection phase is well-implemented: The authors use different models, decoding strategies and prompt types (nominal, phrasal and sentential) with various domains (News, Wikipedia and Web Fictions) to diversify types of erroneous sentences.2. The annotation process and quality control are well-designed to annotate the large-scale dataset while maintaining its quality.3. The dataset has potential usages: Large language models in Chinese can benefit from training on this dataset to mitigating erroneous sentences. Also, the discriminative models can be trained to automatically detect errors made by language models. Thus, this work can be valuable for future research. 1. I have some concerns regarding the quality control:* L188: Who trained the first 4 reviewers? I would like to evaluate the quality of this dataset carefully, since their annotations are used as ground-truths to train other annotators.* L199: What is average performance of 7 well-trained reviewers? Are they trained by the annotations produced by the first 4 reviewers? The reason I asked is that they are the ones who guarantee the high-quality outcomes for this dataset.2. Although alpha-balanced loss was used, some diagnostic tasks such as Erroneous Text Detection, MiSEW Extraction suffer a heavy unbalance that may affect model training and evaluation, so the results on these datasets are not quite convincing.3. No statistics reported for the proposed tasks in two sets of benchmarks.4. The Word Prediction task in the pathology mitigation benchmark does not properly evaluate the ability of language models because there can be many correct predictions for the last token given each sentence.5. No qualitative examples (i.e., model predictions) for each benchmark task in the main text and the appendix.6. Some minor issues in presentation:* Numbers in Table 1 are quite small that makes it hard to read* L240: Error Correction task is missing.* L267: It should be MacBERT instead of MacBEERT.  |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The authors introduce the TGEA 2.0 dataset which is a Chinese dataset where the examples are generated by various pretrained language models. The dataset has been annotated such that the machine-authored texts can be assessed on various tasks within the broad categories of diagnosis tasks and pathology mitigation tasks. The main strength of this dataset is its scale: it substantially extends TGEA 1.0 to now consist of 195,629 annotated sentences. Such a dataset will be particularly useful in devising methods to assess the quality of the generated text from pre-trained language models. Also, the authors have taken great care for a sophisticated quality control process in order to ensure that the annotations for the various benchmarking tasks can be trusted. Further explanation or clarification is required for the following points:-The authors claim that the examples generated are diverse due to the different decoding strategies and also using 4 different pretrained language models. However, are 4 pretrained language models representative in the mistakes they make for future pretrained language models that have come out recently and to come out which are far larger in size and may have different pathological weaknesses?-It would be great if the novelty in the tasks beyond TGEA 1.0 could be clearly spelled out beyond just the scale of the dataset.-How valid is it to compare the pathological weaknesses of models in Chinese to English examples as in SCARECROW?  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	This paper proposes TGEA 2.0, the largest dataset for diagnosing typed errors made by pretrained language models. It is an extended version of TGEA, with various large language models and downstream tasks. The paper mainly compared its contribution with TGEA and ScareCrow. Several experiments are performed using the dataset, and experimental results on various downstream tasks show that there are large rooms exploring the proposed dataset. - It nicely expands previous TGEA in terms of scalability, annotation richness, etc.- Strict quality control on the construction process- Proposed MiSEW and pathology mitigation which can assist the annotation richness. - The intention of MiSEW extraction is plausible, but is somewhat overlapped with erroneous span location as a downstream task. Thus the necessity of the task should be more justified in some manner, such as qualitative analysis.- Proposed pathology mitigation should also be more explained in terms of why it should be jointly considered in future works.  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper contributes to understanding and reducing the text generation errors made by large pre-trained language models. The authors have created the largest Chinese language dataset of machine-authored texts and a substantial subset of the texts (>195k) were manually annotated at a fine-grained level and corrected for text quality issues (grammaticality and semantic coherence). The authors use the annotated data both to benchmark the best performance of 4 major PLMs (with variable architecture and scale) against each top-level error category and to test the extent to which fine-tuning with the human-corrected texts reduces the prevalence of these errors in the PLM outputs.  The size of the dataset and multiple annotations (erroneous spans and minimal set of error-related words and corrections for these words) provide an excellent basis for studying the nature of grammatical and semantic coherence errors in Chinese language automated text generated by current SOTA models performing at their best. As such, this dataset should be of considerable interest to researchers seeking to understand the persistence of certain error types in machine-generated text, and also, to understand, from objective evidence (vs subjective human evaluations, cf. Clark 2021), what kinds of errors may be indicative of machine-generated outputs and therefore may support the detection of machine-generated text. The further interest of this paper is the attempt to use the error-plus-correction MiSEW pairs to improve the quality of the generated text by reducing errors of these kinds in the output. Another strength of the paper is the clear and comprehensive description of the research process, including the annotation process, which adheres to annotation best practices in many ways (including an indicator of annotator confidence, pre-training to support annotator convergence to a high level of inter-annotator agreement and iterative re-training during annotation, etc). It would have been good to see Cohen's Kappa statistics (or similar) cited for the inter-annotator agreement in section 3.3. Average accuracy of annotators is mentioned ("average performance ... increases from 58.9% to 79.7%") and also "inter-annotator disagreement" but not measure of the latter is explicitly provided.   |||| rating: 8: Top 50% of accepted papers, clear accept |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	The work builds on by releasing a larger and higher quality version of a previously released dataset named TGEA. It is a comprehensive collection of machine authored texts in Chinese language that have been annotated for errors based on a novel ontology of errors. This ontology is based on data mining for frequently occurring forms of errors followed by supervision by expert annotators. Furthermore, systematic analysis to the annotated errors is performed to reveal patterns that are helpful in gauging the capability of various PLMs on different datasets.Lastly, the work goes on to validate whether errors found can be fixed automatically with pre-existing large language models and find  it hard to solve by modern means. This paves the way for 1) A dataset to analyze the kind errors PLM makes 2) Developing automated methods that can automatically correct the errors made by PLMs because existing SoTA is not enough to rectify it 3) Benchmark of the performance of SoTA on both diagnostic as well as pathological errors for future works to compare against. * Improvement over previous work where authors used the stochastic decoding strategy and repetition penalty which reduced the redundancy related errors frequency and hence also allowed energy to be focused on harder errors.* The number of annotated samples is large enough to gain confidence and mitigate risk of incorrect conclusions due to spurious correlations.* Benchmark results are shared on the presented dataset using SoTA models which allows the research community to have solid baselines to compare their research and findings against. Furthermore, by evaluating numerous PLMs on diverse datasets, the work also helps users of PLMs in deciding appropriate model for any given NLG task. * The reasoning behind choosing three point scale for asking confidence in annotation instead of standard Likert scale is not provided.* Any inspiration that was taken from related work is missing for the choice of annotator training methodology.* Given that beyond a certain threshold, PLMs show emergence capabilities. The work done here is called into question as whether or not the errors patterns in small PLMs (<5B parameters) are also prevalent in Large PLMs (> 100B) like PanGu-α (200B), Wu Dao 2.0 (1.75T).Grammar errors:1. L168 "think it"2. L190 "in three times"  |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
ZHkbzSR56jA	BASGD: Buffered Asynchronous SGD for Byzantine Learning	long	This paper studies distributed learning in the presence of Byzantine workers in the asynchronous setting. Its main contributions include generalization of the existing literature on Byzantine fault tolerance in distributed learning to incorporate the case of asynchronous learning. This generalization involves an algorithm, convergence analysis for the algorithm, and experimental results. While the results presented in the paper appear to be correct, I would like the authors to focus on the following points during their revision.1. In Section 2.2, the definition of Byzantine worker, it is not clear why the worker is being indexed with $k_t$? What is the meaning of $t$ in this usage of the worker index?2. The writing of the paper could use some proofreading. Some of the sentences are hard to parse on first read, while some other sentences suffer from grammatical errors. As a specific example, I could not understand the meaning of "Only when all buffers have got changed since the last SGD step, ..." in Section 3.1 until I reread the main parts of the paper.3. Related to the previous point, the discussion in Section 3.1 in general is hard to parse because of the notation and could benefit from revision. Also, $m$ in this section is undefined up to this point in time and it is not clear what it means.4. A number of aggregation functions have been proposed in prior works (see e.g. Adversary-resilient distributed and decentralized statistical inference and machine learning: An overview of recent advances under the Byzantine threat model). Do all of these previous aggregation functions satisfy the characterization of Section 3.2? It would be helpful to have some discussion of this.5. Theorem 1 and Theorem 2 leave something to be desired. Since the task is to engage in distributed learning, one expects to see some sort of speedup from the fact that $n$ workers are being used to divide up the work. However this speed-up does not seem to be coming up in the analysis or the discussion. In the absence of such a speed-up, it is not clear if the authors are really providing guarantees that are useful for distributed learning.6. It would be useful to discuss the impact of heavily delayed workers on the algorithm. What if the sum of the number of heavily delayed workers and Byzantine workers exceeds $r$?7. The plots corresponding to the experimental results are too small and should be modified to have bigger font and size.***Post-discussion period comments***The authors have done an adequate job of responding to my queries and have also revised the paper in light of the comments of all the reviewers. While the paper could always be improved, I believe it is now above the threshold of acceptance and it should be accepted into the program, if possible. I am raising my score for this paper in light of the discussion and the revised paper. |||| rating: 7: Good paper, accept |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	#### SummaryThis work proposes a method for Byzantine learning in a parameter-server setting using asynchronous updates, and without the need for storing training instances on the master node. They provide theoretical guarantees and some experiments on small-scale tasks.---#### OriginalitySynchronous methods for byzantine learning enable one to compare communicated gradients with one another in order to filter out byzantine workers. When updates are asynchronous, such a procedure cannot be employed because gradients are not necessarily communicated to the master node at the same time. Previous work on Asynchronous byzantine learning stores training instances on the master node to alleviate this issue. This work proposes instead to use gradient buffers on the server to eliminate the need to store training instances on the master node; this is the main selling point of the algorithm, and deems the method sufficiently novel in my opinion.However, the motivation for avoiding storing instances on the master node is not well fleshed out in my opinion. Though the applications to Federated Learning are mentioned, updates are typically performed locally on-device and *parameters* are communicated back to the master, and therefore this method is not applicable to that setting. It is unclear exactly what the privacy concerns are, but I would suggest reworking the motivation exposition a little.---#### Significance and QualityThe method is actually quite nice and intuitive. The significance relates back to improving the motivation, but the method may be of sufficient interest to the community.*On the theory*:Note that the bounded gradient assumption is very strong! when combined with the L-smoothness assumption, it implies a convergent subsequence a priori! In short, it is like assuming ahead of time that the algorithm convergence.$N^t_b$ is used in the main paper (e.g., page 5), but is only defined in the appendix on page 14. I would suggest including a one sentence description that these are the number of gradients stored in buffer $b$ at iteration $t$.Under asynchrony or non-iid data, theorem 1 does not guarantee convergence to a stationary point… even with a diminishing step-size. Moreover, even with iid data and fully synchronicity and a diminishing step-size, the algorithm still does not converge to a stationary point due to the extra variance term on the r.h.s. I do not mean to criticize the results; only to point out this fact. Moreover, these bounds are somewhat vacuous due to the presence of the non-degrading $D$ gradient boundedness term on the r.h.s of Theorem 1 in the variance term. With L-smoothness and the assumption of $D$-bounded gradients, as I mentioned above, all iterates of any objective (regardless of the algorithm), will remain with a ball of a stationary point, the size of which is proportional to $D$. (To the authors credit, I have seen similar bounds in previous byzantine learning methods).For Theorem 2, as I understand it, the $\alpha^{1/2}$ term actually has a $\mathcal{O}(1/\sqrt{T})$ dependence, so unless i’m missing something, why not remove the constant $\alpha$ and substitute in a quantity that decays with $1/\sqrt{T}$, and state the theorem results for all $T \geq$ some threshold (to satisfy the current $\alpha < 1$ constraint). this reformulation will make it clearer that the only lingering (non-decaying factor) is $A_1$, which is due to gradient bias.More generally, I am curious if there is a way to correct for the convergence errors and improve the results to guarantee convergence to a stationary point (and not some neighborhood thereof), given that a $\frac{1}{\sqrt{T}}$ step-size is employed. ---#### ClarityWork is sufficiently clear. One minor point is that Asynchronous SGD is missing a reference (common references for this method include Dean et al., or Bengio et al.) |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	The paper proposes a practical asynchronous stochastic gradient descent for Byzantine distributed learning where some of transmitted gradients are likely to be replaced by arbitrary vectors. Specifically, the server temporarily stores gradients on multiple  (namely $B$) buffers and performs a proper robust aggregation to compute a more robust from them. When $B = 1$,  BASGD is reduced to ASGD. They also conduct experiments to show the performance of BASGD. The paper is well written and easy to follow. All proof seems correct though I didn’t check very carefully. However, there are some issues:1. Compared to other asynchronous SGD methods, one advantage of BASGD is it doesn’t have the need of storing any samples on the server. However, I didn’t understand why such property is meaningful. The authors declare that it helps BASGD take less risk of privacy leakage. Noting the server store gradients on buffers and gradients may leak privacy [1]. If a third party can have access to the buffers, privacy leak can still happen. BASGD didn’t use techniques like differential privacy, so it seems ill-founded to say “BASGD takes less risk of privacy leakage”.2. In my opinion, Theorem 1 doesn’t guarantee that BASGD is able to find a stationary point of $F$ since the extra constant variance will not vanish when $T$ goes infinity. By contrast, ZENO++, a robust fully asynchronous SGD, could ensure convergence to a stationary point. This strikes me that the theorem is quite weak. Theorem 2 has the same problem.3. It makes me feel strange to assume the gradient is biased (Assumption 2). For example, If we want to minimize $F(w)$ but we use stochastic gradients computed from another $\tilde{F}(w)$ (that is totally different from $F(w)$), could we still guarantee the algorithm converges to the stationary point of $F(w)$? I am afraid the answer is NO. This thought experiment not only shows using biased gradients doesn’t help convergence but also shows Theorem 1 doesn’t guarantee the convergence of BASGD.4. Some parts of the algorithm are not well explored. For example, how the performance of BASGD changes when we vary the value of $q$ or the value of $B$. The experiment didn’t explore these aspects. Besides, Noting that the number of buffers $B$ is quite important for good performance, however, there is no investigation on how $B$ affects convergence and no suggestion on how to choose a proper $B$. In experiments, $B$ is set in advance for no reason.[1] Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In Advances in Neural Information Processing Systems, pages 14747–14756, 2019.---------I have read the authors' response. The authors have addressed most of my concerns, but I still think the motivation is a little farfetched. Considering the paper indeed explorees some aspects (in theories and experiments) of the use of buffers in asynchronous Byzantine Learning, I will improve my point to 5. |||| rating: 5: Marginally below acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	Review: This paper proposes BASGD which uses buffers to perform asynchronous Byzantine learning. In each SGD step, all workers compute gradients and send them to the server where their ad buffer is updated. When all of the buffers are updated, the server performs an model update. When a worker send a gradient to the server, it also pulls the latest model and compute the gradient on it no matter the server update the model or not. The main contribution in this paper is to introduce a new approach to do asynchronous Byzantine learning without storing training samples on the server like zeno++.=======================================================Pros:+ The problem of Byzantine learning in the asynchronous environment is interesting and has rarely been studied, especially under the assumption that the server has no training data.+ The idea of using buffer on the server to achieve (partial) asynchrony is interesting.+ This paper provides extensive experiments to demonstrate its effectiveness.=======================================================Concerns:- BASGD is not fully asynchronous as the paper claims to be. The server only updates the model when all of the buffers are non-zero. That is, the whole system will be slowed down by the slowest buffer. The remaining good workers, at the time of waiting, are computing the gradients on the same model weight. In this case, BASGD resembles SBL with larger batch size but at the cost of tolerating less Byzantine workers.- This paper does not explicitly state how to choose the number of buffers $B$ in order to achieve both asynchrony and Byzantine-robustness. In page 3, there are "BASGD introduces $B$ buffers ($0<B\le m$) on server". However, when $B$ is small, there is no robustness; when $B$ is large, there is no asynchrony.- The Definition 1, 4, 5, are not common in Byzantine robust learning. Are these definitions used only to make the proof easier?- The results in the theorems are not clearly presented. The $O(1/T)$ in Theorem 1 and 2 does not reflect how $B$, $q$ influence the convergence rate.=======================================================Minor comments:- The negative gradient attack and random disturbance attack are very easy to defend. It would be better to choose some more challenging attacks.- It is better to improve the writing in section 4 for better readability. |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
L7n7BPTVAr3	Leveraging Inter-Layer Dependency for Post -Training Quantization	long	The paper studies post-training quantization, and propose to leverage inter-layer dependency to improve the post-training quantized network. The authors introduce a number of approaches: AR, ASoftmax, improved initialization and AMixup, and verify the effectiveness of each part accordingly. The experimental results demonstrate superiority over a number of previous strong PTQ baselines across various network architectures and quantization configurations. In summary, despite the paper is methodologically incremental compared with past literature, the experiments are thorough and solid. Strengths:- The major strength for this paper should be the empirical results, which outperforms a number of strong baselines and pushes the limit of PTQ performance on low-bit quantized networks. The ablations are also thorough and complete.- The writing is clear and easy to follow.Weakness:- The paper is methodologically incremental as a large portion is built upon past efforts (BRECQ and QDROP). Activation regularization resembles knowledge distillation, a widely adopted approach in PTQ approaches utilizing calibration dataset. Asoftmax follows AdaRound and extends the range of discrete space in $h(v)$. Annealing mixup considers to add annealing process into the mixture of quantized and full-precision activations.   The authors improves the performance of post-training quantized networks under low-bit configurations. Despite there is still a large gap with the full-precision model, the empirical improvement in this paper is still an important step for futural PTQ research.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.	Based on the inter-layer dependency of quantization, this paper proposes a Network-Wise Quantization (NWQ) approach. NWQ arises overfitting and discrete optimization problems. In this paper, activation regularization is introduced to handle the overfitting issue. To solve the combinatorial optimization problem,  the authors use Annealing Softmax and Anealing Mixup.The experiments show that NWQ can achive significant improvements over previous works. Strengths:PTQ is widely studied, and the methods proposed in this paper outperfome previous SOTA by a large margin. The paper is easy to follow and the authors provides several ablation study to show the effectiveness of proposed techniques.Weakness:The paper shows some originality but is not novel enough. For example, Annealing Softmax and Annealing Mixup are largely based on AdaRound[32] and QDROP[41]. The authors clearly state the difference and demonstrate the effectiveness of modifications, but previous methods play a more important role in the discrete optimization problem.  Besides, the motivation and theoretical analysis of substituting RSeR with ASoftmax are absent.From finger 3, we can learn the efficiency of NWQ. However, the cost of each iteration is not clear. Does it require less or more computation/memory than previous methods in each iteration. Limited technical contribution. The proposed methods are largely based on previous papers. For the new techniques, no theoretical analysis is provided. For example, no theoretical analysis of ASoftmax regarding the better performance compared with RSeR.  |||| rating: 4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	This paper proposes a post-training quantization (PTQ) technique that aims to search for a network-level quantization policy, compared to the previous works that only considers quantization policies on a layer-wise level. To tackle the challenges of solving a larger scale combinatorial optimization problem, and the issue of overfitting, the paper proposes two approaches: Activation Regularization, and Annealing Softmax and Annealing Mixup. Experiments show the proposed approach achieved higher performance especially when quantizing networks to extremely low bits.  1/ The insight of this paper is straight-forward but very sensible -- quantization errors can cancel each other, and optimizing with a higher granularity can achieve better performance. More importantly, based on the insight, the authors proposes effective approaches that achieved this.2/ The paper is clearly written. The derivation and introduction of the techniques are explained very clearly. 3/ Nice ablation study shedding lights on the hyper-parameter sensitivity of the proposed method.  1/ Seems that the performance gain is not consistent. The performance gain of the proposed method is significant for some configurations, e.g., MobileNet with 2 & 3 bit quantization. However, the gain is quite small for others, such as MobileNetV2 with 4-bit quantization. Why is that? Is it simply because MobileNetV2 2&3 bit configurations do not have a strong baseline?2/ The proposed method seem to contain too many hyper-parameters, from the granularity of AR, to initialization scheme, to annealing schedules, etc. How difficult is it to find a good configuration for a new model and new target bits?  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	### Overall:To avoid the overfitting problem in quantized networks, the authors present several training strategies based on the previous Post-training Quantization (PTQ) methods, such as layer-wise activation fitting as proposed in BRECQ [1], Annealing Softmax (AS) to enlarge the searching space of AdaRound [2], and Annealing Mixup (AMixup) to smooth the training process of QDrop [3]. They conduct thorough ablation studies and experiments on several mainstream networks to fully verify the superiority of the proposed methods. ### Reference:[1] Brecq: Pushing the limit of post-training quantization by block reconstruction. ICLR2021[2] Up or down? adaptive rounding for post-training quantization. ICMLR2020[3] Qdrop: Randomly dropping quantization for extremely low-bit post-training quantization. ICLR2022 ### Strengths:- The experimental results seem strong. The authors conduct extensive experiments on ImageNet with different bitwidth. Especially, the ablation studies are quite clear, which should be encouraged.- The technique part of this paper is well written. I personally think this paper is simple and easy to follow.- "Annealing Softmax" seems new in the community of model compression.### Weaknesses:- Please refer to *Questions* and *Limitations*. The main concern is the novelty of this paper.  ### Comments on limitations:As claimed in the **Abstract**, "This process pays little attention to the dependency across the sub-nets, hence is less optimal." However, the main idea has been rarely discussed in the current manuscript, which makes the submission serve more like a technical report (i.e., the descriptions and effect of different improvements). The authors are encouraged to solve the major concerns in the final version.  |||| rating: 4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly. |||| confidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.
r1Nb5i05tX	The effectiveness of layer-by-layer training using the information bottleneck principle	long	This work is about layer-wise training of networks by way of optimizing the IB cost function, which basically measures the compression of the inputs under the constraint that some degree of information with respect to the targets must be preserved. Both terms of the IB cost function are formalized as mutual informations, but since in neural nets, the latent "compression" is a deterministic function of the inputs, a severe technical problems arises: the joint distribution between p-dimensional inputs X and the q-dimensional latent compression L is degenerate in that  its support lies in a space of dimension p (and not p+q as it would be in the non-degenerate case). As a consequence, no p.d.f. exists (with respect to the Lebesgue measure of R^{p+q}). Thus, defining mutual information is cumbersome. The paper attempts to overcome this problem by using a noisy version of the latent compression, i.e. L' = L + \epsilon, which can be seen as an "ad hoc" fix of this problem. Not too surprising, this additive noise works as a ridge-type (or weight-decay) regularizer, just as a Gaussian prior in regression.On one hand, I find this paper interesting, because it aims at carefully studying the proposed link between DNN training and IB optimization, thereby showing that layer-wise IB training indeed seems to work very well in practice. Such results are certainly interesting, both from a theoretical and from a practical point of view. On the other hand, I honestly think that on the conceptual side, this work does not make that many really interesting contributions. The observation that additive noise works as a weight-decay regularizer is in my opinion almost trivial, and any claims about experimental results "validating(!) the IB theory" seem to contain some degree of over-selling. In summary, I think that this is a paper that certainly contains some interesting ideas, but on the other hand I am not fully convinced about the significance and relevance of the findings.        |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	While overall the writing quality of the paper is high, the paper itself is a strong rejection.  I believe the analysis of the paper is at points flawed, and the experiments are minimal.  This work attempts to study the degree to which a layer by layer information bottleneck inspired objective can improve performance, as well as generally attempt to clarify some of the discussion surrounding Shwartz-Ziv & Tishby 2017.  Here, the authors study a deterministic neural network, for which the mutual information estimation is difficult (I(X,L)) and error prone.  To combat this they use the noise-regularized mutual information estimator (I(X; L+eps)).  To actually estimate the mutual information the authors use the MINE estimator of Belghazi (2018).  Here they suggest using the neural network itself as a structural element in the form of the discriminator to take advantage of the specific circumstances in this case.  Doing this ensured that their estimator diverged in the zero noise limit as expected.  From here they show some experimental results of the effect of their objective on an MNIST / CIFAR10 classification task.This paper fits into what is an increasingly large discussion in the literature, surrounding Information Bottleneck.  The paper itself does a very good job of citing recent relevant work.  Technically however I take issue with the framing of previous work in the last paragraph of the "Deep neural nets" subsection of Section 2.  Technically Achille & Soatto explicitly formed a variational approximation to the posterior over the weights of the neural network and so was not a "single bottleneck layer" as stated in the paper.  More generally at the end of that paragraph it is implied that the single bottleneck layer scheme "deviates from the original theory".  This is a misleading characterization of the original information bottleneck (Tishby et al 1999) in which there was a single random variable, a representation of the data (Z) satisfying the Markov conditions Z <- X -> Y.   I believe the authors instead meant to say that the cited works deviate from the information bottleneck theory of learning suggested in (Shwartz-Ziv & Tishby 2017).  In general the paper does a poor job of distinguishing between the Shwartz-Ziv & Tishby paper and the rest, but this is a distinction that should be maintained.  The original information bottleneck may and has demonstrated utility regardless of whether the information bottleneck generally can help explain why ordinary deterministic feed forward networks trained with cross entropy and sgd generalize well.  This also raises one of the main problems with the current work. The title, abstract and especially the conclusion ("This provides, for the first time, strong and direct emperical evidence for the validity of the IB theory of deep learning") seem to present the paper as somehow offering some clarity and further support for the assertions of the Shwartz-Ziv & Tishby 2017 paper, but that paper hoped to establish that information bottleneck can explain the workings of ordinary networks.  Here the authors modify the ordinary cross entropy objective, and so their networks are necessarily not ordinary and so they cannot claim they have helped clarify our understanding of the vast majority of neural networks currently being trained.  Again, this is distinct and should be kept distinct from the utility of their proposed objective, itself inspired by the information bottleneck.  Here too the paper falls flat.  If instead of attempting to comment on networks as they are designed today they aim to proposed a new information bottleneck inspired objective they really ought to directly compare other attempts along those lines (such as the ones they themselves cite  Alemi et al. 2018, Kolchinsky et al. 2017, Chalk et al. 2016, Achile & Soatto 2018, Belghazi et al. 2018) but there are no comparative studies.The experiments are extremely lacking, not only are any of their cited alternatives compared, they don't compare to what would be an equivalent network to their but where they did utilize the noise at every layer and actually made the network stochastic.  Their reported numbers are not very impressive with their top MNIST number at 98.09 and their baseline at 97.73. These numbers are worse than many of the papers they themselves cite.  Only a single comparative results for both a limited training set run and the full one are shown, as well as only a single choice of beta.  The CIFAR10 numbers are not very good either.  There is some discussion of the text suggesting they believe their method acts like an approximate weight decay, but there are no results showing the effect of weight decay just on the baseline classification accuracies they compare against.Technically a deterministic function need not have infinite mutual information, if it is non-invertible, i.e. the sign function, or just floating point discretization. Their own results in Figure 2 and the main body of the text highlight that the authors believe the true mutual information between the activations of the intermediate layers and the input is infinite.  If the true mutual information is infinite and the noise regularized estimator is only meant for comparative purposes, why then are the results of the training trajectories interpreted so literally as estimates of the true mutual information?Just plugging in the Discriminator for the objective (equation (7)) is flawed.  The discriminator, if optimal would learn to approximate the density ratio 1 + log p(x,y)/(p(x) p(y)) .   ( see f-GAN, Norowin et al. 2016).  How does this justify using the individual elements of the discriminator in the functional form of the IB objective?  At the bottom of page 6 they rightfully say that mutual information is invariant to reparameterizations, but their noise regularized mutual information estimator is not (by their own reference (Saxe et al 2017).The discussion at the center of page 8 is confusing.   They claim that Figure 5 (a) is more 'quantized' than (b) and "has reduced entropy".  I think it should be the other way.  More clusters should translate to a higher KL divergence, or higher entropy.  If you need only identify which cluster an activation is in, that should require log K nats where K is the number of clusters.  (a) shows more clusters and so seems like it should cost more and have a higher entropy not a lower one.Despite a recurring focus of the text that this paper applies and information theoretic objective at each layer of the network, and hence is novel, the final sentence of the paper suggests it might not actually be needed and single layer IB objectives can work as well. |||| rating: 2: Strong rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper provides a method to do explicit IB functional estimation for deep neural networks inspired from the recent mutual information estimation method (MINE).  By using the method, the authors 1) validate the IB theory of deep nets using weight decay, and 2) provides a layer-wise explicit IB functional training for DNN which is shown to have better prediction accuracy.Pros:- The paper carefully constructs a method to estimate the mutual information between high dimensional variables and address the infinite mutual information issue by adding noise to the output. This is novel and theoretical sounding. - The paper connects the IB theory of DNN with weight decay, which is a novel founding.Cons:- The paper claims no literature has been doing IB functional on a layer-by-layer objective, however, see [1, 2] for the total correlation explanation work which is closely related to IB functional and they have also verified the effectiveness of layer-by-layer objective. - The scope of the paper is unclear. It seems that the paper is trying to convince two things to the readers: 1) The compression phase in DNN does exist 2) Layer-wise training helps to improve the accuracy. Although these two things are close related to each other (because they all requires to estimate the IB functional), it seems that neither these two conclusions are convincing. First, the compression phase is achieved only through weight decay; without weight decay, as shown in the paper, the compression phase is gone. Does that verify the incorrectness of IB theory of deep nets? Second, for the layer-wise training, the paper only compares the layer-wise IB objective with the cross entropy loss. But if we really want to show the `effectiveness` of `layer-wise` training, one should compare the `layer-wise` training with `end-to-end` training while keeping the objective itself fixed. Otherwise, it is really difficult to draw conclusions about why the accuracy is improving, it is because of the objective changes or because of the `layer-wise` training.- How does the beta (in IB objective) selected in the experiments for comparison? Do you use a validation dataset, and what is the final beta? If the paper fine-tune beta on the validation dateset, then the comparison of "IB functional, only the first term" and "IB functional" is unfair. [1]  Ver Steeg et al. Maximally Informative Hierarchical Representations of High-Dimensional Data. AISTATS 2015[2]  Gao et al. Auto-Encoding Total Correlation Explanation. Arxiv 1802.05822.[update] After carefully reading the response (also from other reviewers), I decide not to change my rating. |||| rating: 5: Marginally below acceptance threshold |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature
BXewfAYMmJw	Counterfactual Generative Networks	long	This paper presents an interesting conceptual advance connecting causality, disentangled representation learning, invariant representations and robust classification.The authors propose a Counterfactual Generative Network (CGN), which is basically "modular" generative adversarial network that can independently control the generation of independent factors of variations in the data corresponding to Independent Mechanism, i.e. independent factors in the structural causal model of the data. In the context of generating natural images like those comprising the ImageNet dataset, the CGN once trained can be used to generate high-quality counterfactual images with direct control over of factors of variations determining the content of an image shape, texture, and background. These generated samples obtained by independently and uniformly sampling over factors of variation can be used to train a classifier to achieve out-of-domain robustness. The authors show indeed show in simulation that this procedure works as a data augmentation procedure that increases out-of-domain robustness while only marginally degrades the overall accuracy. As the authors explain, this can be thought of as a generalization of "domain randomization".Additionally, CGN can be used as a generative model of high-quality binary object masks and unsupervised image inpainting.The authors also carry out extensive ablation studies that quantify the contribution of the different training costs for CGN to the overall quality (measures as Inception Score) of the generated counterfactual images. This is first and foremost an "idea paper" putting forth a very interesting conceptual proposal. This is then empirically validated on out-of-distribution classification tasks in different versions of colored MNIST, and a coarse grained subsed of ImageNet.A natural question for the authors is whether and by how much this type of data augmentation might help in realistic large-scale classification scenarios. Another natural question would be to quantify the effect of the counterfactual data augmentation in terms of trade-off between performance on in-distribution and out-of-distribution samples.Lastly, it would be interesting to know whether the authors have any thought on how to generalize their architecture to other setting, i.e. how to isolate Independent Mechanisms in a general domain and what type of domain knowledge is necessary to do that effectively. |||| rating: 8: Top 50% of accepted papers, clear accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	--- Summary ---This paper proposes a new generative model that generate images from 3 seperate aspects: foreground masks (shapes), forground texture, and backgrounds. Then they convexly mix these 3 aspects into one image. By doing so, they can vary each aspect individually without changing other aspects, enabling the model to generate counterfactual images. For example, we can generate a cat shape with telephene texture and sea background, which would not exist in natural images. They show that in several colorful MNISTs datasets their methods can generate new combinations of images. In ImageNet, by using the pre-trained BigNet GAN as backbones and pretrained U-Net for foreground object masks, they can distill the knowledge in BigNet into these 3 seperate aspects and generate counterfactual natural images.--- Pros ---1. Intersting ideas of combining pre-trained models with novel loss function that helps disentangle these 3 aspects.2. Plausible and intriguing counterfactual examples.3. Strong improvements in colorful MNIST datasets.--- Major comments ---1. The claim that "we are able to reduce the gap while achieving hig accuracy on IN-9" is not true. In Table 4, the method IN+CGN has lower accuracy than IN alone, and it reduces the gap by lowering the original performance in Mixed-same. So it does not actually improve the performance.2. The above makes me wonder the ImageNet experiment does not actually generate plausible enough images that help improve accuracy, probably due to its unnatural image generation as evidenced by relatively low IS scores (130). Especially seeing there are 6 lambdas to tune listed in Appendix B.3 and other hyperparameters like learning rates etc, this method might not be very practical in high-dimensional natural images. Maybe authors can be honest about it. Or illustrate what's the best way to tune this method, what has been attempted etc.3. The current setting seems to a bit limited that requires a single foreground and background. For example, designing independent mechanisms for medical imaging might not be easy with no exact foreground/background boundaries.4. In Table 3, maybe authors can evaluate on some difficult ImageNet datasets like ImageNet-A to assess if the performance improves.--- Minor comments ---1. The loss descriptions in Appendix B should be moved to main text to help readers understand the method. Details like how to pick $\tau$ for shape loss should be mentioned.2. The name "pre-masks" is confusing that originally I think it's a binary mask, but instead it's a colorful image.3. More failure examples in Appendix E will better help readers understand its limitations.4. The figure 5 and the Appendix D should also include the final image that combines m, f and b to better assess its improvement over the training stages. Also, having an arrow in Figure 5 or specifying epochs might better help readers understand it is showing the transitions.5. The caption in Figure 24 is wrong: should be columns instead of rows.--- Evaluations ---Overall I like this paper. The method is interesting to read and the examples are interesting. The experiments are thorough and do show some improvements in colorful MNISTs. The method, unfortunately, does not seem to work that well in ImageNet, and does not improve generalization performance. I encourage the authors to be upfront about the limitations of this method and write better descriptions of loss and hyperparameters tuning.  |||| rating: 7: Good paper, accept |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	The main idea of the paper, i.e., using independent causal mechanisms to generate interventional images, has already been explored by Kocaoglu et al. in Causalgan: Learning causal implicit generative models with adversarial training, ICLR'18. Same as here, the authors there also "view image generation as a causal process" and "structure a generator network as a structural causal model (SCM)" and use a conditional gan to generate the image from the labels. The generation used here based on three variables, i.e., shape, texture and background seem to be a special case. Therefore, the authors should definitely cite this work. My general remark is that there is very little causality in the approach. The causal structure that is used in the generation of data is not different than a conditional GAN. This makes the claims in the introduction very disconnected from the actual methodology and the experiments in my opinion." we can intervene on a subset of them and generate counterfactual images "-> What the authors call counterfactual images are actually interventional images from a causal point of view. Please consider changing "counterfactual" to "interventional" throughout the paper. This will help clarify the distinction between interventional and counterfactual layers in Pearl's hierarchy. "From a causal perspective, we maximize the average causal effect (ACE) of one IM on the classifier’s decision, while minimizing the ACE of all other IMs."Can you formalize this claim? This does not seem trivial.Could you explain "alpha blending"? This step is not motivated well and seems specific to the used dataset. Even though the ImageNet experiments look impressive, I believe the main factor for success is in the deterministic and manually defined composition mechanism. Furthermore, I believe this composition is doing most of the disentangling during training. Foreground and background segmentation use an existing method U2-Net which is used to create masks, or values for the variables used in the graph. The intuition on comparing with other methods is missing. For example, why do you think training a classifier on interventionally augmented data performs better than IRM? Shouldn't this depend on the number of environments and degree of correlation? These are not reported. "We, therefore, follow an augmentation strategy, i.e., we augment ImageNet with additional counterfactual images."How many samples are added to the original data? I believe the amount of augmentation relative to the original dataset size is important. %% AFTER REBUTTAL %%  Thank you for all the updates.I would like to thank the authors for their humility in the rebuttal and for clarifying the paper's contributions. Accordingly, I will increase my score. However, I still believe Section 3.1's contribution, and the follow-up of using this to improve classifier robustness, is useful only for a very specific type of data and it is hard to assess its value from a practical point of view. The fact that the authors were able to showcase that such counterfactual data augmentation improves classification is, although expected, useful in itself. However, performance improvement is only evident in colored MNIST, relative to GAN augmentation. Furthermore, R4 points out the important issue that the relevant causal feature is assumed to be known in the experiments. This information is normally not available and must be inferred by the classifier. The additional experiments provided by the authors during the rebuttal are welcome but they should be in the main paper rather than the appendix since this is the main setting where spurious correlations create problems. I believe the experimental section should put more weight on this setting.In light of all this, I will provide a borderline score leaning towards rejection. I encourage the authors to expand section 3 to settings that do not restrict the images to have one foreground object and a single background.  |||| rating: 5: Marginally below acceptance threshold |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature	Deep neural network brittleness can be attributed to their tendency to latch on to spurious correlations in the training dataset. The proposal in the paper is to learn to generate samples where these correlations can be eliminated. To this end, the authors, distill trained conditional big gan into a transformation with explicit modules to capture the shape, texture of the foreground object, and the background. The distilled network is called Counterfactual Generator Network (CGN). Thus, an image can be generated with a background of one class, the shape of another class, and the foreground texture of a different class. Then a classifier with multiple heads is learned where each head predicts a class based on only one of the factors among shape, texture, and background. The proposed approach is motivated by the assumption of independent mechanisms where different modules of the causal data generating process are independent of each other. Once the decomposition of a training image into shape, texture, and background is obtained, any component can be swapped to generate counterfactual data.Pros:+ The solutions provided to extract object masks, background and texture are interesting and scale to Imagenet dataset.+  Shows that augmenting the training dataset with the generated counterfactual images can help improve robustness.  Cons + Questions:- The presentation of the paper can be improved. It is not always clear if the causal structure is assumed to known. In Sec 2.2 SCM is defined, but the SCM for the MNIST or Imagenet is not provided. Do all the nodes in the CGN share the same noise or exogenous variables?- The proposed method appears to assume that the causal structure is known. In this, it assumes it is made up of three nodes shape, texture, and background, and thus can limit the counterfactual generation ability. Many semantic changes cannot be achieved as evidenced by the fact that the counterfactual images are not realistic. - in the invariant MNIST classification task it appears that the results are based on the assumption that the invariant feature - shape is known apriori. In practice, this information is not available. IRM does not assume this knowledge, so it does not seem comparison with IRM is fair in this case.- Some related work that seems to be missing [1][2][1] Kocaoglu, Murat, et al. "Causalgan: Learning causal implicit generative models with adversarial training." arXiv preprint arXiv:1709.02023 (2017).[2] Kaushik, Divyansh, Eduard Hovy, and Zachary C. Lipton. "Learning the difference that makes a difference with counterfactually-augmented data." arXiv preprint arXiv:1909.12434 (2019). |||| rating: 5: Marginally below acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct
4P35MfnBQIY	Consistency and Monotonicity Regularization for Neural Knowledge Tracing	long	Knowledge tracing is a longstanding task in educational data mining and has been tackled by various studies. This paper proposed that three data augmentation methods (along with different types of regularization losses) can be applied to boost the performance of existing deep neural network models for knowledge tracing. Overall, the methods developed by this paper seem technically sound. In particular, the experiments are rather extensive, i.e., four widely-used datasets were employed in the experiments and different variants of the methods were investigated and compared. However, my biggest concern for this paper is its connection with previous studies and the design principles behind the proposed methods. To be specific, there are a few places that need to be further justified or a more clear explanation.1. It would be good to provide a more detailed description of existing methods for knowledge tracing, e.g., what their limitations are and how the methods proposed can (theoretically) overcome their limitations?2. In the Introduction section, it would be good to further justify "e.g., the recent GPT-3 model (Brown et al., 2020) has 175 billion parameters, they may easily overfit on small datasets and hurt model’s generalizability.". Any other evidence to show that overfitting is a common problem in existing deep neural network models for knowledge tracing? For existing works or the current study?3. Also, it would be good to provide additional data analysis results to support the assumption behind the three data augmentation approaches? For example, in the existing datasets, to what extent can we observe that "a student is more likely to answer correctly (or incorrectly) if the student did the same more in the past"? In the experiments, to what extent such interaction sequences that were mistakenly modeled by previous studies can be modeled accurately by the newly-proposed methods? |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The authors show that various forms of augmentations can improve the performance on knowledge tracing. The experiments are conducted on ASSIST2015, ASSISTChall, STATICS2011 and EdNet-KT1. Data augmentation leads to a certain amount of improvements. However, consistency training provides more significant improvements. The novelty part is limited since the proposed methods such as insertion, deletion and replacements are intuitive and also seen in prior works in NLP. The monotonicity constraint is specific to the knowledge tracing task though. The improvements are consistent, and especially significant when the training data is limited. More ablation studies on the hyperparameters would be beneficial. My major concern is that the novelty is limited. The paper tackles a less well-studied task so more experiments should be added. For example,1. Would consistency training leads to more improvements when the training data is limited?2. How would the hyperparameter in insertion, deletion and replacements impact the performance?3. Would more advanced augmentation lead to better performance? |||| rating: 6: Marginally above acceptance threshold |||| confidence: 2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper	This paper presents some enhancements for Knowledge Tracing (KT), in which predictions are made about the odds of a student answering a question correctly given a sequence of correct/incorrect responses to previous questions.  The authors observe that the predictive model should obey certain 3 common sense constraints. If a question is replaced in the student's data by a very similar question, the prediction should not change much. If an additional correct question is added to the data, the odds of the student being correct on the next question should go up, and the odds should go down for questions being removed and/or added with incorrect responses.  The learning algorithm's objective function is augmented with additional terms which encourage the model to obey these constraints.Pros:The method for the most part makes sense. The experiments are reasonably thorough (4 benchmark datasets are tested) and non-trivial accuracy gains are demonstrated, although dramatic gains are only achieved on 1 of the 4 benchmarks.  Ablation experiments provide additional confidence that the interpretation of the impact of the method is correct.Cons: The paper should cite previous work from the 1990s from Yaser Abu-Mostafa, who pioneered the use of these kinds of 'regularization' enhancements under the name of 'hints'. See e.g. ' A Method for Learning from Hints', NeurIPs 1993, and several similar papers.  A Google Scholar search of other ML work on monotonicity may also be beneficial if the authors seek to continue this line of research. See e.g. the recent work of Maya Gupta et. al.  I would have appreciated more information about the 'skill sets' associated with each question and how that impacts the replacement.  The authors say question is chosen as a replacement if it has some skill overlap with the original question (page 4).  However, if there are multiple skills associated with the question, wouldn't it make more sense to choose replacements based on percentage skill overlap than a simple binary detection of any overlap?Further comments:One comment I have (and I recognize that not everyone)Some typos:Impose certain consistency or monotonicity bias on model’s predictions -> biases on the model’s predictionsFig 2 randomly insert intractions – interactions…even the student answered more questions correctly -> even if the student answered more questions correctlywhen other researchers will pursue to improve the generalization ability of KT models in the future-> for other researchers attempting to improve the generalization ability of KT models.Section 3.1 among all questios -> among all questions |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	Disclaimer: I am not familiar with the educational AI field. With strong argumentation, or if one of my co-reviewers is an expert in the field, I could be persuaded to change my score.The paper investigates how well data augmentation can help improve the performance of contemporary deep learning models for Knowledge Tracing, which is a key task for educational AI. The authors propose three different types of data augmentation strategies: replacement, insertion, and deletion. The authors provide a detailed experimental section with ablation studies, highlighting the benefits of using their model in addition to recently proposed models. The authors provide confidence intervals for their results proving the significance of their solution.What this paper excels at is the breath and scale of their experimental section. I like that they have taken a large set of different datasets, ablated all improvements, and tried different training set size models.However, I am have the following concerns that leads me to reject this paper:- SAINT seems to be the most modern model, which is why this is particularly interesting as I assume any improvements would indicate a SOTA in the field. Though, in table 1, for the EdNet-KT1 dataset the authors report SAINT to have 74.78, while the original SAINT paper reports 78.11 AUC- I do not see any arguments for why these augmentation methods are of specific interest. What motivated you to try this? beyond just wanting to add noise in the training. I believe adding some noise could give a small improvement, but I do not believe that such finding on a niche NLP subfield is of general interest to the scientific community beyond a workshop.- In general, it is my understanding that this is 3 augmentation functions, something similar to dropout or synonym replacement. I think the explanation of these methods are overly complicated. I would like to see the authors making their method section easier to read and reduce the amount of unnecessary notation.And some more specific comments:- "However, as the number of parameters of these models increases, e.g., the recent GPT-3 model (Brown et al., 2020) has 175 billion parameters, they may easily overfit on small datasets and hurt model’s generalizabiliy. Such an issue has been under-explored in the literature" - The GPT-3 model doesnt overfit. Also I don't think massive language models are relevant to your problem. If it's the overfitting issue I would find something that reports on overfitting and the use of data augmentation to remedy it.- I don't get Figure 1.- What is consistency and contrastive learning? you reference 7 papers, but give no intuition about it's relevance to your work. Please elaborate.- I dont get figure 2 when reading the paper from end-to-end, I don't think it should be on the top of page 3 when it's referenced in the results section.- The math in 2.1 is unclear to me.- What is the metric in table 1? ACC or AUC? there's a huge difference. Also, is it on the validation or test set?- Are the ablation studies on the validation or test set?UPDATE:Thank you for clarifying the ethics concern. However, this makes it much more difficult to assess whether I believe your method works as well as you state. After having read the rebuttal and the other reviews, I am more confident that the methodology proposed lacks connection to educational relevance and novelty for publication at this venue. My score stays the same. |||| rating: 4: Ok but not good enough - rejection |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct
xL8sFkkAkw	Towards Theoretically Inspired Neural Initialization Optimization	long	Authors study the problem of finding (learning) the best neural network initialization, and propose GradCosine - a measure of fitness for neural network initialization, based the similarity between individual sample gradients. This measure can then be optimized using and iterative procedure denoted as NIO (neural initialization optimization). Authors explain how GradCosine relates to model training and show a the relation between this quantity and the density of sample-wise local optima ([52] in the paper).From a theoretical point of view, the paper demonstrates that minimizing GradCosine results in favourable bounds on the training loss. For practical applications, authors offer a sub-batch version of GradCosine that can be computed more efficiently. To the best of my understanding, the theoretical analysis appears sound, based on reasonable assumptions. [That said, I am no expert in the area of learned initialization.] The main experiments look reasonable, though I would recommend making some extra comparisons.1. in Table 5, you evaluate how NIO trains SWIN transformer without warmup. At least one baseline (GradInit[53]) also claims to work in that setting. Perhaps it would be best to compare NIO against that.2. Table 4 does not report standard deviations, while all prior experiments do. It might be useful to include them - or explain why they are missing.On an unrelated note, I must applaud authors for supplying a Dockerfile in their supplementary code. Publishing the corresponding docker container will make it easier for future researchers to reproduce this work and build on it, even if the required libraries break compatibilities.### Typos / nitpicking> L215  making the sub-batches overlapped with each other **stables** the optimizationperhaps a typo? "stables" -> "stabilizes"> L250 Each model is trained **for** four times with different seeds. consider removing "for"> NIO is able to produce a better initialization that benefits model performance agnostic of architecture and dataset.[nit] this is concluded at an early section, where the only evidence is training cnn-only models on CIFAR-10/100. As a weak suggestion, I would recommend making this conclusion later, once you demonstrate NIO performance for transformers and ImageNet. To the best of my knowledge, authors have sufficiently addressed the limitations of their work.As for the societal impact, this specific paper contains fundamental research in deep learning, thus it is hard to foresee its societal impact.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	The paper first introduces a quantity, the cosine similarity of sample-wise local optima to evaluate the model performance at the initialization. They theoretically proved that their proposed quantity is the upper bound of both the training and generalization error under certain assumptions. Based on this theoretical finding, they approximate the sample-wise optimum with the first-order approximation to make the quantity differentiable and tractable. As a result, they simplify the upper bound quantity and achieve the initialization by maximizing the quantity with the gradient-based method. Their empirical results show that they can achieve better performance on various datasets and network structures compared to other initialization methods. Strengths:The paper is well organized and easy to follow. The theoretical analysis seems correct and motivated. Their empirical results are quite good, especially on CIFAR datasets. Weaknesses:It will be better if we can have some experiments to show that the proposed cosine similarity of sample-wise local optima is useful or better than the previous methods. Since the initialization method is motivated by this quantity, it’s better to make the usefulness of this quantity clear. The citation in Appendix A for the proof of Lemma 1 seems wrong.  The authors fairly addressed the limitations and potential negative societal impact of their work.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This work proposes a new initialization method for neural network. The authors first use Fig. 1 to illustrate drawbacks of the sample-wise local optima density $\Phi_{S,l}$ that adopts Manhattan distance between the pair-wise local optima, and lead to a Cosine similarity of sample-wise local optima $\Theta_{S,l}$. Then the authors introduce to approximate local optimum by one step gradient descent and approximate $\Theta_{S,l}$ by Eq. 4 (GradCosine). Finally, the initialized weights $\theta_0$ are obtained by maximizing GradCosine and GradNorm. Extensive experiments are conducted to verify the efficacy of this approach, showing that GradCosine surpasses MetaInit and GradInit on multiple datasets in both accuracy and speed. Strengths:1) The writing is logical and fluent.2) The experimental results are good.3) The motivation is interesting. Nevertheless, I have one question about it, please see the first point in Question.Weakness (Question):1) In Line 28, the authors claim that “these methods merely use the first order training dynamic as the main optimization objective”. However, MetaInit[8] and GradInit[53] both adopts gradient descent method.2) It seems that the theoretical analysis in Sec. 3.2 cannot show that $\Theta_{S,l}$ has a tighter bound than $\Phi_{S,l}$. Then I think Sec. 3.2 is a little redundant, which can be put into the supplementary.3) Typo: ‘GradCoisne’ in Line 6. The limitations are addressed.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper works on the initialization of neural network parameter based on theoretically inspired optimization algorithm. It is to substitute the previous network initialization algorithm, such as Kaiming's method, [52], etc. The proposed approach is closely related to [52] by introducing the cosine similarity of sample-wise gradient for optimizing the initial network parameter. The proposed approach is applied to Resnet, DenseNet, WideResNet, and transformer. The results show that the proposed initialization method achieved better final network training results than the Kaiming's method, and the other compared learning-based methods.   Strength:1. The paper theoretically dig into the network initialization based on the difference of network parameter initialization and sample-wise local minima, and derive a quantity to approximate the upper-bound of training and test error bounds. The paper further approximate and minimize  the bound with constraint on gradient magnitude.  The proposed method is reasonble and inspired by theoretical analysis.2. The proposed method is evaluated on network training and show improved results with the proposed network parameter initialization method.Weakness:My major questions and concerns are on the novelty over [52] and insufficient evaluation details. 1. The  theoretical analysis of this work is similar to the analysis of [52], though the conclusion of this paper is different measuring by the cosine similarity of sample-wise gradients. 2. The proposed approach is subject to an optimization problem of (6) adaptive to mini-batch based implementation. For feasibility in implementation, it makes some assumption and introduced the hyper-parameters, e.g., \lambda. How is the performence affected by the hyper-parameters of the optimization algorithm, including, e.g., \lambda, and number of iterations, etc ?3.   Due to the randomness, e.g., the mini-batch, the performance should be reported by running a same task in several times, and the mean the variance of the performance should be more insightful to compare the different methods. 4. Because of the introduced additional computational cost in optimization, the approach is reported by running 100 iterations. How about the comparisons (speed and performance) if all the compared methods are aligned in number of running iterations for initialization? The paper clearly state the limitations of this approach.   |||| rating: 5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
0O_cQfw6uEh	Gradient Origin Networks	long	This paper proposes a new type of generative models with a new inference method of latent variables. Specifically, the gradient of latent variables with respect to zero vector is taken as the inferred latent variables. Based on this, the authors generalize the propose model to implicit and variational versions and demonstrate the models on image datasets.Pros: the proposed method is easy and straightforward to implement. Cons:1. The model assumption that the one step gradient from zero vector equals to latent vector is quite limited and greatly constrains the model expressiveness. A justification that such assumption is reasonable is badly needed.2. Formulation needs to be carefully checked. For example, Eqn 2 is not entirely correct to me. The second term should not be binary cross entropy as there is no categorical variable involved. Also, please avoid using abbreviations (L^BCE, L^CCE) at the first time to introduce them, which are confusing. 3. Experimental results are not sufficient to demonstrate the efficacy. Need more quantitative analysis and experiments on more challenging datasets. 4. The claim that it saves parameters compared to VAE is confusing. In the variational version, parametrizations of mu(x) and sigma(x) are also required. A principled way to very this claim is to show that with the variational version, the method could use much less parameters compared VAE while has the better synthesis quality. Overall, the method proposed in this paper is new and promising. However, given the current unclear formulation and lack of strong experimental results, I recommend a rejection.  |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The paper proposes GONs which seek to build a generative model with an “implicit” encoder that comes essentially for free with the use of a few re-parameterization tricks. The main idea being that existing generative models with an encoder are “redundant” in that the decoder itself has the ability to compute the gradient with respect to a latent vector, z, which itself can be thought of as the “encoding”. Since the choice of what initial latent vector to choose arises here, the paper advocates for simply choosing a z_0 which is a zero vector. In addition to the “explicit” formulation, there is also an implicit GON which is proposed that can generalize implicit generative models  (like SIREN) to entire distributions as opposed to a single data point, as they are currently used.Overall, I think this is very interesting work but incomplete. Considering GONs are a completely new category of generative models, it would greatly help to study each piece in more detail (theoretically or empirically) to establish what makes GONs successful, different, and how this improves our understanding of implicit representations in neural networks. Strengths:+ An interesting and novel formulation of encoding schemes from decoders that do not need any additional training or networks. + The paper explores several different variants of GONs — from a variational alternative, implicit, and a classifier. Which greatly expands its scope of application in new problems. + GONs generalize implicit generative models like SIRENs to work with an entire data distribution with very few parameters, which I think is a great benefit. This also naturally allows for variational alternatives, meaning we can sample from complex high dimensional distributions using very simple networks. + The implicit GON also enables finer grid sampling in the input space, enabling its use in applications like super resolution naturally — but to any image from the training distribution. Weaknesses:* The paper is very dense in terms of ideas, and as such falls short in thoroughly evaluating all of them. For example, the paper contributes several ideas like GONs, implicit GONs, variational GONs, which is great but it would help if each one of those pieces were studied in some more detail so they can be compared and contextualized better with existing approaches. For example, in the formulation itself the GON loss is presented “as is”, but I think it warrants some more study.     * For example, why is just a single step “sufficient” to estimate “z”? Does the quality of “z” improve if you take multiple smaller steps? How stable is this for different datasets? The empirical studies show promise, that indeed this can work reasonably well in reconstructing different datasets, but it would greatly help to justify some of these choices further.     * In the explicit case, how important is the choice of “F” ? The choice of activation function is explored but what about the architecture/ number of parameters for a given dataset? * In all the experiments, the reconstruction losses are shown are for the training set, how do the validation set samples get reconstructed?  It’s not clear if GONs are so effective in reconstructing because they are memorizing the data? * How does the performance of GONs change as the size of the output space grows larger? For e.g. 128x128 or 256x256? * Some of the terminology is also confusing. What does it mean when you “overfit” to an entire distribution? I understand its usage for a single image, but it's not clear what this means for an entire dataset. Are the samples from Figure 4 all from the *same* trained GON? * Is Figure 7 from an explicit GON or an implicit GON? If its explicit, how are the number of parameters comparable to an implicitGON? Clearly an explicit model will have a lot more number of parameters. esp as the size of the images increase?* I really like and appreciate the variationalGON experiments. How do they compare with  standard VAEs? Can they recover CelebA 64x64 images? How would they compare on quantitative metrics like FID etc.?* In the super resolution experiment, can it super resolve *any* image from the distribution it was trained on? For e.g. in figure 5. is it just a matter of resampling the grid to 256x256 and running them through the pre-trained model for any sample from p(x)?---------- Update on the revised manuscript ----------I have read the new version of the paper and it reads a lot better. The new expanded methods section, and the definitions for different variations of GONs makes the paper much stronger and easier to understand. I appreciate and like the new experiments that show GONs capabilities on LSUN, comparisons with VAE on ELBO. Most of my concerns have been addressed in this version. I think this paper makes an interesting and novel contribution and I will raise my score accordingly.  |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper introduces a "new" inference method for autoencoder-type models, where the encoder is taken as a gradient of the decoder with respect to a zero-initialized latent variable. The method is evaluated for both a deterministic autoencoder and a VAE on toy image data (cifar10 being the most complex of them) and applied to convolutional decoder and to SIREN-type implicit representation networks. This is, for all intents and purposes, a single step iterative inference setup. In its VAE variant it is extremely similar to old-school iterative inference, albeit with a single gradient step. The paper is very-well written and interesting. The method seems to be getting very good results,. Still, the paper seems to be rushed. The results are only on small scale and toyish datasets, and there are very few baselines. In its current state I recommend rejection due to rather limited novelty (although it's cool to see that this type of inference works for implicit scene representations) and very limited evaluation. There are also very many links to existing literature that are not properly described. Let me elaborate.Baselines:To determine the efficacy of this method, the authors would have to compare against some similar methods including:* old-school multi-step variational inference* semi-amortized variational inference* the proposed method with multiple gradient steps* the proposed method with detached gradient (as in: not use 2nd order gradients)* a fully-convolutional autoencoder with parameters tied between the encoder and decoder. This is for two reasons: a) this would reduce the number of parameters by half, making it more similar to GON, but also b) the transposed-convolution used in such a setup corresponds almost exactly to the gradient of the encoder, which is an idea very similar to GONs.Missing links to the literature:* the above fully-conv AE setup.* model-agnostic meta-learning (and related, e.g. CAVIA, LEO etc), where the "latents" are produced by single- or multi-step optimization.Missing experiments:We would need more evidence to determine if such a simple method is useful. A good experiment would be e.g. on imagenet.Further suggestions:Subfigures in fig2 and 3 (and most of figs in the appendix) use different scales on the Y axis. It would be easier to read the figures if the scaled were normalized within a single figure.Update: I've updated the score given the authors' response, see my comment below. |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
b0VDQiNLPy9	ETAB: A Benchmark Suite for Visual Representation Learning in Echocardiography	long	The paper pursues the research question of how to evaluate representation learning for echocardiographic data (ultrasound images of the heart). This is tough because available open-access datasets, while valuable, each cover only a subset of the supervised tasks of interest and may not offer too many labeled examples for their chosen tasks.The paper shows how to build representation learning benchmarks for many tasks of interest by remixing 3 open-access datasets: EchoNet from Stanford in California; CAMUS from University Hospital of St Etienne in France; and TMED from Tufts Medical Center in Massachusetts.The first contribution is a suite of 25 specific "visual task adaption" benchmarks, listed in Table 2. Each involves a source and a target choice of dataset-view-label. The second contribution is an evaluation protocol that helps grade overall representations, via an average across tasks and task-categories (Eq 1). The idea is to be able to assess the "overall usefulness" of a representation across tasks, via a single metric.Experiments show how their contributions enable insights about whether pretraining helps (Fig 3), which deep architecture might be best (Tab 3), and whether pretraining on medical images is better than generic images (Fig 4). Strengths are:* Tackles an important problem for an exciting application area (echocardiography); could spur significant ML methods development that is *useful* for improving the efficiency and quality of patient care* Broad coverage of tasks of interest within echocardiography, ranging from segmentation for structure, function estimation, view recognition, and diagnosis prediction* Specific experiments that show benefits of pretraining on medical tasks (3DSeg8) vs generic images (ImageNet) are valuable* Experiments that assess performance versus target set data size (as in Fig 3a) are quite valuable, esp. for thinking about combining public data with in-house private labeled sets that are small **Update after author response on 2022-09-02**:I have raised my score to an "accept", because* W0, W2, and W3 below have been completely addressed. * W1 has been mostly addressed (see my detailed comments in response to the authors for a remaining minor issue related to how CAMUS test data is used)**Original review submitted in July 2022**The key issues I see with the present paper are:* W0: Confusing task definitions for a few tasks in Table 2* W1: Missing target-only baseline in Fig 4 and Tab 4; makes hard to assess quality* W2: Reproducibility is poor: key experimental design details / hyperparameters not available* W3: Documentation not ready; seems to have many completely blank sectionsSee detailed subsections labeled "W__" below for elaboration on each of these weaknesses.I think these are very addressable in the response period, and I look forward to engaging with the authors via discussion.This work has lots of promise; I just want to emphasize that at present, the reproducibility and documentation really need to improve for this to be accepted.  |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The paper proposes a benchmark approach for representation learning of visual data in cardiac ultrasound. It provides 25 adaptation benchmarks in which a source task is performed on a source dataset, and then learned representation is employed for a target task on a target dataset. ETAB involves three datasets: EchoNet, CAMUS and TMED. The proposed benchmark suite captures a degree of adaptation on multiple datasets; nevertheless, the impact of this suite remains questionable.  - The paper involves three well-known public datasets: EchoNet, CAMUS and TMED.- Adequate classification of adaptation tasks, including clinical prediction, view recognition, cardiac function estimation and cardiac structure identification.- Comprehensive experimental setup with the use of popular methods such as ResNet50, U-Net etc- Provided some basic insights on transfer and representation learning in Echocardiography. - The paper presents a benchmarking suite to investigate the task adaptation of representation learning methods which is simplistic with applications in a very limited context.- The adaptation benchmarks were proposed based on limited views/knowledge of the existing datasets, which might not be exhaustive and "unified" for tasks in Echocardiography.- Whereas enabling adaptation of pre-trained models are good to have, how does ETAB impact the existing/new workflows in developing representation learning methods?- Limited discussion on theoretical contributions and practical implications of the paper in real-world scenarios.  |||| rating: 4: Ok but not good enough - rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper provides an echocardiographic task adaptation benchmark for the various clinically-relevant task using publicly available datasets. Their benchmark evaluates some model architectures, pretraining techniques to standardize the evaluation method for clinical usage. This paper shows powerful usage in specific clinical tasks like Cardiac struct. identification, estimation, and clinical predictions. Translating the clinical evaluation protocols into ML-friendly tasks will bridge the gap in deep learning applications in the medical domain. - **Novelty**: as addressed in the paper, "the domain-specific analogue of the general benchmark for vision tasks developed in [19,20]". Though the ETAB definition involving the formula is slightly different, it does not increase superiority in methodology. - **Methods are not comprehensive**: benchmarking a dataset should consider SOTA methodology in model architecture and pretraining. Though the paper mentioned ViT and Swin-ViT in line 173, it's not included in the main context and supplementary. In the baseline for the cardiac function estimation task, it only mentioned LSTM to handle the video frames without justification. The other methods for video data like ViViT should at least be mentioned. Other methods like semi-supervised learning and contrastive learning, which achieved a good performance recently, should also be considered as a good pretraining strategy in addition to transfer learning. - **Pipeline not user-friendly**: Github repo does not provide a user-friendly interface to reproduce the result. All the links at the bottem are invalid.  |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This is a very well written paper and I believe that this benchmarking setup can later be used for other medical specialties. The use of AI in medicine requires a higher threshold of security and testing than other use cases because the difference between right and wrong output can mean life or death. The authors of this paper are suggesting a novel standardized approach to solving this problem. Novelty. Well written. Strong main experiment and sub-experiment. The GitHub website is not as complete as I would like it to be. However, I can understand that it is constantly being updated.  |||| rating: 8: Top 50% of accepted papers, clear accept |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	The work presents a benchmark suite of tasks for evaluating the performance of learned visual representations for use in echocardiography. This benchmark, the echocardiographic task adaptation benchmark (ETAB), provides a meta-dataset constructed from 3 existing echocardiogam research datasets. ETAB formalizes several categories of source and target task categories, and defines an aggregate ETAB score for estimating performance across a these benchmark categories. This benchmark enables evaluating a number of backbones and other representation learning approaches across a heterogenous mix of echocardiographic tasks and datasets, which is critical for assessing robustness and other properties of representation learning methods.  - The benchmark is very promising. Moving beyond traditional benchmark datasets, which are often narrowly constrained to iid assumptions by task and dataset, is critical for practical development of applied medical ML. - Incorporating small training sizes (16+ training examples) is a nice focus area, as few-shot medical benchmarks is a critical research area. - The downstream tasks cover a nice range of realistic use cases (segmentation, cardiac function estimation, view classification, clinical attributes). - A metadata with a formal evaluation protocol across datasets would be a nice contribution The most significant weaknesses of this work are- A lack of clarity outlining the structure of the overall experimental pipeline in terms of measurable sub-components. - Reported results are terse or in some cases missing (i.e. all non-ResNet50 performance numbers). Currently the manuscript lacks a full  breakdown of results even for the ResNet50 backbone use case that forms most of the paper's results. This incomplete reporting makes it difficult to follow the overall merits of the benchmark. Clarity of results could be substantially improved. The paper proposes an evaluation strategy that covers a large number of model configurations, spanning architectures, datasets, pretraining strategies, and downstream task performance inclusive of some adaptation/transfer approaches. This is a very large cross product. While the gestalt ETAB score provides some useful assessment of representation learning quality (to the points raised in lines 146-147 around how individual scores assess specific transfer learning scenarios) a complete reporting of the individual source/target scores would make this paper much easier to follow. Figures 3 and 4 include subsets of tasks. Why not report performance for all task categories?  In general, I feel the paper is missing reporting for many intermediate results (e.g., performance metrics for the source models) as well as not providing a consistent report for all downstream tasks.  The paper only includes results for ResNet50 architectures. Lines 173-176 states that other backbone architectures were evaluated, including vision transformers (ViT-L/16, Swin-ViT) and multi-layer perceptrons (MLP-mixer), however these performed worse than the ResNet50. While these results are stated as included in the supplementary material (line 176) they are actually not included. The supplement (which is quite short) includes pointers to the ETAB GitHub, but many links are broken and no results or leaderboard are available. I feel this is a substantial limitation and the paper would be substantially improved if we could observe ETAB scores across more than 1 family of backbone architectures.  I also find that not including the performance for the transformer and MLP architectures due to inferior performance to the ResNet50 is counter to the utility of a leaderboard benchmark, where these backbones are very reasonable baselines and increasing in popularity.  Some of the datasets may have availability issues. While EchoNet has a complete DUA process and provides archival information the other 2 datasets seem problematic. The "Tufts Medical Echocardiogram Dataset" is currently a dead link. CAMUS is from a challenge dataset and it's unclear from the website if the data is available to researchers who are not participating in the challenge (which ends in 31 Dec 2024). I was unable to fine an explicit discussion of licensing terms for CAMUS. Sometimes medical challenge datasets are only available for a given challenge. What guarantees do you have that these datasets will be available to researchers moving forward?   |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
Kyswf8Kj83	TwiBot-22: Towards Graph-Based Twitter Bot Detection	long	The paper proposes a new graph-based Twitter bot detection dataset. The authors describe the dataset in detail together with the annotation method. TwiBot22 is significantly larger than other similar datasets, with almost 1 million users. The authors use an annotation method by employing several human annotators and using this manual annotation in a weak supervised manner to annotate the rest of the dataset. The paper evaluates TwiBot22 on several methods for bot detection, comparing the accuracy of the methods with other similar datasets.  The paper is well-written, with an impressively large dataset. By comparison with previous data, TwiBot22 seems significantly larger. The annotation method seems comprehensive. The dataset seems to build on the methodology and data of TwiBot20 by extending the dataset and annotation.  The authors perform an evaluation comparison between TwiBot and several other datasets on 35 proposed methods. While this analysis is particularly comprehensive, accuracy for TwiBot22 is not that high compared to the rest. Still it is on average over 70%, which seems fine, and it performs well for the graph-based methods, so this is not a big issue. I appreciate the detailed Appendix, the data description together with the additional results seem consistent with the results in the main text.  However, Table 8 seems to have some issues: it seems like it's showing the average model performance (F-1) in addition to the table in the main text (which shows accuracy), but the average F-1 for TwiBot22 seems significantly worse than for the other datasets. Unless it's a reporting issue (some typo), this seems significant. For example, by Rodriguez-Ruiz, C-15 is 82.6, C-17 is 86.6, TwiBot-20 is 75.9, while TwiBot22 is 0.8. I am assuming this is a reporting issue given that TwiBot20 is not that different.  I appreciate the short discussion on societal impact and the potential of the dataset to be used to build bot evasion techniques. It would be great to have a slightly longer discussion on that and assess this potential by analyzing the related literature.   |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The paper tackles the problem of bot detection on Twitter with graph-based methods. To that effect, the paper makes the following contributions:1. Creates an annotated graph dataset, TwiBot-22, of human and bot Twitter users, with 1M users in total.2. Presents an evaluation framework to standardize the measurement interface across various different datasets and methods. The paper has the following strengths:1. It tackles the very relevant problem of bot detection on Twitter, which is of consequence to various downstream objectives like spread of misinformation, propagation of rumors, etc.2. The dataset released is heterogenous with 4 different node types and 14 different relation types, making the dataset close to real-world graphs.3. Extensive experiments are conducted with a range of state of the art methods, including graph neural networks, to demonstrate the usefulness of the released dataset. The experiments also cover interesting ablation studies to demonstrate the usefulness of having graph signals. 1. In the experiments section, I would have liked to see testing for statistical significance as well.  |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper proposes a new graph-based Twitter bot detection benchmark dataset. This dataset has been presented as an improvement over the already existing graph-based datasets in terms of dataset scale, extensiveness of the graph structure and annotation quality. The authors evaluated the proposed dataset, along with 8 more existing datasets, using 35 twitter bot detection baseline algorithms. Based on evaluation, the authors argue that the graph-based component of the sota algorithms is responsible for their better performance hence the need of an extensive graph-based dataset. Finally, the authors provide a publicly available evaluation framework which includes all the datasets and baselines used in the paper. A. The dataset improves upon the existing graph-based datasets in terms of dataset scale, graph structure and annotation quality. B. The evaluation framework which includes implementation of many baseline algorithms and few existing datasets can facilitate bot detection related research. A. Regarding the first stage of data collection process, this dataset appears to be a direct extension of a previous dataset “TwitBot20”[1] by the same authors with a minor change in the "diversity-aware" sampling.B. Line 198 : "...We further evaluate annotation quality on the test set of expert annotations...". The test set implied here seems to correspond to the 200 users' set mentioned in line 181. However, in appendix A.5, the test set seems to consist of 1000 users (500 from TwitBot20 and 500 from TwitBot22). Need to address this ambiguity.C. In section 4.4 (Generalization Study), more details about the 10 sub-communities could be included (similar to what is done in Table 1 for the whole dataset). This could be useful in inspecting the results presented in Figure 3.D. The authors have stated a possible limitation to be the lack of multimedia content in the dataset. But they have indicated that it is possible to do so using media links in TwitBot-22. The solution being alluded to is not clear.[1] Shangbin Feng, Herun Wan, Ningnan Wang, Jundong Li, and Minnan Luo. TwitBot-20: A comprehensive twitter bot detection benchmark. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 4485-4494, 2021.  |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper utilizes a graph based approach to create the largest Twitter bot detection dataset. The primary strength of the paper is the dataset which contains over 100,000 bot users which is very useful for research on social media data. Additionally, the authors perform extensive evaluation using latest methods and re-implemented previous baselines and compared the performance of their dataset. The following are the contributions.1) A graph based approach, TwiBot-22, which contains bot detection dataset that establishes the largest benchmark2) benchmark framework and re-implemented 35 baselines for evaluation3) code available to reproduce and dataset available for research The following are the strengths of the paper1) Using labeling functions to create labels and utilizing weak supervision approach. This demonstrates scalability 2) The dataset is useful as there is an immense need for bot detection techniques with increase in social media research3) Extensive evaluation with several existing datasets to draw comparison with the graph based method  The following are the weakness. These needs to be addressed to strengthen the paper1) TwiBot-22 has less performance when compared to TwiBot-20. Authors made the statement on the decrease in the performance but have not discussed potential reasons for decrease in the performance. Is it because of adding more noise through labelling functions? 2) The authors do not discuss any limitations of the approach or the dataset. A few limitations could add more perspective to the paper 3) More details on the dataset is required. For example in Table 1, the authors list number of users and number of humans and number of bots in the dataset. But the authors fail to mention how many of tweets are actually bot tweets. This would be a great contribution factor and add value to the dataset if there are more bot tweets than user tweets. Tweet distribution must be added to the paper.   |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper proposes a large graph-based dataset for Twitter bot detection that helps prevent misinformation and maintain the integrity of the online discourse. Twibot-22, proposed by the authors, includes five times more users than the existing dataset and is a heterogeneous information network, including 4 entities and 14 relation types. It also uses a weak supervision learning strategy to generate high-quality annotations at a low cost. The authors conduct 35 benchmark tests on the proposed dataset. Experimental results imply that Twibot-22 can be used to develop improved automatic methods optimized for Twitter bot detection in more complex real-world situations. (1) This paper significantly and robustly extends the data available in Twitter bot detection. It proposes a dataset 5 times larger than the previous dataset.(2) The dataset contains rich information for Twitter bot detection, providing for the first time ever, a heterogeneous graph.(3) Extensive experiments are conducted, and the results provide insight into future work. Also, a well-organized evaluation framework is proposed to allow easy access and reuse of datasets and all baselines. (1) The user expansion process described in Section A.2 is not clear. The authors clarified that the number of users increased from the starting user by expanding to their following and follower users. However, how to achieve 1 million users consequently from starting users? For example, how many iterations of random selection are performed? Did the target number of users exist? More details are needed for clarity.(2) In Section 3.3, annotation quality comparisons are conducted between Twibot-22 and Twibot-20 only. However, a comparison with other baseline datasets such as cresci-15 should be considered for fair comparison. A comparison of the quality with all other baseline datasets should also be presented that verify whether the proposed dataset has high-quality annotation labels compared to others.(3) In Section 4.4, the generalization study, the criteria for selecting the five users (e.g., @ BarackObama, @elonmusk, …) for constructing sub-communities should be clarified. For example, what exactly is the “interest domain” used as the criteria? How are the users of the sub-community distributed? Do the characteristics of the sub-communities significantly statistically differ? If so, how the authors evaluate the differences? The authors should provide more details and discussion for verifying that 10 sub-communities could represent the real-world data.  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	The paper provides a new, larger and more comprehensive dataset for Twitter bot detection. It benchmarks a large number of existing methods on the new dataset and 8 others from the literature. There are also some experiments on the importance of graphs in several models and on how well different models generalize. 1. The new dataset contributed is significantly larger (more users) and more comprehensive (more data types/graph relations) than previous bot detection datasets. 2. The benchmarking is extensive, with 35 models over 9 datasets.3. The dataset and benchmarking codes are accessible.This is an important real-world problem, with both social good and business applications. Given that and the points above, it seems highly likely that this work will be useful for future researchers. 1. I have doubts that accuracy is an appropriate metric for the benchmark with the proposed dataset, due to class imbalance. This may affect reliability of some of the conclusions - at the least, I feel they should be justified with another metric. I elaborate in detail below (point #1 under Correctness).2. Because the annotation process involves several specific model architectures, I am concerned the benchmarking might have some bias in favor of similar models (point #2 under Correctness).3. There are a few parts which are not completely clear to me. Most importantly, some of the expert annotation process and corresponding evaluation of the dataset annotations (point #4 and part of point #5 under Clarity).  |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
qnfYsave0U4	The Dollar Street Dataset: Images Representing the Geographic and Socioeconomic Diversity of the World	long	The authors present an image dataset of everyday household items from homes around the world, including metadata about the region, country, home monthly income, and tags of the objects in the images. The dataset includes images from homes with very low income and with no internet access, which is a subgroup that is not often represented in such dataset.Having personally read about algorithm audits which use the web-based predecessor of this dataset (on GapMinder.org), I can confirm the work contributes greatly to the field, providing a dataset which allows for increased representation of vulnerable groups rarely represented in image datasets. The inclusion of lower-income households and geographic regions not typically represented in large image datasets is the biggest strength of this work. These populations, despite their under-representation in the training and deployment process, are subjected to computer vision algorithms. Therefore, it is an important contribution of this work that the algorithms can now be checked for their biases along these dimensions.A strength that perhaps goes beyond the NeurIPS community and into some social science fields is that some tags used for the photos (often objects, such as "stove") include things like "favorite home decoration" and "most loved toy". Similarly, closer to the NeurIPS goals, is choosing to name certain tags in vague enough ways that it encompasses what both ends of the income spectrum may look like, e.g., "place where eating dinner" [sic].The income levels are not raw numbers but instead take into account various things, including the purchasing power parity of the country, 'available consumption' of each adult in the family, and others, and is translated to US dollars for easy comparison. The lack of standardization in the photos (regarding angle, lighting, etc), potentially very few examples of a single type of an object, and the - relatively - small number of images in the dataset may not make this a very suitable dataset for training machine learning algorithms. However, it would be useful in testing (auditing) them for their geographic and income-related biases as described earlier, so I would encourage the writers to emphasize the latter in their work. This is especially important as the paper very rarely discusses the future use of the dataset beyond a little during the introduction and conclusion.There are some repetitions in the list of Topics, such as "cooking" / "cooking food" / "preparing food" or "bread - ready" / "bread ready". It is said in the paper that a single image has "a set of topic tags", implying that these very similar tags may apply to the same image, in which case it is unclear why they were not merged. The topics are not described, and while most topics do have self-explanatory labels, the boundaries of the category would need to be implicitly gathered from looking at the images under each similar topic and may be understood differently by different people.Perhaps due to their focus on improving the representation of certain countries/regions, the dataset does not equally represent all regions/countries.While it is a strength that the monthly income value is not a raw number to be compared across households, and instead standardized for comparability, this process is not described in detail (the only description is vague: lines 170-174). It would be much better if the process could be elaborated on and the reference explicitly mentioned to be a longer description of the calculation.There is not a lot of information about how the raw data was collected for the dataset - while it is understandable that the authors are not the same team which collected the raw data, the collection process needs to be described as well (or a description shared elsewhere, referred to, as with the income calculation).  |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper presents the Dollar Street Dataset which includes images with geographic and socioeconomic diversity.  A variety of different image recognition models are evaluated using the dataset and the main finding of the paper is that, without fine-tuning, models do substantially worse on images which are linked to lower socioeconomic status.  After fine-tuning, the gap decreases substantially.The main contribution of this paper is twofold: release of a new dataset and initial benchmarking across a variety of different image classification datasets. Clarity: This paper was clear and easy to follow for me.Quality: The main experiment in the paper seemed well done.  A few different models were evaluated including one model fine-tuned on the dataset.Originality:  The experiments are similar to other experiments in the literature, but the paper goes to great lengths to make the dataset easy to use.  For example, the paper specifies that all images have the right licensing for academic + industrial labs, images will be updated to comply with various standards (e.g., GDPR) and the dataset will be updated with additional images as they are collected.  All of this makes the dataset appealing.Significance:  The paper both provides a dataset that should be easy for others to use (across both academia and industry) as well as benchmarking a variety of models on image recognition across diverse images.  Besides highlighting an important problem for computer vision models, I could the dataset driving important research in this area.   In particular, one thing the paper did not mention is if they feel it will be easy to add new annotations to the dataset.  If this is the case, then the image data might be a rich source of images for a variety of base tasks (e.g., segmentation, detection, image captioning) similar to how COCO images have been reused for a variety of computer vision tasks. Quality:  The main experiment demonstrates that object recognition models will likely fail on data that represent a more diverse view of the world.  However, I had a few issues with the way the main experiment was done:In Table 3, we see that there are a variety of image resolutions.  Image resolutions/image quality can impact how well a vision model works.  Is image quality correlated with things like geographic region?  This might be an important confounder.I am somewhat concerned about the fuzzy matching done for labels.  In particular, there are some “noisy” mappings like books → bookcase and get water → water jug.  I would have liked to see more discussion about this.One experiment that could be run (and partially address the point above) would be to finetune the model on data from one socioeconomic quintile and test on other quintiles with the Dollar Street original labels.  This would “adapt” the vision models to the kinds of labels in the original Dollar Street dataset.Alternatively, in Devries + Misra et al., they do a human evaluation to see if labels predicted by models are accurate according to human judges.Though I think the above experiments would definitely strengthen the paper, I don’t question the results as they are in line with others results in this space (e.g., Devries + Misra et al.).I would have liked to see more example images or have had the chance to examine the images myself to make sure that they looked diverse and high quality.  One concern I have is that the images in Figure 1 do not include any complex scenes which might make the dataset less useful.Originality:  Others have performed other similar experiments (Devries + Misra et al.).  I see the contribution of this paper is to put together a dataset that can be easily used for this sort of experiment, which is not something done in Devries + Misra et al.Significance: As mentioned, the experimental results are similar to results obtained in other papers.  My main reason for rating this paper highly is that I believe it will be a rich source of data for the community.Devries + Misra et al:  (Does Object Recognition Work for Everyone?)[https://arxiv.org/pdf/1906.02659.pdf].  |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper presents the dollar street image dataset - a collection of image data depicting various household items (and several further categories such pictures of whole households or hand palms) typical for and used in countries of significantly differing economic wealth. The paper addresses the well-known problem that widely established machine learning datasets - and especially the ones consisting of images - have a significant bias towards higher-income countries and strongly underrepresent low-income ones. This imbalance does, for instance, lead to the situation that items such as "toothbrushes", "stoves", or "beds" as broadly used in low-income countries are detected as such by image recognition systems with significantly lower accuracy than those from higher-income countries. Foreseeable implications of such biases materialized in pre-trained models are manifold and vividly discussed in, among others, the FAccT community. In the most simple case, imbalanced dfatasets lead to image recognition software functioning worse in low-income countries (because of toothbrushes not being detected as such, for example). More severe impacts with complete devices not functioning properly are, however, also dicussed and increasingly reported.The dataset introduced with the paper is explicitly tailored to counteract such imbalances. It is based on a pre-existing image database collected by the Gapminder foundation (and additionally known from the Factfulness book), which was pre-filtered based on formal factors such as licenses that could otherwise hinder the dataset's use in research and industry. In addition, the dataset was extended with additional tags such as income of the represented household, country, region, etc. and packaged as a dataset for ML-based image classification. The dataset is provided under a CC-BY license, providing broad applicability in academia as well as in industry and thereby maximizing the possible impact. The authors definitely delivered a top-notch dataset paper. The addressed problem is clear and of utmost societal relevance, the approach of collecting and tagging large amounts of images showing "basically the same things" across countries of significantly differing economic wealth (and tagging them uniformly) is at the same time creative as well as solid and sound. Details of the dataset such as distributions across countries or items and the method used for income tagging are laid out properly and understandably (also supported by an extensive and well-prepared appendix). This confirms that the dataset is largely well-balanced and serves the goal it is intended for. At the same time, existing (minor) limitations of the dataset are also explained properly.Trust in the dataset actually serving its goal is also strengthened by the evaluation. Here, the impressively low detection accuracy of widely used pre-trained pytorch image classifiers for low-income images could be vividly confirmed ("all pre-trained models show monotonically increasing accuracy with increasing income quartile." -- what a statement!). At the same time, it is also demonstrated that fine-tuning a pre-trained model with images properly representing the global income spectrum significantly increases detection accuracy for low-income images. This paves the way for less biased and, in the end, less discriminating applications of image classification and recognition in a broad variety of usecases. The dataset thus provides a significant societal value and at the same time allows academia to base and evaluate their respective works on a societally and economically well-balanced and inclusive dataset. The strong support of the MLCommons NGO as well as the collaboration with the well-known Gapminder foundation only adds on top of this. The only possible weakness I might see lies in the reliance of a pre-existing dataset. The details of the collection of this underlying dataset are out of the authors' reach and could thus be subject to biases etc. However, the Gapminder foundation sponsoring the initial collection can also be considered scientifically trustworthy (probably even more than it is the case for other image datasets). Another possible "weakness" regards the extent of the evaluation. In the current form, it is unquestionably sufficient but some even more detailed analyses would be even more interesting.  |||| rating: 10: Top 5% of accepted papers, seminal paper |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	**[Contributions]**- The dataset includes 38,479 images of 289 common household objects photographed from 404 homes across 63 nations. Each image has full demographic information such as global region, country name and monthly income.- The raw data for this dataset comes from the Gapminder foundation—an independent Swedish organization with no political, religious, or economic affiliations.**[Why]**- The authors wish to reduce amerocentric and eurocentric representation bias in current datasets (ImageNet, OpenImages, etc.) that hurts the performance of classification tasks on images from other regions.**[Experiments]**- When the ResNet model is fine-tuned using this dataset, the performance of classification goes up by up to 50 percent on the evaluation set. **[Diversify the geographic and socioeconomic representation of the data]**- As an extreme example, the dataset includes images from homes with no internet access and incomes as low as $26.99 per 9 month. These images visually capture valuable socioeconomic diversity of traditionally under-represented populations.- This dataset can show people from the first world that the data we take for granted has bias. And one way to deal with it is to get data from areas that haven't been represented as much in the past. It demonstrates this with many examples. For example, Figure 1 shows different images of stoves by country and monthly income in USD. It reveals that there is a big difference between how everyday stoves look in different parts of the world and socioeconomic standing in income. Figure 6 vividly demonstrates all pre-trained models show monotonically increasing accuracy with increasing household income.**[Thoughtful curation]**- The data is thoughtfully curated and analyzed. Figure 4 shows, for example, that the number of images by monthly income is quite evenly distributed. The authors also provide a reasonable dataset maintenance plan.**[Commercial License]**- All images and data are licensed under CC-BY, which allows for their usage in both academic and commercial purposes.  This is in contrast to ImageNet, where access is restricted to non-commercial research and/or educational uses only. **[More impacts can be derived from the the value of the socioeconomic diversity captured in this dataset.]**- I believe the authors of this dataset can claim more impacts. Is the improved performance of classification the only positive thing that happened? If so, how does better classification of objects from different parts of the world affect people's lives? If so, how does the better classification of objects from different places improve people's lives?- Since this dataset is commercially accessible, it would have been great if the authors provided some practical effect of improved classification of everyday objects.**[Experiments]**- > We assess the performance of existing pre-trained models from torchvision on the top-5 multi-class classification accuracy using the mapped ImageNet classes as labels.    This is the only experiment in the paper, and it is a straightforward way to examine how biased existing pre-trained models are. However, for real change to happen, we need many researchers to add this dataset when they train their models. This way, it can be used by more people and the whole community can benefit from models that are less biased. In this regard, have you done any experiment in reverse? You might, for example, add this dataset to the training data of, say, ImageNet and test on ImageNet evaluation set. My guess is that it will probably hurt the performance. Then, the bigger question will be what kind of effort should we make as a community to encourage people to include this kind of diverse dataset to the evaluation set of widely used benchmarks.- After manually mapping the classes between ImageNet's 1000 classes and this dataset, you are left with 21K images. About 45 percent of the pictures were gone. I think that matching can be made better or made less strict.  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The presented paper introduces a new image dataset presenting household items with their respected income value. The dataset contains a new collection of images. Their income tag is estimated by the country income for this demographic group. Afterwards some comparisons with fine tuned networks have been done. A nice addition to this paper is the run benchmark by different image classification models. This shows the potential of this benchmark.  The paper does not have any specific weaknesses.  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	The paper addresses the issue of an existing amerocentric and eurocentric bias in widely used datasets leading to bias in many machine learning applications. So, the authors present DollarStreet, a dataset that of everyday household items from around the world with a variety of countries and incomes. The dataset contains this demographic information as well.The authors also present experimental analysis on the dataset. After mapping some of the categories to ImageNet and filtering the rest of the dataset, the authors run existing pre-trained torchvision models to show that models do not perform as well, demonstrating the diversity of the dataset compared to ImageNet and other existing datasets. The models also improve in performance with increasing income. The authors also finetune the pre-trained ResNet model on the training data to show the possible gain in accuracy when used on the dataset.  The strengths of the paper are summarized as follows:1. The authors address a very relevant issue and provide an intuitive solution. The data is quite diverse, and Figure 1 does an excellent job of depicting the diversity even within a label. The diversity of the dataset could truly help combat inherent bias in machine learning models. 2. The experiments are done well and Figure 6 is a helpful visualization of the model performance to drive home the point that models do better on "higher-income" representations due to the existing bias of the pre-training dataset (ImageNet). 3. The legalities, ethics, and logistics of the dataset are well handled. The authors have a good plan for how the dataset will be maintained. The weaknesses of the paper are summarized as follows:1. The results of the experiment are only presented in Figure 6. It would be more insightful to also see statistical results, and standard deviation.2. The fine-tuning of ResNet is mentioned but there is no presentation of what hyperparameters were tuned, what ranges were used, and what the final parameters were. 3. Since it is a completely new dataset, I would have appreciated thorough benchmarking of the dataset as well (preliminary results of completely training on the dataset and validation results).   |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct
hUjMhflYvGc	Dynamic Tensor Product Regression	long	The paper considers standard linear algebra problems with two twists on the input. First, the design matrix $A = \otimes_{i=1}^q A_i$ is a kronecker product of many smaller matrices, and it is these smaller matrices that are given to the algorithm. Second, it is a dynamic setting where, at every point of time, exactly one of the matrices undergoes arbitrary changes (so long as the matrix doesn't change dimension).In this "Dynamic Tensor Product" setting, the paper studies standard L2 regression, Spline regression (i.e. generalized ridge regression), and Low-Rank Approximation in the Frobenius norm. These problems have been studied in the _static_ tensor product regime, but not the dynamic one, so the new results focus on the improved rates for _dynamic-ness_. They initialize this line of research.They show that in this dynamic setting, by maintaining a binary tree of sketches of tensor products of $A_i$ matrices, their algorithms can achieve runtimes $q$ times better than the naive algorithm, where a new static data structure is generated at every update.This is a theoretical paper -- there are no experiments. This paper makes no claims about the optimality of their algorithms, and only gives upper bounds. The paper initializes an interesting line of research, it is very clearly and well written, it has a single core observation that explains the design of their algorithm and their improved runtimes. **It's a clean and easy paper to accept, but nothing groundbreaking.** I elaborate below.## OriginalityThe paper has two cores of originality:1. Initializing the research of dynamic tensor products regression and low-rank approximation1. Designing a dynamic data structure that can outperform repeatedly building the static data structureThe first point is stronger, since there seems to be no particular prior work on this precise problem, and because tensor product data does arise in practice (Lines 21-28). The second point is a bit weaker since the problem statement allows for updates to be *very* small -- just changing one of the $q$ base matrices, so an algorithm with updates that are $q$ times faster than rebuilding from scratch is ... believable. The way that the paper builds and updates matrices via sketches is nice and the intuition is clear, but it's hard to ascribe the construct a huge weight of novelty.## SignificanceThis ties closely into the "Originality" section above. The paper's new line of research, on specifically _dynamic_ tensor product regression and low-rank approximation, is conceptually significant. I don't want to undersell how important it is to formally design a theoretical problem. It can be hard to translate an intuitive notion like "tensor product regression, but in a dynamic setting" into a formal problem statement. Formalizing this is a legitimate contribution. That said, for a paper with no experiments which introduces a new problem to solve, I feel it is very important to state why your specific definition of a dynamic setting is meaningful. Recall their definition of the dynamic setting:> At every time point, one matrix $A_i$ in the kronecker product $A = \otimes_{i=1}^k A_i$ changes arbitrarily.Why is this the natural setting of dynamic tensor product regression that people in downstream applications would care about? For instance, the authors specifically site how large graphs are modeled as tensor products of smaller graphs, and that these smaller graphs can change dynamically as (eg) social networks evolve (Lines 36-41). Why should I expect a small update to a social network to appear as a single arbitrary change of only one of the $A_i$ matrices, and not as a super-sparse change to all of the $A_i$ matrices? Why doesn't the response vector $b$ change with each update? While the authors argue that dynamic tensor product regression is natural, **they don't justify their precise formulation of the problem**.For significance, it's worth examining the runtimes they achieve in clear detail. The body of the paper, i.e. before the appendix, is all written out in terms of a sketching dimension $m$, whose rates are not clearly stated (Line 199). Taking constant failure probability and constant error tolerance, we can state the runtimes of the static and dynamic algorithms for Least Square Regression pretty clearly:- Statically solving Least Squares Regression takes $\tilde{O}(\sum_{i=1}^q \text{nnz}(A_i) + qd^{3.5})$ time- Dynamically updating the Least Squares Solution takes $\tilde{O}(\text{nnz}(B_i) + d^{3.5})$, where $B_i$ contains the additive changes to $A_i$.- _(the details of how I derive these rate are in paragraph are at the end of this text box)_where $d$ is the number of columns of $A$, which is the product of the number of columns in each $A_i$, so generally a very large number even for moderate $q$.So the dynamic runtime improvement is genuinely by a factor of $q$, but a factor of $q$ seems very small compared to $d^{3.5}$. If each base matrix $A_i$ had exactly $d_0$ columns, then $d^{3.5} = d_0^{3.5q}$. So, saving a factor of $q$ time is nice, but the runtime is still prohibitive for even moderate $q$ and small $d_0$.**Sure, a factor of $q$ times faster is good, but it doesn't tackle the real bottleneck which should be the term that's exponential in $q$.**Of course, for tensor-shaped problem, this may be very hard to resolve. I mostly intend this little analysis to emphasize the scale of the improvements relative to the runtime of the algorithm. It should be $q$ times faster than the static algorithm, but if both algorithms have runtime exponential in $q$, then saving a linear factor seems relatively meager.## Clarity and QualityThe paper is well written overall. I have notes on clarity in the "Questions" box below. My only real qualm of clarity is that precise rates for $m$ should be stated in the body of the paper instead of the appendix, and the fact that $m = \Omega(qd)$ for all given constructions feels somewhat significant.---### Showing my workI'm taking $m=O(qd^2)$ from Line 618 where I use CountSketch and TensorSketch with constant $\varepsilon$ and $\delta$. Then, $md^{3/2} = O(qd^{7/2})$ and $md^{\omega-1} + d^{\omega} = O(qd^{\omega+1})$. Since $7/2 = 3.5 > 3.37 = \omega+1$, the term $qd^{3.5}$ which arises from the _Initialize_ and _Update_ runtimes (Lines 244, 246) dominates the _Query_ runtime (Line 249). So, the runtime of the static method is $O(\sum \text{nnz}(A_i) + qd^{3.5})$ and the runtime of a single dynamic update is $O(\text{nnz}(B_i) + qd^{3.5})$, even including the time to run the _Query_ procedure. No concerns  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper studies a lightweight computation of a special case of linear regression where the design matrix is given by a tensor product. Specifically, it considers the case where one of the factor matrices temporally changes. The authors propose a tree-based data structure to maintain the sketch of the data so that we can keep up with the temporal changes.  Strengths 1. (quality) Solid, reasonable data structure and algorithm are proposed. 2. (significance) The error analysis is provided.Weaknesses1. (significance) The motivation of the proposed method is not described. See Question 1.2. (clarity) Some notations are not clear. See Question 2.3. (significance) Empirical evaluations are not provided.  See Questions.   |||| rating: 5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	Given an input of $q$ matrices ${A_1, ..., A_q}$ (dimension of the matrices might differ) and a response vector $b$ of size equal to the product of all the input sizes from the $q$ matrices the 'tensor product regression problem' is to find the unknown vector whose dimension is equal to the product of all the dimensions from the $q$ matrices. such that it minimizes the $\ell_2$ norm of the difference vector between the response vector b and a vector which is a matrix-vector product between the Kronecker/tensor product all the input matrix and $x$ i.e., $||kr(A_1,...,A_q)x - b||_2$.This paper studies a dynamic version of the problem, where at each time stamp some matrix $A_i$ gets updated and the goal is to quickly update the tensor product and also quickly update the solution $x$ (without recomputing it). To solve this problem the author shows how [AKK+20] tree data structure can be used in combination with known sketching techniques. They also show its application to tensor spline regression problems and tensor low rank approximation problems.  Strength:- The paper introduces a dynamic tree structure to handle tensor product updates in $O(\log(q))$ time instead of $O(q)$ time.- The paper handles a more generalized version of regression and low rank approximation problems that depend on tensor products.- The paper is very well written. I liked reading it.Weakness:- Motivation for the problem is limited. Some experimental results or highlighting the exact improvements in terms of running time/working space compared to naive techniques could help motivate up to some level. - Once you build the tree, the majority of guarantees come readily from the existing results. - Here d itself is exp(q), so improving the running time in terms of $q$ is less productive. Typos/suggestions:- From line 173, I understand that the leaf $T_i = S_i*A_i$, however in line 200 you use $T_i$ to represent your sketch matrix. If I am correct then the notational inconsistency makes it hard to follow else please fig-1 as an example to explain the outline of tree line-197-209. - The sudden introduction of $J_{k,\ell}$ in line 203 is not clear. Again a relation to the fig-1 might be helpful. - In the related work please discuss more about known tensor decomposition techniques and if your low rank approximation can be used to get an approximate decomposition of the known techniques.  Yes, authors do talk about both scope of improvement and extension from the current result in section 9.   |||| rating: 4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
vGQiU5sqUe3	Contrastive Learning as Goal-Conditioned Reinforcement Learning	long	The authors have proposed a well-motivated contrastive learning method for solving goal-conditioned RL tasks, thus removing the need for any auxiliary losses or data augmentations for representation learning. The authors then show how this contrastive objective leads to a critic implicitly learning the Q-value function, and empirically demonstrate the effectiveness of their simple approach on many tasks.  # Strengths1. The proposed method simplifies the representation learning + planning in RL problem, by using the inner product of learnt representations as a correlate for the Q-value function.2. This is especially useful for the pixel-based environments, for which the authors have shown performance gains in many continuous-control tasks.3. I also appreciate that the authors mentioned certain failed experiments in Appendix H, which help in better understanding of the proposed method.4. The paper presents theoretical motivation for their method and show how this contrastive objective relates to a critic in deep RL methods. 5. Thorough comparison with several related works.# Weaknesses1. It is unclear why *random goals* are sampled to train the actor loss? Can the authors shed more intuition on this decision?2. Can the authors also comment if there’s some minimum goal dimension, after which contrastive RL starts performing better than the model-based baseline?3. In Figure 5, while contrastive RL and its variants surely perform better in simpler state-based tasks, however, looking at the plots for image-based observation tasks (Fig 5(b)), it seems that C-learning alone does quite well, especially in the challenging tasks of Sawyer bin. Also given that in fetch push (Fig 5(b)), contrastive RL (NCE + C-learning) gets a huge boost over other the baseline, I think that’s also because of the addition of C-learning pipeline only.  Yes, the authors have addressed the major limitations of their method.  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	This paper proposes leveraging contrastive learning to action-labeled trajectories, where the learned representation will connect to the goal-conditioned value functions. Specifically, they utilize contrastive learning to estimate the Q-function. The critic function (parameterized by the inner-product between representation) in contrastive learning can also be applied as the critic function in actor-critic algorithms in the RL context. Theoretical analysis of the convergence guarantees has been provided in the meantime. The empirical results suggest that the proposed framework can lead to better performances on goal-conditioned benchmarks compared with prior methods, including methods using goal-conditioned RL and different representation learning strategies.  ### *Strengths*#### **1. Originality and significance**Although this work is built upon existing literature on goal-conditioned RL and contrastive learning (in RL), it has enough novelty since it provides a clear direction to link the reward maximization and learning representation using contrastive learning. The way offered in the paper to directly use contrastive learning to perform goal-conditoned RL is very simple and efficient. This work may have the potential to benefit the RL, representation learning, and robotics communities. #### **2. Relevance**The authors discuss most related works, including goal-conditoned RL, representation learning for RL, and contrastive learning (for RL). Detailed comparison has also been given in the associated sections. #### **3. Algorithms, theories, and evualation**The designed algorithm is decent, and I feel this can broadly apply to other RL regimes. The convergence guarantee makes it more solid. The experiments part is also very solid, including most relevant baselines and benchmarks. The appendix H on failed experiments is also very helpful for the community. ### *Weaknesses*Please note that below are only for questions and potential discussions. There is no need to rerun experiments during the rebuttal phase.#### **1. About experiments**It seems that, in some cases, improvements compared with C-learning are minor (Fig. 5). Can the authors give some explanation. Meanwhile, why not compare with other contrastive learning approaches in RL mentioned in the early sections?#### **2. About other common challenges in goal-conditioned RL**Can this approach tackle common challenges in goal-conditioned RL, especially for off-line datasets? e.g., the generalization ability to different distributions.#### **3. Minor typos**-> Line 107: $\pi\left(\tau \mid S_{t}\right)$ -> $\pi\left(\tau \mid S_{g}\right)$;-> Eq. 4: consider using another color, the current $v^{+}$ is hard to recognize when print out the paper to read.  The authors mentioned the limitations in the paper, mainly about how to generalize the work into other RL settings. I think this would be the potential direction for future works. I raise a few questions, which are more like vague points instead of limitations. I think this is a decent paper and I would vote for acceptance.   |||| rating: 8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	The paper presents a novel view of how contrastive learning can be used by itself, and without any additional RL training on top, to learn goal conditioned policies from pre-collected data. This is achieved by using the similarity learned during contrastive learning as a Q-function, that is then used to improve a policy in the policy gradient step. It is also shown, via a proof, that the contrastive objective estimates the Q-function, and some additional convergence guarantees are provided. The authors well motivate their approach, compare it extensively with related work on learning goal-conditioned policies, as well as works that use an additional representation learning objective on top of a RL algorithm. The paper shows a good comparison between prior work and the proposed method on a range of goal-conditioned RL tasks that have been used in these prior works. The overall conclusion is that a much simpler method, like the one presented, can perform similarly well, and in some cases even better, than more complicated baselines. ### Strengths1. The paper presentation and clarity is excellent and of high quality. The authors have performed an extensive evaluation of their method and compared it against two sets of baselines, one based on representation learning methods and one based on learning goal-conditioned policies. The authors describe their method, motivation and results very clearly and do a very good job at relating it to prior work.2. Their method is grounded in theory, though I cannot claim I could follow 100% of their proofs and arguments as I am not an expert on the theoretical side.3. Simplicity of the method, while simultaneously performing similarly well to other methods, and in some cases outperforming strong and more complicated baselines.4. The authors present an original perspective on how contrastive learning can be used to learn goal-conditioned policies directly.I did not find any noteworthy weakness to the paper other than some typos and some more questions about the limits of this method that I have included in the next section with Questions.  The limitations have not been added to the author’s submission checklist yet, but are addressed in the actual paper.  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
O-r8LOR-CCA	Open-world Semi-supervised Learning	long	The paper considers open-world SSL settings  where the model recognizes previously seen classes, and detects novel classes which are not present in the labeled dataset. The method contains three losses to train a model in this setting: a) supervised loss on labeled data, b) unsupervised loss on unlabeled data from pseudo-labels obtained  from confident pairwise similarities, and c)  regularization term that avoids assigning all the unlabeled samples to the same class. Then the paper evaluates the effectiveness of the  method on CIFAR-10/100 and ImageNet-100 datasets.The paper is well organized, well-written, and tackle SSL problem in a more realistic setting. Experiments are enough to some extent. However, I feel that the paper just combines several well-known and well-studied techniques for different tasks which intersect with open-world SSL settings. (e.g., self-supervised learning in SSL, clustering for SSL or clustering for transferring knowledge  across domains and tasks, using confident pseudo-labels for training an SSL model, etc). From this perspective, technical novelty of the work is limited, although experiments show good results for open-world SSL setting. -First, I think this set-up does not contain all the scenarios for a real-world SSL. For example,  this set-up does not consider covariate shift where the data belonging to the same classes but different image statistics or style (e.g., dogs in natural images, dogs in the painting or sketches images). However, in the real-world scenario, we may have images of the same class but different domains. -The paper adds several losses studied in the literature without analyzing if any has a negative effect on the others in the open-world SSL setting.-Assuming that the number of novel classes |C_u| is known is a bit unrealistic to me in the open-world SSL setting.-The method ranks the distances, and for each sample generates the pseudo-label for its most similar neighbor. However,  in an open world SSL setting due to the very-limited-label regime, the representation may not be ideal, and  therefore, pairwise-labeling may not be ideal and can possibly propagate the error through the network over the course of training.-Does the regularization towards uniform distribution consider unbalanced novel classes which is common in open-world SSL? -Does the method perform well in cases where the test set contains novel classes that do not appear in the unlabeled set?Generally, the work has some potential from practical point of view, however, it needs more work to be improved technically.   |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	Pros- The paper explores an interesting semi-supervised learning (SSL) setting in which the unlabeled data contain not only the seen class but also novel classes. The problem is interesting in that it is more practical than the classic SSL setting in the real world and has been seldomly researched. - The method includes a self-supervised pretraining step and a finetune step. The finetune step jointly solves the classification task and the clustering task with a unified objective loss. The supervised loss, which is the main contribution of the method, overcomes the imbalance problem caused by BCE with a novel adaptive margin-based loss. The authors validate this loss with empirical results.- The paper is well-organized and clearly written. As far as I can see, the method is technically sound.Cons- The comparison methods (such as pseudo-labeling, DS3L) are not strong enough. It is suggested to compare with more SOTA methods. Since SimCLR is more powerful than RotNet, it is strange to see RankStats get worse performance than the original paper.- The number of novel classes has to be prefixed. |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	--Paper summary--The authors propose a method to tackle a new problem setting of semi-supervised learning, called an open-world semi-supervised learning, where the model is required to accurately discriminate known-class data as well as to appropriately discover unknown classes contained in an unlabeled dataset. The objective function to be minimized in the proposed method comprises three terms: unsupervised loss, supervised loss with uncertainty based adaptive margin, and entropy regularization. The experimental results with several datasets have validated the advantage of the proposed method.--Review summary--This study is well-motivated and tackles an important problem that would occur in real-world applications. The design of the proposed method seems reasonable, and it works well in the experiments with several datasets. However, some points remain unclear or are not convincing, which makes my score a bit conservative. To appropriately determine my score, I would appreciate if the authors clarify these points in their response.--Details--Strength- The setting of the open-world semi-supervised learning is interesting and should be practically important.- The paper is well-written and well-organized.Weakness and concerns- It is not clear whether the design of the proposed method really leads to the improvement of the performance or not. Since the authors use SimCLR to pretrain the model, the pretrained model has already had a substantially good feature representation. Consequently, if we know the number of classes in the training data, unsupervised learning methods like [R1] (or simple clustering methods) might work well to discriminate all classes. Although the authors adopt a new idea to the supervised loss, it seems unclear whether the supervised loss itself really contributes to good performance. Are there any experimental result on performance sensitivity to \eta?  [R1] "Learning Discrete Representations via Information Maximizing Self-Augmented Training," ICML 2017.- The experimental setup is not convincing. Due to the motivation of SSL, the number of unlabeled data should be much larger than that of labeled data, and it is often tens or hundreds times larger in the literature. However, in this paper, it is basically 3~7 times larger in the experiments. Is there any reason why the authors did not much reduce the number of labeled data?- Two concerns on uncertainty based adaptive margin.- How did the authors estimate the class posterior probability in Eq. (4)? Is it just the output of the softmax function?- Since this adaptive margin is adopted to improve the accuracy of pseudo-labels, how much it is improved should be reported in Fig. 3.- How did the authors conduct validation? Due to the existence of unseen classes in unlabeled data, how to conduct validation is not trivial.- Is it reasonable to call L_BCE unsupervised loss? Since Z_l' is obtained using ground-truth labels, L_BCE cannot be computed in an unsupervised manner.Minor concerns- I could not get the reason why the proposed method is called ORCA.--After receiving authors' response--I would like to thank the authors for providing additional experimental results and giving clarifications. Since my concerns are almost solved, and I updated my score from 5 to 6. This study would be a great first step to tackle the challenging problem, open-world semi-supervised learning, though there are several remaining issues (e.g., validation is hard to conduct, the number of unknown classes should be known, etc.). I vote for "weak accept." |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	Summary: the authors propose Open World Semi-Supervised Learning (ORCA), a semi-supervised method that learns to classify previously seen classes in the labeled data and novel class in the unlabeled data. The proposed method is end to end and achieves significant improvement on ImageNet dataset compared to baseline methods.Pros:The paper takes one of the most important issue of semi-supervised learning: recognizing novel classes. For me, the problem itself is real and practical.The experiments are strong and methods shows significant improvement compared to competing methods.Cons:The paper lacks clarity in some sections. The writing could be better and the notations better defined.Comments:The unsupervised loss function is not clear to me, I find it difficult to parse equation 1 from the way it's written. Could you please explain the equation better?It's my understanding that each z_i in the unlabeled set is assigned a class based on the ranking of pairwise distance in the mini batch.  If the mini batch of z_i contains labeled samples, then it could be assigned to the labels example if the images are similar. For example if there's an unlabeled example of a horse, it could be assigned to the labeled donkey class. Wouldn't this be problematic for the novel class prediction if the batches always contain similar examples from different classes? Why the choice of cosine distance for the similarity measure? Using cosine similarity means that you do not consider the magnitude of the embeddings. This would make sense for text embeddings but the experiments are on image datasets |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct
_bqtjfpj8h	Ask4Help: Learning to Leverage an Expert for Embodied Tasks	long	This paper proposed a method to decide when to ask for expert’s help to improve the task performance of embodied agents. Instead of using heuristics, e.g. model confusion, or expanding the action space of the embodied agent, this paper learns a separate policy on top of the decisions of off-the-shelf embodied agents using reinforcement learning.   The experiments on RoboTHOR show that the proposed method can achieve higher success rates using fewer expert queries.   Strength:- The proposed method is generic. Without modifying the trained embodied agent, the method can learn when to perform expert queries.- The proposed method reduces the number of expert queries compared to baselines such as model confusion and naive helper.- The presentation of the paper is clear. It is easy to follow the paper. Weakness:- Missing ablations on different key components. There are two major inputs for the Ask4Help model, success rate prediction and the embodied agent’s belief, it is unclear the contribution of each component. The success rate is a strong indicator of whether the embodied agent needs help. It is likely that a classifier based on success rate is sufficient.- Unclear how to adapt to different reward preferences at inference time. Ask4Help trains the model with a sample of different reward configurations, but the paper doesn’t show how to estimate the expert’s reward configuration at inference time. Do experts select which reward profile they want? It is also possible that an expert adjusts their reward preference while interacting with the system.   This paper has discussed the main limitation of the proposed method which provides opportunities for future work. Another limitation of the proposed method is the fixed user preference, the method doesn’t adapt the number of questions asked based on the interactions with experts.  |||| rating: 4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper is about learning an ask for help (ask4help) policy on top of an already trained embodied agent. Specifically, the ask4help policy first measures the agent's uncertainty at finishing the given task based on its internal belief state using a pretrained Success Prediction Model (SPM). Then, the ask4help policy decides if it should ask for the expert's help (i.e. the next action to do in the environment) or use the pretrained embodied agent's predicted action. In addition, the ask4help can be adapted at inference time to different users' preference regarding the frequency of being "disturbed" by the agent to answer it. Experiments were conducted on two different environments: the RoboTHOR Object Navigation and the Room Rearrangement in iTHOR. Empirically, it was shown that ask4help manages to yield a very higher success rate while requesting the least amount of expert feedback compared to different baselines: the Embodied Agent only, a Naive Helper with predefined frequencies for asking for help, and Model Confusion that measures the agent's uncertainty based on the confidence of the predicted action rather than relying on the agent's belief state. **What I like about this paper**- How to reuse existing trained models is an important research direction with positive environmental impact.- The authors propose a solution to train a single ask4help policy that can deal with different user preferences via sampling different reward configurations during training.**Potential weaknesses**- It is not clear to me what reward configuration is used at inference time? Section 4.3.2 and 4.4.2 do mention a **single** reward configuration used for training.- In 4.4.3, it is said "If more performance models are proposed for the task, the Ask4Help policy will accordingly adjust the amount of expert help requested...". That sentence seems to imply that this will happen at inference time. However, it is my understanding that both the ask4help policy and the Success Prediction Model will need to be retrained to accommodate for the likely different embodied agent's belief $b_t$.**Originality, quality, clarity, and significance**This work shares many similarities with active learning but takes place in interactive environments. The proposed approach is a novel combination of existing techniques applied together to tackle the important research problem of deciding when to ask for help. I could see future work building on this to integrate better with human workflow (e.g., using language to ask questions and interpret the answer similar to [Asking for Knowledge, Liu et al., ICML2022]). I found the paper well-written and well-organized. Figure 2 helped me understand the model overall but it was not clear how the Success Prediction Model was getting any training signal since the gradients were blocked. I realized later it is pretrained and frozen. The submission seems technically sound to me except for how the different reward configurations were defined during training.Overall, I tend to recommend this paper for acceptance because of the research problem being addressed and the proposed solution that doesn't require training an embodied agent from scratch. I might have missed some flaws, especially with respect to active learning. The authors did mention two main limitations of the proposed approach. At the moment, expert feedback is not used to improve the embodied agent which can be annoying to a human user. The second limitation of this work is the authors used proxy experts (available for the tested environments) to train the ask4help policy. Using human-in-the-loop for training is not explored in this work (it is still an active area of research).  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This is a strong paper that examines the question of how to enrich existing policies for embodied tasks (in this case, object navigation and rearrangement) with the ability to “ask for help.” Unlike prior work that defines heuristics for model uncertainty, requires extra supervision in terms of language or subtasks, or requires retraining the base embodied agent policy, the proposed approach — Ask4Help — learns a base policy-agnostic approach for learning when to ask for help, *without the need to retrain the base policy*. Formalized as a separate policy that takes in a set of user preferences, the “belief” (hidden state) of the embodied base policy, and a small number of interactive rollouts in known environments, Ask4Help is able to learn to trade-off task success with “expert load” (amount of queries to the expert) with minimal data, and with remarkable results on two benchmark tasks — RoboTHOR object navigation and AI2-THOR room rearrangement.This paper further goes above and beyond to show how the proposed system compares to scenarios where users have different preferences (weightings on success rate vs. desired query load), comparisons to ablations with fixed probabilities of picking “expert” actions, comparison of querying actual (vs. synthetic) humans, as well as the robustness of Ask4Help in the presence of noisy experts. This paper is original and significant, proposing a system that not only makes sense and seems necessary as we build stronger, more powerful embodied systems. The evaluation is incredibly convincing, and the ablations are well chosen and are crucial in showing the efficacy of the proposed approach.The clarity of the paper is also an added bonus — the motivating examples were clear and helped ground out the early parts of the paper, the approach is simple yet flexible, and in general, led to a strong paper.The sole weakness is that I wish the user preference component had a little bit more discussion; it’s still not clear to me how this component interacts with the rest of the system (and especially the training/reward function), nor how the various “preference embeddings” are specified/chosen. This paper is very transparent about its limitations — namely that this Ask4Help approach only augments an existing policy with the ability to ask for help, not necessarily learn from the provided expert feedback.I agree that this is probably out of scope for this work, but for future work I’d suggest the authors look into frameworks like Lazy or ThriftyDAgger as ways to bootstrap systems that (1) know when to ask for expert help, and (2) learn/update policies based on that information.  |||| rating: 8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area, or high-to-excellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper introduces a method, Ask4Help, for incorporating expert knowledge for embodied AI. The authors leveraged a pre-trained embodied AI agent and designed a policy to switch between agents' prediction and expert action as the final acting policy. The switching policy is trained by RL given  the reward from both success rate and the portion of expert knowledge used in the episode. The resulting human-in-the-loop policy achieves performance improvement on several common embodied AI challenges. [+] The problem of incorporating expert knowledge and collaborating with humans is becoming increasingly important over the past few years, especially with more agents showing strong potential for indoor tasks. This makes the main topic of this paper important and meaningful.[+] The overall writing of this paper is clear and illustrative with ideas, methods, and results clearly stated. The authors showed significant performance improvements on several embodied AI challenges by augmenting a pre-trained embodied AI agent with expert knowledge. This shows strong potential for the pipeline of prompting large-scale pre-trained agents with expert knowledge, especially with recent trends on language understanding (e.g. few-shot capabilities of GPT-3).[-] The major concern of this paper comes with its design and evaluation metrics. The authors stated in the limitation section that the proposed model possesses the disadvantage that it can not learn from the expert's feedback. This, in my opinion, is a critical issue since it always requires expert knowledge to perform well and fails to leverage extra in-context expert knowledge. As the current policy only switches between the pre-trained agent's action and the expert's action, I don't see a way to make the current system a self-adaptive one when put into a new training scene. This makes the whole point of designing such a policy for querying expert knowledge questionable.[-] Following the previous point, I do think the current experimental settings and results are not convincing or have little impact. The authors used RL to train the Ask4Help policy for deciding which action to use (expert's or the model's) and seem to provide full accessibility of expert knowledge when generating the final scores (since the model can choose the amount of expert knowledge to use), this makes the current results hard to interpret as the model can always learn to use more expert knowledge without a limit. In this case, a proper comparison in my opinion should be constrained under the amount of expert knowledge available and over the performance gain of applying each method. Next, the baseline models are also designed to follow the same pipeline of policy switching without baselines from previous works that leverage the same amount of data for imitation learning or refinement. The authors have partially stated their limitations on continual learning. However, I think there is still more work to do to make the current experimental design solid and sound. I hope the authors would consider the facts stated previously in Weakness and Questions, especially on the significance of the results since it now needs all expert knowledge during training and adopts partial of them during acting. This setting, to me, is not reasonable and might need better adjustment.  |||| rating: 4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	The paper introduces Ask4Help, a method for augmenting an existing policy with the ability to fall back to an expert policy during an episode. This is achieved without retraining the existing pre-trained agent by introducing a meta-controller that will select whether to follow the agent or the expert at every timestep. The meta-controller does not receive raw observations, but the agent's belief state, a prediction of the agent's success rate, and an embedding informing it about the user's preference (i.e. how costly it is to ask for help). The proposed method is evaluated on two tasks, namely object navigation (RoboTHOR) and room rearrangement (iTHOR), where it greatly boosts the success rate of the pre-trained agent while comparing favorably to other baselines in the amount of expert usage. **Strenghts**- The problem of providing agents with the ability to ask for help is an important one.- The paper is easy to follow.**Weaknesses**- The method itself is not novel, as it can be seen as a Hierarchical RL with two low-level policies: the pre-trained agent and the expert.- While the quantitative results look strong, the videos in the supplementary material show that the discovered behavior is extremely simple. During a first phase the meta-controller selects the agent, which does not seem to know how to solve the task and simply roams around the room. Then, it selects the expert for a few timesteps -- which finds the object and solves the task. This results in high success rate (thanks to the hard-coded expert) but low expert usage (which is diluded due to the long initial phase where the agent is used). It would be helpful if authors could provide some statistics about expert usage aggregated over all tasks (e.g. detailed stats about the timesteps in which the expert is used within an episode, aggregated over all episodes).- In light of the aforementioned qualitative results, I am not convinced that the baselines are adequate. There are some additional baselines that would help understanding whether the Ask4Help policy is learning something trivial or not:  - Replacing the pre-trained agent with a random and/or no-op policy. This would provide insight about whether it is really learning to combine the pre-trained agent and the expert, or whether the observed gains just come from the fact that the expert will always solve the task when given enough time.  - Replacing the meta-controller with a hard-coded policy that selects the agent for M steps and then runs the expert for N steps (where both M and N should be swept over).- The Ask4Help policy has a non-standard architecture, e.g. it takes the predicted success rate as input. There are no ablation studies in the paper (nor the supplementary material), which make it difficult to understand whether this is an important component of the agent or could be removed.- The dataset split described in L208 is non-standard in machine learning. My understanding is that the purpose of the split is to deploy the agent on a set of tasks it has never seen before and where it might not perform well. However, why isn't there a third, truly held-out, set ot tasks where one can evaluate whether the Ask4Help policy generalizes? Otherwise, if one is allowed to train and evaluate on the same set of tasks, why can't we re-train the agent on the validation set instead of using Ask4Help? Yes.  |||| rating: 4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
pLxPPTPd	On Optimal Learning Under Targeted Data Poisoning	long	This paper studied developing robust algorithms in presence of an attacker who can corrupt a portion of the training set in the classical version space learning scenario. In the realizable scenario, the paper provided impossibility result that shows if the attacker can perturb eta fraction of training samples, then the test accuracy cannot be lower than eta*d with high probability, where d is the VC dimension of the hypothesis class. To complement this result, the paper then proposed robust PAC learning algorithms that provably achieve this upper bound, which perfectly closes the robust learning problem in the realizable case. In the agnostic scenario, the paper showed that no algorithms can achieve 2*OPT test accuracy, where OPT is the optimal error rate without poisoning. Then the paper proposed a robust algorithm that achieves O(OPT)+delta error, where delta depends on the VC dimension. Strengths:(1). The paper provided a very comprehensive study of PAC learning under data poisoning attacks. When the attacker can corrupt eta fraction of the training set, the paper showed impossibility result and proposed robust PAC learning algorithms that achieve the error lower bound. This result is fundamental as it extends PAC analysis from traditional machine learning to the robust machine learning domain.(2). The theoretical results developed in this paper is important. The proof techniques are novel and can be used to study related topics in future research. The topic is extremely fundamental in the machine learning community, which help researchers builds deep understanding of data poisoning attacks.(3). The paper is well-written and the structure is articulated in a very clear way. The annotation goes from realizable scenario to the agnostic scenario, which makes the paper easier to read and understand.Weaknesses:(1). I think the threat model studied in this paper is "untargeted data poisoning attack" instead of "targeted poisoning attack". In typical targeted poisoning attack, the goal of the attacker is to change the prediction on a test example to a pre-specified target label. However this paper considered poisoning attacks that aim at reducing the test-time accuracy overall, which is often consider as untargeted poisoning attacks. I suggest the authors check if the terminology is aligned with the SOTA literature.(2). In the agnostic scenario, while the paper proved that no learners can achieve 2OPT error rate under data poisoning attacks. It's not very clear how the upper bound O(OPT)+delta compares to the lower bound. That said, I am not sure if the upper bound achieved by the proposed robust PAC learner is tight or not. I hope that the authors could elaborate more on this point. Yes  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	This work studies the optimal rate of learning for binary settings under a fraction of $\eta$ poisoning data under targeted data poisoning attack. Under realizable setting, the error rate is proportional to $\eta$ and the VC dimension of the hypothesis set, which can be attained by a deterministic learner. Under agnostic setting, for any deterministic learner, a lower bound is present to show an inevitable multiplicative deterioration in the regret. Strengths:1. This work shows that under the realizable case the adversarial risk bound is tight in terms of corruption fraction and VC dimension of the hypothesis set.2. This work provides a lower bound on the adversarial error for agnostic setting.Weaknesses:1. I find the poisoning scenario that is described a bit confusing. Notice that targeted poisoning can also be regarded as selecting a targeted label $y_t$, and adding poisoning data to make the test sample misclassify as the targeted label $y_t$. But here to my understanding, the targeted poisoning is the adversary can see the test sample ahead of time so that the adversary can add poisoning data to make the test sample misclassify.  Yes.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	This work studies learnability under a targeted poisoning attack.  Specifically, they consider the best achievable error in both the realizable and agnostic settings.  Their method focuses on $\ell_0$ attacks and is fundamentally based on the ensemble-based robustness proposed by Levine and Feizi [2020]. This work follows in a long tradition of learnability under "noise" (defined broadly).   In particular, the authors address several open questions from previous work.  For example, Gao et al. prove a general bound of $o(1)$ but do not consider specific poisoning fractions, e.g., ${\eta = \frac{1}{100}}$.Overall, I enjoyed reading this paper.  Its organizational structure was clear.  The ordering of the arguments allowed me to easily follow along without having to go back and re-read previous sections. I thought it was well done. The authors do not include a "Limitations" section nor did I expect one for this type of theory paper. I would have preferred some intuition in the main paper regarding the origin of the "carefully chosen" subset sizes in Figures 1 and 2.While the writing overall was clear, the number of typos was a problem, and if this paper is accepted, it needs thorough proofreading.  Some errors are more egregious than others.* Lines 153 & 155 "Gao et al. Gao et al." as well as "Balcan et al. Balcan et al."* Line 284: "Besides of Prediction" -- two issues* Line 32: "ensure the trustworthy of such" * Line 339 & 346 regret boundLine 39: "can can"  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	The paper information-theoretically studies the possibility and impossibility of learning from poisoned data, in a supervised learning setting with deterministic model predictions, against a relatively powerful attacker who knows the data $x$ on which the model will be tested (or deployed). Remarkably, in the realizable setting, they prove matching upper and lower bounds, up to a multiplicative constant. Namely, this error is $\Theta(\eta d)$, where $\eta$ is the fraction of poisoned data and $d$ is the VC dimension of the class of hypotheses.In the agnostic setting, the paper also provides interesting lower and upper bounds, though the gap is then arguably larger. Namely, they prove that the error is at least (roughly) $2 OPT + \Omega(\eta) - o(1)$, for a large number of training data, while their upper bound only guarantees $\mathcal O(OPT + d \eta)$.The authors also note that their solution in the realizable setting is not proper (the learned hypothesis aggregates the predictions of several hypotheses). In the specific case of halfspace learning, they propose a construction of a proper hypothesis, but the guarantee is significantly worse ($\mathcal O(\eta d^3)$). I believe that this paper is an exciting important step towards understanding the possibility and impossibility of learning. So far the literature has only provided guarantees in the case of a vanishing fraction of poisoned data. However, especially in applications like online content moderation, this assumption is very unrealistic. Arguably, as a model gains more users, while it gains more data, it also attracts more malicious users, which may thus represent an increasing fraction of the provided data. Given this, I believe that the paper provides an original and very significant contribution to the important literature on adversarial machine learning.The paper is well-organized, but there are many typos which make the reading challenging at times. Below is a list of such typos (the list is not exhaustive):- Line 53: “Due to adversary” => “Due to the adversary”- Line 58: “even if the adversary knows” => “especially if the adversary knows”- Line 66: “the Gao et al. [2021]” => “Gao et al. [2021]”- Line 153: “Gao et al. Gao et al. [2021]” => “Gao et al. [2021]”- Line 155: “Balcan et al. Balcan et al. [2022]” => “Balcan et al. [2022]”- Line 156: “under instance targeted” => “under instance targeted attacks”- Line 161: “Rosenfeld et al. Rosenfeld et al. [2020]” => “Rosenfeld et al. [2020]”- Line 209: “depend its changes” => “make its changes depend”- Line 212: “based on adversary’s power” => “based on the adversary’s power”- Line 216: “In Definition B.1 present” => “In Definition B.1, we present”Also, the word “semi-agnostic” is not defined in the paper.Finally, the paper should clarify the difference between what they regard as “targeted attacks”, and what is sometimes known as “model-targeted attacks”, where attackers want a specific target hypothesis to be learned.- https://proceedings.mlr.press/v139/suya21a.html - https://arxiv.org/pdf/2202.08578.pdf (to appear in ICML’22)EDIT after reading the authors' responses:The authors' responses were satisfactory to me, which motivated me to increase my rating. It is a shame that the introduction examples that motivate the paper are unconvincing. It is indeed unclear how and why an adversary would fool autonomous cars into accelerating when shown a stop sign. It is also unclear how a loan applicant could poison a bank’s training set.I strongly urge the authors to provide much more compelling examples, which I think are numerous. For instance, in content moderation, a troll farm employed by an authoritarian regime or a multinational firm could flag dissents’ and critics’ content on social medias to fool the moderation algorithm into considering that all such critical content should be taken down. As another example, a language model trained on “social media conversations” (like PaLM https://arxiv.org/abs/2204.02311) could be hacked by propaganda and marketing campaigns, who may want the word “meat” to be systematically associated with “delicious” rather than “not environmental-friendly”. Today, the training datasets of such algorithms are arguably full of poisonous attacks (Facebook reports removing 15 billion fake accounts every two years: https://www.techdigest.tv/2021/09/facebook-removes-15-billion-fake-accounts-in-two-years.html).In particular, the attacker’s knowledge of $x$ can be justified in practice by the fact that, in may use cases where the model is trained on user-generated data, attackers are also users who will interact with the trained algorithm, and provide the values of $x$ that the model will process. This use case belongs to the family of backdoor attacks, which the paper would gain by mentioning.I believe that the authors should also discuss the role of $d$. Their lower bounds essentially prove that larger models are more vulnerable to adversarial attacks. This is a very important discovery, as it strongly suggests that the race for ever larger models is also leading to significantly more vulnerable models (if we are to believe the paper).  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
_qoQkWNEhS	Ricci-GNN: Defending Against Structural Attacks Through a Geometric Approach	long	##########################################################################Summary:The paper proses a new adversarial (poisoning) defense based on a known graph reweighting scheme known as the ricci curvature. The ricci curvature assigns a weight to each edge that captures the graph structure, i.e. the value reflects whether the edge is an inter-community connection or an intracommunity connection. Empirically, the ricci curvature is known to be more robust w.r.t. random edge insertions/deletions. The authors propose a new sampling method based on the ricci curvature and use it within their novel training scheme. Empirically, the effectiveness of their approach is shown via experiments on synthetic SBM graphs. Moreover, the authors use a random attack and Metattack on various datasets. They show superior performance to multiple baseline architectures/defenses. ##########################################################################Reasons for rating:Overall it is an interesting work and the empirical performance seems to be good. However, neglecting the very weak random attack and the experiment on synthetic data, the authors effectively only evaluate against one strong attack. Hence, the question arises if the defense is solely effective against the characteristic of Metattack? I would recommend to add at least one further strong attack.Furthermore, it is not clear if a transfer attack is used (a surrogate used for Metattack). It would be very interesting to see if the ricci curvature calculation itself is adversarially robust. In the chosen setup this fact is obfuscated.Last, the authors cite but do not compare to the Curvature Graph Network which also uses ricci curvature instead of the widely used symmetric normalization of the adjacency matrix (e.g. as a GCN). Hence, it is not clear to the reader if the sampling scheme/training scheme or the ricci curvature is the major reason for adversarial robustness. ##########################################################################Pros:+ Interesting and promising approach+ Consistently improved performance over the baselines+ Interesting analysis of their defense via SBM graphsCons:- Only one strong attack on real-world graphs is used to benchmark to other architectures.- Is the ricci curvature itself robust w.r.t. adversarial attacks? The authors seem to use transfer attacks and the referenced literature claims only robustness against random attacks and this is also only evaluated empirically.- Curvature Graph Network should be added as a baseline.- The authors do not discuss the space and time complexity. Only the time cost for the ricci curvature is discussed. Moreover, the authors use the two-hop neighborhood for adding potential edges — this can still be very expensive specifically for power-law graphs. A discussion would be appreciated.Further points:- The proposed sampling based training scheme seems to be highly related to adversarial training. The authors should add a corresponding discussion.- The paper lacks clarity at some points and has inconsistencies in notation. For example, in Section 2 "F" is not introduced. S denotes the geodesic distance (aka length of shortest path) which is denoted by d(x, y) in Section A.- At some points the authors say that a graph is sampled in each "iteration" (epoch) and sometimes for every layer.- Figure 2: What is H_0, H_1, L?- Section 1.1: The authors should make clear that Figure 3 is an example and does not imply superior robustness in general.- What are the limits of ricci curvature? Beyond some level of perturbation, there should be a tipping point (i.e. communities cannot be distinguished anymore).- Related work: There are many more (relevant) attacks/defenses. Please add them and/or make clear that your discussion is not exhaustive.  |||| rating: 5: Marginally below acceptance threshold |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature	Strengths: The paper is well written and clean. Weaknesses: I have several concerns regarding this paper. •Novelty. The authors propose to use Ricci flow to compute the distance between nodes so that to sample edges with respect to that distance. Using Ricci flow for distance computation is a well-studied area (as indicated in related work). The only novel part is that each layer gets a new graph; however, this choice is not motivated (why not to train all layers of GNN on different graphs instead?) and has problems (see next). •Approach. Computing optimal transport distance is generally an expensive procedure. While authors indicated that it takes seconds to compute it on 36 cores machine, it’s not clear how scalable this method is. I would like to see whether it scales on normal machines with a couple of cores. Moreover, how do you compute exactly optimal transport, because the Sinkhorn method gives you a doubly stochastic matrix (how do you go from it to optimal transport?). •Algorithm. This is the most obscure part of the paper. First, it’s not indicated how many layers do you use in experiments. This is a major part of your algorithm because you claim that if an edge appears in several layers it means that it’s not adversarial (or that it does not harm your algorithm). In most of the baselines, there are at most 2-3 layers. There are theoretical limitations why GNN with many layers may not work in practice (see, the literature on “GNN oversmoothing”). Considering that you didn’t provide the code (can you provide an anonymized version of the code?) and that your baselines (GCN, GAT, etc.) have similar (or the same) performance as in the original papers (where the number of layers is 2-3), I deduce that your model Ricci-GNN also has this number of layers. With that said, I doubt that it’s possible to make any conclusive results about whether an edge is adversarial or not with 2-3 graphs. Moreover, I would expect to see an experiment on how your approach varies depending on the number of layers. This is a crucial part of your algorithm and not seeing discussion of it in the paper, raises concerns about the validity of experiments. •Design choices. Another potential problem of your algorithm is that the sampled graphs can become dense. There are hyperparameters \sigma and \beta that control the probabilities and also you limit the sampling only for 2-hop neighborhoods (“To keep graph sparsity, we only sample edges between pairs that are within k hops of each other in G (we always take k = 2 in the experiments).” This is arbitrary and the effect of it on the performance is not clear. How did you select parameters \sigma and \beta? Why k=2? How do you ensure that the sampled graphs are similar to the original one? Does it matter that sampled graphs should have similar statistics to the original graph? I guess, this crucially affects the performance of your algorithm, so I would like to see more experiments on this. •Datasets. Since this paper is mostly experimental, I would like to see a comparison of this model on more datasets (5-7 in total). Verifying on realistic but small datasets such as Cora and Citeseer limits our intuition about performance. For example, Cora is a single graph of 2.7K nodes. As indicated in [1], “Although small datasets are useful as sanity checks for new ideas, they can become a liability in the long run as new GNN models will be designed to overfit the small test sets instead of searching for more generalizable architectures.” There are many sources of real graphs, you can consider OGB [2] or [3]. •Weak baselines. Another major concern of the validity of the experiments is the choice of the baselines. Neither of GNN baselines (GCN, GAT, etc.) was designed for the defense of adversarial attacks, so choosing them for comparison is not fair. A comparison with previous works (indicated in “Adversarial attack on graphs.” in related work section) is necessary. Moreover, an experiment where you randomly sample edges (instead of using Ricci distance) is desirable to compare the performance against random sampling. •Ablation. Since you use GCN, why the performance of Ricci-GCN is so different from GCN when there 0 perturbations? For Citeseer the absolute difference is 2% which is quite high for the same models. Also, an experiment with different choices of GNN is desirable. •Training. Since experiments play important role in this paper, it’s important to give a fair setup for the models in comparison. You write “For each training procedure, we run 100 epochs and use the model trained at 100-th epoch.”. This can disadvantageous for many models. A better way would be to run each model setup until convergence on the training set, selecting the epoch using the validation set. Otherwise, your baselines could suffer from either underfitting or overfitting. [1] https://arxiv.org/pdf/2003.00982.pdf[2] https://ogb.stanford.edu/[3] https://paperswithcode.com/task/node-classification==========After reading the authors comments.I applaud the authors for greatly improving their paper via the revision. Now the number of layers is specified and the explanation of having many sampled graphs during training is added, which was missing in the original text and was preventing a full understanding of the reasons why the proposed approach works. Overall, I am leaning toward increasing the score.I still have several concerns about the practicality of Ricci-GNN. In simple words, the proposed approach uses some metric S (Ricci flow) that dictates how to sample graphs for training. The motivation for using Ricci flow is “that Ricci flow is a global process that tries to uncover the underlying metric space supported by the graph topology and thus embraces redundancy”. This claim cites previous papers, which in turn do not discuss what exactly is meant by “a global process that tries to uncover the underlying metric space”. Spectral embeddings also can be considered as a global metric, so some analysis on what properties of Ricci flow makes it more robust to attacks would be appreciated. Also including random sampling in comparison would confirm that the effect is coming not from the fact that you use more graphs during the training, but from how you sample those graphs. In addition, as the paper is empirical and relies on the properties of Ricci flow which was discussed in previous works and was not addressed in the context of adversarial attacks, having more datasets (especially larger ones) in the experiments would improve the paper. |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	Summary:In Ricci-GCN new graphs are resampled in each iteration of the training phase based on the Ricci flow metric. The Ricci flow incorporates curvature information and captures the intrinsic geometry of the graph. Compared to e.g. spectral embedding it is more robust to structural perturbations. This leads to improved robustness against adversarial attacks on the graph structure.Reasons for score: Overall, I vote for accepting. The idea is well motivated, the paper is well written, and the experiments show a clear increase in robustness on real data. My major concern is not using an adaptive attack to evaluate robustness (see weak points). Strong points:* The main idea of using Ricci flow is interesting, well motivated and well executed.* Evaluation on SBM graphs helps with better understanding why the proposed approach works.* The comparison to other metrics (Spectral and HC) is appreciated.Weak points:* It is not clear whether META (the meta-learning attack) is computed w.r.t. the vanilla GCN or the Ricci-GCN. If META was run on the original GCN it is not clear whether the attack is not successful because Ricci-GCN is more robust or because the adversarial edges found for GCN are not transferable to Ricci-GCN. Since the proposed defense is only heuristic (not certifiable) in order to show robustness it has to be evaluated against an adaptive attacker that takes the defense into account [1]. Otherwise, an adaptive attacker may easily break the defense in the future. For example, META can be adapted to account for the Ricci flow. If META was indeed run on Ricci-GCN, the author should discuss the details, e.g. whether they use the reparametrization trick to compute the gradients through the sampling.* The gain in robustness is only significant for large perturbation rates (>0.1) which might not correspond to realistic threat models in practice, e.g. for perturbation rates <0.1 GCN-SVD is on par with Ricci-GCN.* One interpretation of the proposed approach is that Ricci-GCN is doing (a specific type of) data augmentation which is known to improve generalization and by extension the clean and the adversarial accuracy. For example, one augmentation in [2] is to randomly sample edges to add or remove, and in [3] edges are randomly dropped. Even though [2] and [3] are not motivated by robustness they are a relevant baselines since similar to Ricci-GCN they generated different graphs during training.* A big drawback of the proposed approach is the large number of hyperparameters: \gamma=0.5, p=2, k=2, \sigma, \beta, etc. It this is not clear how sensitive is the method to these choices or to the definition of the "probability measure of each neighborhood".* Ignoring the random attack which extremely weak, the evaluation is limited to a single attack (META).  Evaluating against other attacks (see [4] and [5]) would help to better evaluate the robustness of the model. Question for the authors:1. How robust is Ricci-GCN to adaptive attacks? (see weak points) 2. How does Ricci-GCN compare to other data augmentation techniques (see weak points) 3. Is sampling performed only during training? If so are there any benefits to also sampling during inference (and aggregating the predictions)? Can sampling during inference help defend against evasion attacks such as Nettack?4. Is there any improvement if we are willing to pay the price of decreased sparsity, take k larger than 2?5. Why is the row for perturbation rate of 0 omitted from Table 3? Where the hyperparameters tuned separately for Spectral and HC?Additional feedback that did not affect the decision:* The related work should also discuss the difference between certifiable and heuristic defenses and how the proposed approach fits in this context.* It would be nice to quantitatively show "The probability of edges in the original communitystructure are higher than the attack edges"* It would be beneficial to provide a reference or evidence for the claim "generally lacks descriptive power to provide desirable resolution and differentiation". While the results in Table 3 provide indirect evidence, it is not clear that this is due to "lack of descriptive power".* It would be interesting to see whether Ricci-GCN is also more certifiably robust than vanilla GCN, e.g. by computing model agnostic certificates such as [6]. Typos:* Figure 5 Captions: Purterbation Rate ## After RebuttalThe authors' response clarified some of the issues and partially addressed some of my concerns. Based on this and the remaining reviews I have decided to keep the score unchanged.One additional comment regarding the evaluation: In the authors' response they state "Finally, we would like to point out that it is common practice to use GCN as a subroutine for Meta-attack against different defense methods. This was shown in the original Meta-Attack paper, as well as multiple follow-up defense papers." I would like to again point out that the fact that this is a common practice is not ideal, even though multiple follow-up defense papers use the same strategy. We have already learned the lesson in the computer vision literature that adaptive attacks are the least we can do to evaluate heuristic defenses (see [1]) and even that might not provide strong evidence.References:1. Tramer, Florian, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. "On adaptive attacks to adversarial example defenses."2. Wang, Yiwei, Wei Wang, Yuxuan Liang, Yujun Cai, Juncheng Liu, and Bryan Hooi. "NodeAug: Semi-Supervised Node Classification with Data Augmentation."3. Rong, Yu, Wenbing Huang, Tingyang Xu, and Junzhou Huang. "Dropedge: Towards deep graph convolutional networks on node classification."4. https://github.com/gitgiter/Graph-Adversarial-Learning5. https://github.com/safe-graph/graph-adversarial-learning-literature6. Bojchevski, Aleksandar, Johannes Klicpera, and Stephan Günnemann. "Efficient robustness certificates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more." |||| rating: 6: Marginally above acceptance threshold |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature
9h3KsOVXhLZ	SwinTrack: A Simple and Strong Baseline for Transformer Tracking	long	This paper presents a simple yet strong baseline tracker, named SwinTrack, for single object tracking (SOT). SwinTrack adopts the classic siamese structure, and uses Swin Transformer for both feature extraction and fusion of template and search region. A special motion token which encodes the past trajectory is introduced to further boost the tracking performance. SwinTrack is evaluated on major SOT benchmarks and achieves SOTA performance.  Single object tracking (SOT) is an important problem in computer vision and for video understanding in particular. The proposed method is technically sound and achieves SOTA performance on major SOT benchmarks. Although most components in SwinTrack are existing ones and cannot be said novel, I appreciate the system work which puts them together in a proper way and eventually achieves good results. In particular, the design choices to use concatenation-based fusion over cross-attention-based fusion and not to use query-based decoder are reasonable and well-justified by ablation studies. The motion token design is new, based on my knowledge. According to the ablation, it achieves non-negligible gain, making it a good added component to the tracker.  This reviewer does not see potential negative societal impact of this work.   |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.	The paper addresses the problem of general object tracking, where the target object is given during test time. The authors propose a simple but strong baseline approach based on a Swin Transformer image encoder and a standard transformer decoder. Further, the paper introduces a motion token, which encodes past bounding boxes in order to improve the final prediction. The authors also revise the losses employed to train the tracker. Very strong results are reported on 5 popular benchmarks. Before dwelling into the strengths and weaknesses, I want to discuss the novelty aspect of this paper. On one hand, the novelty is not remarkable: the authors are adopting the Swin backbone, while concatenation fusion is also used in Stark. However, the novelty is not limited to these aspects. On the contrary, I believe that this paper brings substantial value to the tracking field by consolidating existing techniques, while investigating important details in order to achieve a simple, yet highly powerful tracking framework. Importantly the authors provide valuable insights when motivating their approach and comparing to other techniques (modifications of fusion, transformer architecture, losses, etc.). Lastly, I find the motion token a very interesting novelty that could reopen a long-forgotten direction in tracking, namely exploiting motion prediction and other dynamic information. Although seemingly incremental at the first glance, I consider the novelty to be significant based on how much this paper advances the useful knowledge in the field.Other strong points of the paper are:• Very strong results, clearly SOTA.•Simple and elegant architecture.•Insightful discussions.•Method and motivation are easy to follow.•Interesting ablative experiments on multiple datasets.•Relatively fast frame-rates.Weaknesses:1. Details regarding pre-training are missing. I assume that the authors use ImageNet-22k pre-training, while most trackers use ImageNet-1k. This has shown to give about 2-3% on LaSOT in previous papers. The authors should therefore analyze this in a separate experiment.2. By looking into the more detailed results in the supplementary material, it seems that the improvements mostly stem from increased accuracy. That is, better bounding box regression. The robustness (low overlap scores in the success plot) seems to be on par with recent trackers. While accuracy is also important, the major challenge in tracking is to improve the robustness. It is important to discuss these aspects in the paper in order to understand where and how SwinTrack performs better compared to other trackers. Moreover, please add more high-performing trackers to the success plots in the supplementary material.3. Unfortunately, the design of the motion token is motivated. For instance, why are the past box encodings concatenated in the channel dimension, and not processed in some other manner? Why add it as a single token in the transformer, instead of one per box?4. Results on UAV and VOT should be added, even if the tracker does not beat SOTA.5. There are quite a few language mistakes. I could not find discussion of negative social impact or limitations. It would be good to add these.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.	The authors propose an interesting Siamese architecture for single object tracking based on the Transformer model. The encoder is divided into the two feature transforms and a transformer encoding step where correlations of spatiotemporal features on the concatenated branches are made. Instead of CNNs the authors suggest SwinTransformers as branches with shared weights. In the encoding part of the Transformer motion information is added to the stream of features by assuming smooth motion of the object and by maintaining a limited history of the BBs of the object. BBs are regressed with the output embeddings. The experiments show state-of-the-art results on large-scale tracking datasets. The idea to use SwinTransformer instead of a CNN in the branches seems to be novel and also seems to be fruitful at least in terms of algorithmic efficiency and surpassing the 70% limit in LASOT (Fig. 1). Concatenating the features of the branches and then applying a Transformer like attentional mechanism to extract embeddings for which BBs can be regressed is then straightforward [5, 36]. The use of non-learnable queries might be a novel change in the design and beneficial in terms of interpreting the tracking problem as classification problem.Motion consistency restricts the method to track objects properly e.g. when it comes to abrupt motions. Although the common experimental procedure on standard datasets including ablation studies has been conducted, the experiments show little insight into the effects of assumptions and design decisions. While overall benchmark scores and leave-one-out results give an answer to the usefulness of (parts of) the method compared to others, the methodology gives very little insight into the challenges and the potential causes of low performance, e.g. in case of rapid motion, occlusion, illumination changes, etc. I very much miss a deeper qualitative analysis of the model and understanding of its behaviour when it comes to the challenges of tracking.The paper is well structured, however I recommend to proof-read the document as there are typo errors in the text, e.g. line 120. The English needs to be improved, e.g. line 163: “chopped off” sounds flappy. Technically the paper needs also to be improved, e.g. line 145, for n=0 T_traj is not defined properly, for n=1 T_traj = {o_c-delta, o_c, o_c-delta} which makes no sense. No.  |||| rating: 4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper proposes a fully-attentional tracker where both the representation learning and the feature fusion modules are based on the Transformer. Besides, a motion token is introduced to embed the historical target trajectory, further improving the tracking performance. Extensive experiments and ablation study on several large-scale tracking benchmarks are performed to demonstrate the effectiveness and efficiency of the proposed approach. For the strengths, 1) Compared with recent Transformer-based methods that use the Transformer to perform the feature fusion, the proposed tracking framework takes advantage of the Transformer on both the representation and feature fusion modules, which is more thorough and more reasonable.2) The design of introducing the motion token is novel and effective for tracking.3) Experiments, especially the ablation study are sufficient, the effectiveness of each designed module is proved.For the weaknesses, 1) For the motion token, the template-search region pair is based on the single frame, while the object trajectory is sequential. To make the trajectory invariant to the cropping, do all bounding boxes in one trajectory perform the same transformation, i.e., based on the cropping of the current frame? This point is somewhat confusing. 2) The fixed sampling interval of the trajectory is set to 15 and 8 for different datasets, is it the same setting for the training and inference phase? Besides, will it make a significant performance fluctuation when increasing or decreasing this value? I think the setting of this value needs to be explained.  N/A  |||| rating: 9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations. |||| confidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.
SbAaNa97bzp	Understanding Robust Learning through the Lens of Representation Similarities	long	This paper studies the robustness of deep neural networks based on the perspective of representation similarity.  Such a perspective provides an interesting direction to delve deeper into the properties of robust representation learning. The authors make several novel discoveries on "salient pitfalls" in robust networks. According to the observations, the author introduces several ways to design and train better robust networks.  Strength:1. Analyzing the properties of robustness from the representation similarities is intuitive and has been explored by some previous works. However, the paper provided a very systematic study and provided lots of interesting discoveries. 2. The paper is well written. The analysis and discussion are conducted in a very logical way. 3. There are extensive experiments conducted to demonstrate the conclusion and observation. I believe the results are solid. Weakness:1.  I appreciate the efforts in providing different views/frameworks for an important research problem. While I would also like to find more practical effects of the proposed analysis and observations.   yes.   |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	This paper presents a probing analysis on clean (non-robust) vs. adversarially trained robust models. The paper's novelty is questionable and the insights gained are also sort of obvious, e.g., that the representational differences between inputs increase as one goes deeper down the layers. While this is true (and has been observed by the authors in their experiments), the authors didn't make an attempt to analyze whether two inputs from the same class are do have similar representations, which is a good thing. How does this compare between robust and non-robust models?The paper introduces a lot of defence mechanisms (mainly centred around the min-max idea). However, it only employs PGD asdversarial training for obtaining a "robust" model. What about other approaches that provide defence mechanisms against potential attacks, e.g. the references 23, 32, 43 etc. that the authors themselves cite? Strengths:1. Good analysis work on robust vs. non-robust networks.2. Uses CKA to measure representational similarities.Weaknesses:1. Only one version of robust network considered - those trained with PGD based adversarial examples.2. The rational of using CKA is not appropriately justified.3. Some analysis could/should have been at a more detailed level, like one would still want the differences between different classes to be high and similarties between identical classes to be high. What observations can we make regarding this expected behavior in a robust and a non-robust network?  Limitations are not properly explained. The expression "fundamental disjunct between aggregate properties and layer-wise representation similarity metrics..." is rather vague.  |||| rating: 4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	This paper uses an existing method for comparisons of intermediate neural network activations (CKA) for a comparison of robust and non-robust networks. The authors analyze the similarities in different aspects and try to deduce insights in adversarial training. - The paper uses an interesting & potentially insightful approach to better understand how adversarially trained networks process information and how they differ from non-robust networks.- The attached code for this submission is well documented and looks clean, making the results more trustworthy.- It feels like the paper spends too much time describing plots/results compared to interpreting the results and presenting hypotheses for what results mean. - While the authors mentioned criticism on the similarity metric they use (CKA), they don't really explain why this doesn't apply to their analysis. This should be properly addressed.- It is also unclear whether the results differ if a different adversarial attack (with the same thread model) was used for generating the adversarial perturbations - this should ideally also be addressed.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	The authors examine the effects of adversarial robustness training on representation. Specifically, they use CKA to compare robust vs non-robust networks, benign vs. adversarial inputs, and how these comparisons change over the course of learning. They find the following results:- Robust representations are less specialized; distant layers are more similar in robust networks, and block structure is weaker- Early layers in robust networks are largely unaffected by adversarial examples; representations are similar for benign vs. perturbed inputs- Deeper layers overfit during robust learning- Models trained to be robust to different threat models have similar representations Strengths: The paper follows in a well-established tradition of using representational similarity analysis to understand neural network behavior and training interventions. The analyses are well-motivated and straightforward. The results are generally presented clearly and easy to understand. The results are interesting.Weaknesses: The paper seems somewhat limited in scope: It primarily addresses adversarial (worst-case) robustness, which is only one type of robustness. I'm not totally convinced of the utility of this work; it could at the very least do a better job situating itself within existing robustness research. It's unclear if any of the experiments were run in replicate. The figures captions could be more informative and self-contained.Overall, I think this work could be suitable for publication if it is sufficiently revised. The authors devote a paragraph at the end of the discussion to the limitations of their study, which seems appropriate given the space limitations. I do, however, think the authors could be more careful and nuanced about some of their claims.  |||| rating: 7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.	The paper contrasts representation similarities of networks trained to perform image classification with and without adversarial noise. To do so, the authors measure the similarity of representations using the Centered Kernel Alignment (CKA) metric (as well two other similarity metrics in appendix) for CIFAR-10 and two subsets of ImageNet. The authors highlight 1) networks trained with adversarial noise have layers similar to one another compared to those of standard trained networks (which have a block structure) 2) representations in early layers are unaffected by adversarial perturbations both for standard and adversarially trained networks 3) networks trained with and without adversarial perturbations have similar representations until the last 10 or so layers 4) early layers converge faster and later layers overfit to local minima. The authors also analyze the similarity of representations when other perturbations are applied JPEG, Gabor, and Snow, finding  This work investigates representations learned for image classification by contrasting the representation similarities of networks trained with and without adversarial noise—leading to several insights into properties of the learned representations as well as learning dynamics of modern networks. Such properties as the authors point out are not captured by aggregate performance metrics such as loss or accuracy, leading to insights about the learned representations. The paper is well-written, experiments clearly described, and motivation is well-grounded.The experiments conducted are convincing and well-formulated. For example, multiple similarity metrics are compared, variants of the adversarial perturbations are explored, and experimental claims are well-founded. However, the authors'claims should be sufficiently couched within the experimental settings studied: supervised image classification for ResNet-based models on CIFAR-10 and two subsets of ImageNet. For example, the work only studies ResNet-based architectures yet claims to cover “DNNs with different architectures” (line 12) and the “impact of choice of architecture” (line 37). I would expect a comparison of “different architectures” to encompass for example transformer-based architectures, MLP-architectures, etc. I suggest the authors more explicitly couch claims in the introduction, abstract, title, and conclusions within the confines of the experimental settings studied. For example The comparison of representations for "threat models" (JPEG, Gabor, and Snow) in Section 6 was not particularly informative and seemed removed from the primary findings of the remainder of the paper. Robust representations is a broad term. This work studies robustness to random noise in the input. Yet, the authors intermix “robustness to adversarial examples” and often plainly use the term “robust network” to describe a particular type: robustness to noise (defined via L-p bounds on the input) throughout the work. For example, robust in the context of image classification can just as well refer to robustness with respect to rendering method ImageNet-Sketch, artifacts such as blurring (ImageNet-C), adversarial examples (ImageNet-A), or even robustness to natural transformations such as pose (Alcorn et al.). I suggest the authors clarify the wording to only include the specific definitions of robustness studied here.The insights gained from the authors’ analysis are interesting and well-described. The finding of most value, in my opinion, is that later layers overfit (matching existing work relating overfitting in later layers to spurious correlations [1]). While the analysis sheds light on differences between networks trained with and without adversarial noise as well as their learning dynamics the findings are confined to the L-infinity definition of robustness studied for supervised image classification using ResNet-based architectures (for the main claims in section 4 and 5).[1] “Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations” [https://arxiv.org/abs/2204.02937](https://arxiv.org/abs/2204.02937) Yes.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
BygMAiRqK7	Entropic GANs meet VAEs: A Statistical Approach to Compute Sample Likelihoods in GANs	long	SummaryThe authors notice that entropy regularized optimal transport produce an upper bound of a certain model likelihood. Then, the authors claim it is possible to leverage that upper bound to come up with a measure of 'sample likelihood', the probability of a certain sample under the model.EvaluationThe idea is certainly interesting and novel, as it allows to bridge two distinct worlds (VAE and GANs). However, I am concerned about the message (or lack of thereof) that is conveyed in the paper. Particularly, the following two points makes me be reluctant to recommend an acceptance:1)There is no measure on the tightness of the lower bound. How can we tell if this bound isnt tight? All results are dependent on the bound being close to the true value. No comments about this are given.2)The sample likelihoods are dependent on a certain "model". Here the nomenclature is confusing because I thought GANS were a probabilistic model, but now there is an additional model regarding a function f. How these two relate? What happens if I change f? to which extent the results depend on f?3)related to 2): the histograms in figure 2 are interesting, but they are not conclusive that the measure that is being proposed is a 'bona fide' sample likelihood. |||| rating: 5: Marginally below acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	1. The assumption made by the authors that "generator is injective" is problematic or even wrong, as it is well known that GAN suffers from mode collapsing problem. 2. It is very confusing when the authors mentioned the negative Shannon entropy. Because the equation the authors wrote is the Shannon entropy, not the negative version.3. In the 5th paragraph in the  introduction section, the paper (Cuturi, 2013) has nothing to do with "improve computational aspect of GAN", maybe the authors want to cite this paper "Learning Generative Models with Sinkhorn Divergences".4. The authors failed to discuss their paper with "ON THE QUANTITATIVE ANALYSIS OF DECODERBASED GENERATIVE MODELS", which uses AIS to estimate the likelihood.Suggestion:1. Please use \cdot instead of , i.e. F(\cdot) instead of F(.)2. Typo: in Appendix ?? and ??, in section 4 |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The contribution of the paper is to show that WGAN with entropic regularization maximize a lower bound on the likelihood of the observed data distribution. While the WGAN formulation minimizes the Wasserstein distance of the transformed latent distribution and the empirical distribution which is already a nice measure of "progress", having a bound on the likelihood can be interesting.Pros:+ I like the entropic GAN formulation and believe it is very interesting as it gives access to the joint distribution of latent and observed variables. + While there are some doubtful statements, overall the paper is well written and easy to read.Cons:- The assumption of injectivity of the generator could be problematic, as it might not be fulfilled due to mode collapse.- I feel the theory is not very deep. Since one has a closed form of the transportation map (Eq. 3.7), the likelihood of the data is obtained by marginalizing out the latent space. However, this assumes that the inner dual maximization problem is solved to stationarity so that Eq 3.7 holds, which is not the case in practice (5 discriminator updates).- Thus in Sec. 4.1 for the likelihood at various points in training it is not clear what is actually happening.- Sec 4.3 for unregularized GANs might be problematic. In general, the transportation plan is not a density function, so I'm not certain whether Theorem 1 / Corollary 2 still hold. Furthermore, the heuristic for "inverting" G^* is very crude. - There are also some minor problematic statements in the paper. While they can be easily fixed, they give me doubts:  * The original VAE paper is not cited in the introduction for VAEs  * The 2013 paper by Cuturi cited on page 2 has nothing to do with "computational aspects of GANs". It is about fast computation of approximate OT between two discrete prob. measures.   * First-order / second-order Wasserstein distance is I think a bit unusual name for W_1, W_2  * On pg. 4, the point of the entropy term is to make the objective strongly convex. Strict convexity has no computational benefits. |||| rating: 6: Marginally above acceptance threshold |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature
lJuOUWlAC8i	Learning Contextualized Knowledge Structures for Commonsense Reasoning	long	The paper proposes a graph network (called HGN), aiming to better leverage commonsense knowledge graphs (KGs) to solve commonsense question answering and reasoning tasks, by jointly generating representations for new triples from KGs, determining relevance of the triples, and learning graph model parameters. The proposed model is tested on several tasks: CommonsenseQA, OpenbookQA, and CODAH.Pros:-  Overall, the paper is easy to follow, although there are a number of typos or grammatical errors that need to be fixed.  The overall idea is clear.-  Jointly learning (pruning) the graph structure with the network parameters is interesting.-  The proposed model outperforms the baselines in comparison.-  Human evaluation is provided.Cons:-  My major concern about this paper is the novelty and contributions in terms of methodology. Compared to existing methods (e.g., those PG models proposed (Wang et al. 2020)), the novelty of the current submission is rather limited---the proposed model of jointly generating new triples and learning (pruning) the graph structure with the network parameters is an interesting, but a pretty incremental idea. -  The empirical comparison to previous work (e.g., Wang et al. 2020) needs to be clearer to help understand the empirical advantages of the proposed models. The paper mentioned some reason of excluding PG-Full from comparison, but since PG-Global does not include static knowledge embedding and PG-full does, is the latter a more reasonable baseline to be compared with? The model does not achieve better performance than existing models on some tasks, which casts doubts on its effectiveness; e.g., whether its advantage is orthogonal to that brought by stronger models such as those performing much better on the OpenbookQA task.More comments:-  The paper uses much space to discuss neural symbolic approaches. Given the vague benefit of doing so, it may be better to use the limited space to focus more on establishing the contributions w.r.t. existing models; e.g., more details about (Wang et al., 2020) can be provided and compared to in both methodological and experimental analyses.- The human evaluation was performed on the questions with correct questions. More analyses on the edges and weights generated for questions that were not correctly answers may help better understand the proposed model.  |||| rating: 5: Marginally below acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	The paper proposes a question answering model that is augmented with a common-sense knowledge graph (KG). The paper builds on the following two observations — (a) KGs are incomplete often lacking facts that would be needed for reasoning to answer a question. (b) Current methods over-retrieves facts (edges) from the KG leading to a lot of unrelated facts that potentially makes reasoning noisier and harder.The paper first retrieves all possible facts from the KG connecting entities in the question and answer. However, due to the incompleteness of the KG, the retrieved subgraph might be missing important edges between entities. To deal with this, they connect all nodes between question and answer entities and initialize the embedding of the newly added edge with hidden layers of a sentence generated from a fine-tuned GPT2 language model (this important detail was mentioned in the appendix). However, currently the graph is over complete and very noisy. The proposed model then sparsifies the graph by learning edge weights via a two-step message passing process. The edge weight is learned as a function of the current edge representation and the textual representation. Lastly, an entropy term is added to the objective function to encourage more peakiness (and sparsity). The model is tested on three common-sense QA benchmarks and on three of them they beat the baselines albeit only around 1-2%. Statistical significance of the result was not reported. Ablation study show that the efficacy of including the generated edges and pruning the graph. There was a small human-study also done where the annotators were shown a binarized graph and were asked to rate each edge. Annotators had moderate agreement between themselves in finding that the pruned graph was better than the original retrieved graph.Strengths:* Developing models that can use symbolic external knowledge present in common-sense KGs and also overcome the sparsity in KG is important and this model is a step in that direction* The paper achieves a little improvement in performance in all three datasets and ablation experiments are helpful in understanding the results* The paper is clearly written and it was easy to follow for the most partWeaknesses & clarifying questions for the authors:* My biggest complain of the paper in its current form is that several modeling choices were not motivated at all. For example, generating edges between nodes using GPT-2 language model is fairly non-standard. However, the paper lacks any motivation on why this is the right approach to generate facts which are not captured in a KG. What is the guarantee that GPT-2 will not hallucinate and generate a false fact and thereby adding unnecessary noise in the reasoning process.* Following up on the previous point, there could have been several other modeling choices. For example, instead of generating text via a language model, one could gather text (sentences) from Wikipedia or other text corpora containing the entities (which would mean the text would probably not be a false fact). These modeling choices were not explored and were not discussed. * The GPT-2 modeling choice was also moved to the appendix and I think it should definitely be moved to the main section of the paper as it is one of the core technical contribution of the paper.* Another modeling decision that was not motivated was the graph reasoning part. It is unclear to me why the edge weight is modeled as a part of the message passing process. Another (simpler) alternative could be modeling it as an edge attention, which is computed wrt the text and the current node embeddings. I would be curious to know how this simple model worked and if it didn’t why was the case.* Even though there are improvements across dataset, the improvements are relatively minor (<1% in few datasets). I think it would be useful to have statistical significance test.* Regarding the human study, if I understand correctly, was only the node and adjacent matrix shown to the annotators?. Was the relation type (KB relations and generated sentences) included too? If they were not included I think they should be because knowing the relations is also very improvement.* Can you elaborate on the average helpfulness score of edges in table 5? How many (what proportions) were scored 0, 1 or 2 for both the graphs? I think it would also be helpful to report how many facts all/majority of the annotators found to be helpful for both the graphs. Missing Reference: It would be nice to cite Sun et al EMNLP 2019 -- PullNet: Open Domain Question Answering withIterative Retrieval on Knowledge Bases and Text since one of the core contributions of that paper was to retrieve and keep only relevant facts from the KG. Relation paths in KG were explored by several works before Wang et al 2020 such as Neelakantan et al ACL 2015 - Compositional Vector Space Models for Knowledge Base Completion, Das et al EACL 2017 -- Chains of reasoning over entities, relations and text etc. It would be nice to cite those work as well.Recommendation:  In light of the current weaknesses of the paper, I am giving it a score of 5 and I look forward to the discussion.=======11/22======I am deciding to keep the same scores as before. Some of the initial concerns remain. I think the paper still lacks motivation wrt the GPT2 model generating missing edges. Thank you for getting the latest results, the paper is stronger than before and with some more work, I am confident it will be a good contribution to the research community.=====11/24======After having read through the explanation behind using GPT2 as edge features (and sufficient backing by 2 closely related work), I am increasing my score to 6. I think the discussion helped in somewhat convincing me that this approach would work for ConceptNet because of its limited schema. |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	=== Summary ===In this paper, the authors propose a new approach towards incorporating knowledge graphs (KG) into commonsense QA frameworks. KGs are helpful for adding structured "world" information, which neural-symbolic architectures can leverage to do commonsense reasoning, e.g., "what is the expensive resource in printing on paper?" (paper). In such architectures, however, the authors argue that KG quality is a large impediment (e.g., missing or incorrect edges, distracting nodes, etc). Therefore, they propose a "hybrid" KG-based model (accordingly named "Hybrid Graph Network") that jointly learns to refine/augment the graph structure while also optimizing it for inference performance.Experiments are conducted on a number of commonsense reasoning tasks with multiple KG sources, and compared to relevant baselines. They also perform a user study to examine the "helpfulness" of the refined KGs produced by the HGN.=== Justification for Score ===This paper is well-written and well-evaluated. The proposed method is also relatively simple and intuitively motivated. The experiments, however, only show modest (yet still positive) empirical gains. Perhaps not a game-changer for commonsense QA, but still a reasonable contribution that I would recommend for acceptance.=== Strengths ===+ The paper is clear and well-written. + The experimental section is strong. The model is compared to strong baselines, and I appreciated the extra user-study on learned graph structure.+ The method is well-motivated, and provides (modest) empirical gains compared to some baselines.+ The method shows good performance with respect to increasing data efficiency (Fig. 4).=== Concerns ===- The main concern is on the empirical effectiveness of the model. The results appear to give only modest gains at best (against comparable baselines to the best of my knowledge). For a number of the results the variance is large compared to the relative difference---it would helpful to also include tests of significance for these improvements.- On OpenbookQA the model significantly underperforms T5-based models. Though I appreciate T5 is unwieldy due to its large size, it makes me question if this method indeed presents a complimentary gain, or is climbing the wrong architectural hill.=== Update After Rebuttal ===I commend the authors on a through rebuttal and active rewrites/experimentation. I still think the work is good, and can warrant acceptance. However, I still find the empirical results to be only moderate at best (though I appreciated the authors' rebuttal and significance testing). I am keeping my score the same. |||| rating: 7: Good paper, accept |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct
kGvXK_1qzyy	Drift Detection in Episodic Data: Detect When Your Agent Starts Faltering	long	This paper designed a hypothesis testing procedure for detecting changes in episode sequential data.  For online operation, it also proposed a novel Bootstrap mechanism for False alarm rate control. The method is demonstrated based on a non-iid and non-Gaussian setting for reward signals.In all the method is theoretically sound and seems useful for detecting changes, however there is no experiments provided to show that the method can help improving the performance in RL tasks. The strength of this paper would be significantly boosted if the proposed method can be used to solve an non-stationary RL problem.Moreover, it is a little unclear the specific setting of this work. Is the underlying sequential decision making problem based on Markov decision processes(MDPs)?There is no surprise that the RL feedback from an environment is highly correlated over consecutive time-step if the underlying problem is an MDP. It seems that the paper mainly deal with per-step reward without considering state and action information when detecting changes. Some related work that also applies hypothesis testing procedure for non-stationary MDPs might worth mentioning. E.g,Banerjee et al. Quickest change detection approach to optimal control in Markov decision processes with model changes“In setup 2, … we consider a fixed trained agent … , “ It is unclear if the trained agent exhibit an optimal behavior or arbitrary behavior(sub-optimal)? In figure 2, does each curve correspond to a fixed policy?=========after rebuttal===========I appreciate the authors’ effort to address my questions. I still think this paper is below my expectation especially if it is put into the context of RL.  I would expect to see how this method can solve or help solving a fundamental problem (e.g., reducing sample complexity) in RL or be applied to a novel application (e.g., nonstationary RL tasks). Otherwise I didn’t see why it is necessary to compare this method with other statistical testing methods in the context of RL, if they can be easily made in other non-iid settings without mentioning RL.  |||| rating: 5: Marginally below acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	The authors propose a sequential testing procedure for a fixed RL agent reward drift. Major issues:1. The authors state in some places of the text that their method can be applied in statistic domain other than RL. However, for me, the entire proposed methodology does not relate to RL and is more general: it can be applied to any sequential statistics (series of random variables) that have a proper [auto]correlation structure (just see how the main statements). Due to this, I have concerns in the way how these proposed approaches are presented: I believe that a paper requires a notable refactoring to improve the presentation.2. Seem that the contribution is not enough for the current venue.  All the proposed statements (Theorem 4.1, Theorem 4.2) looks very straightforward. Possibly, there untrivial tricks in their proofs, but the current proofs (proof sketch) does not reflect them at all (see). I expect a clearer presentation in the main text why these results are non-trivial and non-incremental. As for now, the provided theoretical grounds looks as non-enough for publication @ ICLR.Other issues:Paper cites the paper [Vineet Abhishek and Shie Mannor. A nonparametric sequential test for online randomized experiments. Proceedings of the 26th International Conference on World Wide Web Companion, pp.610–6, 2017] where seq. testing is applied to A/B tests, but there is an earlier paper on this approach: [Eugene Kharitonov, Aleksandr Vorobev, Craig Macdonald, Pavel Serdyukov, and Iadh Ounis. Sequential testing for early stopping of online experiments. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR’15, pages 473–482, New York, NY, USA, 2015. ACM.]================After the author response, I raise my score by 1 (see my comment to them) |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	The authors present a novel change detection test for non i.i.d. data motivated by applications in RL. At first, they provide an offline version of the test, then they extend it to the online setting.The paper is clearly written and presents both theoretical results and convincing experimental results. My two concerns are about the novelty of what has been proposed w.r.t. standard CDT procedures and on the fact that a consistent part of the material of what has been proposed in the paper is deferred to the appendix.I would like to have more discussion on the difference between what has been proposed here and the standard multivariate CDTs, for instance:Kuncheva, Ludmila I. "Change detection in streaming multivariate data using likelihood detectors." IEEE transactions on knowledge and data engineering 25.5 (2011): 1175-1180.Boracchi, Giacomo, et al. "Quanttree: histograms for change detection in multivariate data streams." International Conference on Machine Learning. 2018.A strong motivation of the novelty w.r.t. to the literature might make me increaese the paper score.In my opinion, the paper is not self-contained. Not even the main theorem proof are included (the sketches are not useful in understanding the proof line) and the experimental setting is described in details only in the additional material. I think you should rearrange some of the material from the appendix to the main paper and viceversa.I would have appreciated a more detailed description of Algorithm 1. In this version of the paper it is difficult to understand the procedure you proposed, if you do not refer to the appendix for details.In your setting the change in the episodic reward is only about the expected values. What happens if the new reward distribution changes in terms of covariance \Sigma?Minor:"in RL ... life-time of the task." I would have preferred a citation about this statement. Showing evidence using your experiment is a bit premature at this stage of the presentation.assume strong assumptions -> require strong assumptions--------------------------------------------------------------------------------------------------After rebuttals the authors significantly improved the presented work, including and discussing some relevant work which was previously missing.  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	This paper considers the drift detection for episodic data, where data episodes are assumed to be i.i.d. but data within each episodic can be correlated. It is assumed that the pre-change (nominal) mean and covariance of each episodic is perfectly known or can be accurately estimated from reference data. The Uniform Degradation Test (UDT) and Partial Degradation Test (PDT) are proposed to detect the mean shift. Moreover, this paper uses bootstrap to control the false alarm rate by setting the threshold as empirical quantiles of the detection statistic computed from reference data. Overall, this paper is well written and well-structured. The most impressive part is that they include comprehensive details, covering almost every related aspect. And this paper has tried to make the problem as general as possible, such as considering non-Gaussian distributions, etc.However, the technical contribution is very incremental. And the problem itself, although is proposed in the reinforcement learning setting, is of no fundamental difference with the classical mean shift in change-point detection literature, for example, change-point detection for a mean shift in multivariate Gaussian distributions. And I didn't see the classical Hotelling T^2 test mentioned in this paper, which is a classical and also widely-used method to detect the change in mean/covariance, and it also utilizes the pre-change covariance matrix in the detection statistic. Moreover, the proposed Uniform Degradation Test methods are a direct consequence of the likelihood ratio test; the false alarm control by bootstrap is also widely used in change-point detection literature for problems where the theoretical characterization of false alarm rate (or average run length) is difficult to obtain. Looking forward, I think this paper may be improved by expanding the horizon of the problem set-up and possibly leading to new theoretical findings. ================ After the author response, I raise my score by 1 ================Thanks to the authors for the detailed response and extended discussion on theoretical results and experiments, and I have raised my score by 1.  |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
i3k6WjDXECC	Hierarchical Channel-spatial Encoding for Communication-efficient Collaborative Learning	long	CL system has the problem of communication deficiency caused by limited bandwidth.Other solution (model slicing, matrix factorization) has may degrade the model accuracy if compressing features.This paper propose new quantization scheme, called SGQ (Stripe-wise Group Quantization).SGQ captures both channel and spatial-level similarity in pixels, and hierarchically encodes the features in these two levels to achieve a much higher compression ratio. *strengthsThe idea is straightforward, and the paper is easy to follow.The observation of motivation is good, and the solution makes sense.*weaknessesIn main paper, Cifar-10, Cifar-100  and mini-ImageNet are too small datasets. CL system has the problem of communication deficiency caused by limited bandwidth.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	In order to improve the communication efficiency in collaborative learning systems, this paper proposes to consider not only the pixel similarities but also channel information by channel-attention grouping. Theoretical analysis is provided to calibrate gradients of both features and CAG blocks in the backward process. Experiments demonstrate higher traffic reduction and image processing speed while keeping comparable accuracy. Strengths:1. The observation this work is based on is that some channels hold quite different features when the corresponding filters are orthogonal to each other. This phenomenon is quite interesting and makes it reasonable to design the channel-attention grouping block. 2. The analysis of gradients in the backward process is thorough and sound.Experiments on NVIDIA Jetson Nano and HUAWEI Atlas 200DK demonstrate high efficiency and practical values of the proposed method with comparable performance with the baseline.Weaknesses:From the first few sections and figure 2, we would consider channel-attention grouping is in front of the stripe-wise group quantization, but these two methods are arranged in the opposite order in the methodology section. It seems to be confusing to readers. The number of clusters $k$ is tied to the bit, then the performance degradation of extreme cases including 1-bit or 2-bit is still a problem.  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.	Network bandwidth poses challenges to the collaborative learning system. The existing communication-optimized methods either are insufficient in reducing the traffic or may degrade the model performance. This work proposed a hierarchical compression algorithm for feature data, called Stripe-wise Group Quantization (SGQ). SGQ refactors feature based on similarity in both channel and spatial levels. This work also presented the theoretical analysis for the convergence order of SGQ. Experiments show that SGQ offers a higher reduction ratio without accuracy loss compared to state-of-the-art methods such as Product Quantization (PQ). Strengths:+ This work tried to tackle the communication challenges of the collaborative learning system deployed in reality.  Experiments show that the proposed method SGQ is very practical:   + SGQ offers a sufficient reduction ratio in the data traffic   + SGQ maintains the model accuracy and the convergence is proved   + the overhead of SGQ is only around 8-9% of the total runtime, which is acceptable compared to the communication overhead, and thus SGQ is able to offer 9.22X to 11.37X speedup.+ The paper is well organized and easy to follow.Weaknesses:- The evaluation section only shows the traffic reduction ratio and traffic size. It would be better to show the tradeoff between accuracy and actual measured latency and throughput.- It is unclear the contribution of each part of SGQ. It would be better to have the ablation study on different parts of SGQ. The authors have adequately addressed the limitations and potential negative societal impact of their work.  |||| rating: 5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly. |||| confidence: 1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.	This work proposes a structured strip-wise quantization method, named Strip-Wise Group Quantization (SGQ), to implement the scalable collaborative learning systems on tiny edge devices through a hierarchical compression on refactored feature structures. Strengths1. The novelty of capturing such channel-dimension structured information to compress feature size is good, which is different from the viewpoint of the conventional methods.2. The authors deliver a clear set of challenges and motivations for deploying CL systems on tiny edge devices.Weaknesses1. This article addresses the communication bottleneck and builds a novel communication-efficient CL system; it is better to bring out a well-through analysis in the experiment section. There are not enough demonstrations regarding communication efficiency.2. The analysis of the sublinear convergence order of SGQ is not clear in Section 3.3. The ablation study is based on the performance of the final model with different quantization bits, which shows the impact of quantization bits. I think it would be better if the performance of other components of SGQ could be added to analyze their different effects on the final model. None  |||| rating: 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations. |||| confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
Dh29CAlnMW	Parsed Categoric Encodings with Automunge	long	This paper introduces a library that preprocesses tabular data called Automunge.For software packages I feel that one of two criteria must be met: The software implements a scientifically novel algorithm, framework, model, etc.; or the software package is so complex that a well-designed implementation in itself is of scientific significance.Whereas Automunge seems like a useful library, I am not convinced that it falls in either of these two categories. As such, I am not sure it justifies a publication at a machine learning conference. I suggest the authors target a different venue (e.g., PyCon, SysML, etc.) or elaborate on the scientific impact of their software (e.g., show experimentally that this framework allows practitioners to train better performing models).Pros* The library seems usefulCons* No significant novelty* Little relevance to the scientific machine learning community* Not very clearly written |||| rating: 4: Ok but not good enough - rejection |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature	The submitted paper describes a very nice featurization library, AutoMunge, that converts NLP into features suitable for NNs.It's clear that the authors of the library have put a lot of thought into its construction, and it looks very useful.However, ICLR is about /learning/ representations, not about feature engineering. So this is off-topic for the conference. To make it on-topic, the authors could, e.g., compare using standard word representation techniques vs AutoMunge on a set of NLP tasks using some popular modern NLP architecture (perhaps BERT?). That would be a really interesting paper. |||| rating: 4: Ok but not good enough - rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This package aims at automating some repetitive tasks that data analysts (especially within the NLP domain) deal with. I confirm that tasks like feature selection and string encoding are beneficial for applied researchers with textual data. I can see a wide interest in the community.On the negative side, the paper lacks a literature review and comparison with any existing and relevant work. I expected to see performance plots for different tasks (whether just for this package or with comparison to some baseline alternatives.As a minor comment, I don't think having "String theory" in the title is a good idea because this keyword is already taken to refer to another scientific topic ( a sub-field of physics). |||| rating: 6: Marginally above acceptance threshold |||| confidence: 2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper
Y0MgRifqikY	Visual Explanation using Attention Mechanism in Actor-Critic-based Deep Reinforcement Learning	long	- Summary    - This paper proposes an interpretable RL agent architecture that uses attention masks to produce visual explanations of the action selected by the policy and output of the value function     - The authors demonstrate their method on 3 Atari games and use A3C as the training algorithm- Strengths    - To the best of the reviewers knowledge, this is the first work to apply this type of visual explanation to RL    - The interpretable agent performs on par with the black box one.- Weaknesses    - How were the points/frames in figure 2 chosen?    - To my untrained eye, the attention masks in figure 2 aren't very interpretable.  Human studies to verify that the explains actually help the humans understand (or predict) the agent's decision would be very helpful in this regard.    - I am uncertain that the contribution is enough to warrant publication at ICLR.  While this is the first work I am aware of to apply this type of visual explanation to RL, using attention masks is well known in the literature (Mascharka et al, 2018; Fukui et al, 2019) and it doesn't appear like any considerable modification necessary to apply it to this domain.    - Using the attention masks to to interpret the decision of the agent based on just the current frame is misleading.  This is because the attention is conditioned on s_t, not o_t, where s_t the output of ConvLSTM(o_t, s_{t-1}).  The consequence is that we do not know whether attention is high for a given location because of the visual information in o_t or the visual information in any other observation. While it is entirely plausible that the most influential location in the ConvLSTM output is most correlated with the current frame, this hasn't been shown.- Suggestions    - Show both the frame and the frame with attention in Figure 2.  Currently it can be hard see the content of the frame.- Overall    - Overall, I am not convinced the contribution is enough for publication at ICRL.  More importantly, without additional verification the attention masks cannot be used to explain the decision based on the current frame as they are conditioned on the current frame __and__ all previous frames.  - References    - Mascharka et al, 2018: Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning    - Fukui et al, 2019: Attention Branch Network: Learning of Attention Mechanism for Visual Explanation## Post RebuttalI have increased my rating slightly but still don't think the paper is ready for publication.  I am still not convinced by the quality of the provided visual explanations nor am I convinced that the attention is well correlated with the current frame (the additional experiments provided do help somewhat in this regard, but are not extensive and reasonably inconclusive). |||| rating: 4: Ok but not good enough - rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	Summary:The paper introduces an attention mechanism into A3C-based reinforcement learning agents to identify the attended visual regions for vision-based reinforcement learning tasks. Specifically, they applied a mask-attention mechanism for the policy and value prediction neural network columns of the A3C model based on the convolutional LSTM (Xingjian et al., 2015). They evaluated the proposed attention mechanism on three selected ATARI games (namely, Breakout, SpaceInvader, and Ms.Pacman) and show that the attention mechanism generates intuitive visual attention in decision-making. They also compared the attention's impact on game scores against some ablative attention mechanism variants.  Pros and Cons:++ The paper contributes to transparent decision making of reinforcement learning methods by figuring out attended regions in the observational space (pixels). The identified attention regions are intuitive and action-conditional. The cases discussed in the three selected ATARI games are informative.-- The analysis of the learned attention masks seems selective. Some automatic metrics or systematic studies of different game categories (shooting, maze-like, and ball-and-paddle) may shed light on the learned attention's general property. Current analyses on very sparse time indexes of three selected ATARI games may not provide sufficient evidence or insights to support claims.  Some additional experimental studies on other games or similar domains with high-dimension perceptions would strengthen the paper's contributions. -- The paper briefly mentions other attention mechanisms for reinforcement learning methods. It seems that some in-depth discussions on the relationship between the proposed approach and the prior art are needed. How is the proposed attention mechanism different from previous ones? Does it address some limitations of previous methods, such as capturing more action-conditional information or more robust to initialization conditions? An additional empirical comparison would also be informative. -- The motivation and goal on the inverting gaze area are less clear. It would be of interest to see if the attention mechanism would make the learned policy robust to interventions in un-attended regions.   -- The paper has some confusing details. The comparison method named Mask-Attention A3C Double seems identical to the proposed Mask-Attention A3C method. Some clarification on this would be helpful. The paper also has some typos. ``is indicates'', ``our method also learn'', ``is calculates'', etc.   |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This article proposes to include an attention mechanism to Deep RL (focusing on actor-critic architectures), to provide a visual "explanation" of the learned policy. There is solid evidence that attention is an important aspect in perception and learning, and the use of soft attention to improving deep neural network performance has been successful for a variety of tasks such as visual recognition (Wang et al, CVPR'2017) and image captioning - the proposed approach is similar to those. The proposed approach is to learn attention maps to inhibit part of the visual feature, separately for the value and policy networks. The attention maps being differentiable can be learnt jointly with the rest of the network. The approach is straightforward and similar to attention maps used in, eg, image captioning. The issue of including attention in RL is not completely unresearched. In particular, it would be valuable to discuss the 2019 DeepMind paper at NeurIPS paper by Mott et al ("Towards Interpretable Reinforcement Learning UsingAttention Augmented Agents") as the claims and purported aims are similar. The discussion of the type of attention that could be applied (eg, map vs spotlight, soft vs hard) is missing in the article as only one model of attention is evaluated. The article is generally clear, although some design choices could have been discussed in more details and some arguments are unclear. For example, I did not understand the author's argument of why using bottom-up saliency requires backpropagation. More importantly, the novelty of the article is not clearly argued: The use of attention maps to analyse and explain deep neural networks is not new in itself, and learning attention maps to improve vision tasks is not new either. Another issue is that I think the use of the term "explanation" is a bit misleading in this article: the proposed approach provides activation maps, which offer some hints at the system's process, but still require a large amount of human interpretation for an actual explanation. In sum, the article is fairly well written, but the contribution should be outlined more clearly, and more experimental work could be provided to justify what type of attention is most effective - the relation to some previous works (in particular Mott et al) would also be desirable. Refs. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. ICML'2015.Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, XiaogangWang, and Xiaoou Tang.  Residual attention network for image classification.  CVPR'2017. Mott, Alexander and Zoran, Daniel and Chrzanowski, Mike and Wierstra, Daan and Jimenez Rezende, Danilo. Towards Interpretable Reinforcement Learning Using Attention Augmented Agents. NeurIPS'2019 |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper applies the mask attention mechanism on the DRL model (actor and critic), to make the learned policy explainable. The empirical results in Atari 2600 show that the performance of A3C is further improved by implementing mask attention on the actor and critic network separately.Strength: - The paper is well-written and easy-to-follow.- The experiments demonstrate the effectiveness of the attention mechanism in three Atari environments.- The comparison of the score by the inverting gaze area is interesting.Weakness:- The contribution is marginal. The idea is straightforward. It is not a new concept that implementing an attention mechanism in a deep neural network to make it explainable. Applying attention mechanisms to augment the RL agent is also studied in previous work[1, 2, 3]. This key idea of this paper is similar to them, inserting the attention mechanism into the network to explain the RL policy in a top-down fashion. The difference is in the implementation details of the mask-A3C, e.g. using ConvLSTM in the state encoder to keep the spatial information and applying different attention mechanisms for the actor and critic. It is necessary to further discuss and compare these works in the paper. - The method is only evaluated in three environments. It is necessary to validate it in various environments, especially showing the results in 3D environments (visual navigation, robot arm manipulation). It would be nice to report and discuss the failure cases in the experiments, instead of only the success cases.Ref:[1] Mott, Alexander, et al. "Towards interpretable reinforcement learning using attention augmented agents." Advances in Neural Information Processing Systems. 2019.[2] Manchin, Anthony, Ehsan Abbasnejad, and Anton van den Hengel. "Reinforcement learning with attention that works: A self-supervised approach." International Conference on Neural Information Processing. Springer, Cham, 2019.[3]Tang, Cheng-Yen, et al. "Implementing action mask in proximal policy optimization (PPO) algorithm." ICT Express (2020). |||| rating: 4: Ok but not good enough - rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
xW9zZm9qK0_	Class2Simi: A New Perspective on Learning with Label Noise	long	This paper proposes a new algorithm on learning noisy datasets by transforming class labels into pairwise similarity labels. It also gives some theoretical analysis on the fact that the induced similarity noise transition matrix works better than the class noise transition matrix. The paper empirically demonstrates that the proposed method works well on several synthetic datasets and a large-scale real-world dataset.Strengths:- I believe the idea to use pairwise similarity as supervision is novel and interesting, and it is easy to implement.Weaknesses & Questions:- I think the analysis is a bit problematic. Th. 2 shows that when the number of classes is large (>8), the noise rate of similarity labels is less than class labels. And the authors use Th. 3 to prove that if the noise rate of transition matrix decreases the model will have a better generalization. However, as far as I understand, the supervision effect of the pairwise label differs a lot between positive and negative labels. In fact negative pairwise supervision is not very meaningful as there are a lot of gradient directions that can minimize the loss. Thus I think evaluate on the noise ratio of the whole pairwise similarity matrix is not very meaningful. And since the supervision effect of class-level and similarity-level labels is so different, it casts questions on the whole theoretical analysis in my understanding.- The baselines on CIFAR seem too low compared with the SOTAs, e.g. [1], and the improvement of the proposed method is limited. And the final result is not comparable as well. For example, under the setting of 0.5 sym noise of CIFAR-10, the best result of the proposed method is 81.15, while [1] has 84.78.[1] Chen, Pengfei, et al. "Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels." International Conference on Machine Learning. 2019.-----------------------------Post Rebuttal ModificationRegarding A1: I agree with R2 that the theory has major concerns and the authors were not able to fix it during rebuttal. I think we need to be clear that whether the method can work empirically and whether the provided theory can explain it are two problems. Now it seems to me that it is clear that the theory is wrong, and the problem is that the authors did not take into account the difference of the class-wise labels and pairwise labels. I suggest the authors to change the theory completely or remove the theory before next submission.Regarding A2: I don't chase SOTAs and I could certainly appreciate works that give nice theoretical insight but limited improvement. Now that the theory is wrong I have to be critical about the experiments. Since the performance is much worse than STOA, it is no longer clear whether the proposed algorithm works or it's just because the baselines are too bad.I adjusted my rating from 5 to 1.-----------------------------Regarding the authors' 2rd and 3rd responsesFirst, please allow me to clarify that my wording "the theory is wrong" means the theoretical justification on why the proposed algorithm can benefit from the transformation and achieve better performances is wrong, as the authors wrote “This theoretically justifies why the proposed method works well” in their submission. The major flaw/concern has been raised by R1(Q1) and myself(Q1), and the authors’ responses on these two questions are not convincing. This is the concern that I have been asking, so I assume it is not “vague”. I don’t see any potential way to fix this major concern in the current theoretical justification sketch, so I think this submission needs a major revision. I want to give my apology if my wording "the theory is wrong" leads to misunderstanding to the authors or other reviewers.Second, I would like to see ACs or PCs to step in and let me know if I could rate the submission as 1 in this case. I have temporarily increased my rating from 1 to 3 as it has been questioned by the authors, especially the author who “have served as a reviewer 100+ times and as an area chair 10+ times for top conferences like NeurIPS/ICML/ICLR”.What’s more, I would also like to request apologies from the authors. The wording “angry” is unpleasant and misleading. As the author asked, “what are you angry for?”, I’m not angry at all. I simply adjusted my post-rebuttal rating with my expertise after reading the authors’ responses and other reviewers’ comments.Finally, I would like to remind the author who “have served as a reviewer 100+ times and as an area chair 10+ times for top conferences like NeurIPS/ICML/ICLR”, one of the main rules of academic writing is to avoid using second person. I hope this will be helpful.Best |||| rating: 3: Clear rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	**Summary**The paper addresses the problem of learning with noisy labels by transforming the original category classification task into a semantic-similarity prediction task. The new task takes pairs of samples as input and predicts if the two samples are coming from the same category or not. It is theoretically shown that the similarity pairs have lower noise rates compared to the original categorization. It is then argued that this lower noise rate makes the learning more robust in the presence of class label noise. In practice, the idea is applied on top of a standard categorization network (equipped with other approaches to handle class label noise). Specifically, for each pair, a similarity score $\in[0,1]$ is obtained from the category outputs of the two samples as the dot product of their softmax distributions. Then, a similarity transition matrix (that is pretrained) is applied to the prediction. Finally, a two-class cross-entropy loss is optimized. The similarity transition matrix is obtained by using prior techniques for finding a $C\times C$ class transition matrix (with $C$ being the number of classes) and then analytically turning it into a $2\times2$ similarity transition matrix.**Quality**The writing is of a noticeably-low quality and seems to have been overly-rushed. The experiments are done on several datasets of different modalities. The theories are partially informative but not entirely and/or directly relevant.**Clarity**The method section is not clearly written. Section 3.1 and 3.2 were quite hard to follow due to some notational inconsistencies, lacking definitions, and not being self-contained (relies on the knowledge of Hsu et al. 2018). The relevance and implications of the theories are not properly discussed. **Originality**The idea of turning category classification to similarity prediction in order to make the learning robust to class label noise seems quite original to the reviewer’s knowledge.**Significance**The results show improvements on various benchmarks as well as various underlying category classification methods for learning under noisy labels. The improvements are not always substantial but they seem to be statistically significant and consistently present for large synthetic symmetric noise levels of CIFAR100. The idea being general and the results somewhat significant indicate a potential significance of the method.**Major technical comments***Theory*1. when learning with similarity labels, is it important to consider the total noise levels among similar and dissimilar pairs or also the worst case of noise level in similar pairs and dissimilar pairs separately? To make this more clear, imagine the extreme case that all of the given labels are noisy. Even for this case, when constructing the similarity labels, while all similar labels can be wrong, still the majority of dissimilar labels (and vast majority if the number of classes are high) will be correct. This renders the similarity noise rate to be (arbitrarily) low (depending on the number of classes and samples per classes). Does this low noise rate mean the task is learnable although there is literally zero information on true class labels? I doubt it. That would essentially mean we can take any set of images and apply random similar/dissimilar pairs to them and consider it a low-noise-rate dataset. Thus, formal analysis and/or informal theoretical discussions are required in this regard.2. from algorithm 1 it seems the method requires two independent trainings, shouldn’t that at least double the computational complexity? Learning with pairs could potentially take longer to converge due to the quadratic increase in the number of input data. How is the claim in section 3.3 that “Class2Simi increases the computation cost *slightly*” supported?3. regarding theorem 3, based on the first point above, I have a concern that the relevant risk to be studied here should be the original category classification risk as opposed to the similarity risk. The latter could be dominated by the dissimilar pairs and unless formally analyzed or at least directly discussed it’s hard to draw any conclusion on how the bound on the similarity prediction empirical risk translates to a bound on the original classification risk which is the objective of interest.*Experiments*1. Regarding the experiments on Clothing1M, while I understand the raised points regarding the dominant class confusion, I believe the method should still be compared on the original dataset along with the proposed Clothing1M*. In fact, this can be a weakness of the proposed method that should be studied further and more thoroughly with designated experiments. Such a noise is possible in the real-world applications due to semantic ambiguity or human error (as demonstrated in Clothing1M).2. Is the same set of hyperparameters used for all the baselines as well as the variants of the proposed approach? How are the hyperparameters optimizations done? In particular, which variant of the method or baselines are the hyperparameters optimized on? In our experience, when it comes to noisy labels, it is quite common that different methods perform better with different sets of hyperparameters, so it’s important to optimize the parameters per method.3. the improvements for asymmetric noise, and on MNIST and CIFAR seem marginal. A statistical paired significance test could be useful.**Minor technical comments**- the notation of the summands’ subindices $i,j,i’,j’$, in theorem 1 is a bit confusing. For instance, when $i=i’$ should only $i$ be used?- the similarity label is denoted as $H_{ij}$ in section 3.1but as $S_{i,j}$ in figure 2 and section 3.2.- what is the difference between samples denoted by $X_i$ in section 3.2 and $x_i$ in section 3.1?- what does $\theta$ parametrize? The network or the similarity transition matrix? What is the difference between $f(X_i)$ and $f(X_i;\theta)$ in figure 2?- the algorithm defines a function $g(.)$ which is not referred to in the methods section- where are parameter matrices $W_1,...,W_d$ defined? What is d?- how is the expected risk $R$ defined? - what is $\hat{f}$ - what is $R_n$?**Overall**While the reviewer can guess the meaning of some of the notations based on the literature it renders the paper hardly readable and at times the discrepancies were unresolvable to the reviewer. Furthermore, there are concerns regarding the motivation, relevance of the theories, and the significance of the results. So, overall, while I believe the paper has original contributions of potentially high impact I strongly believe it needs a major revision before it’s presentable at a conference.  **Post Rebuttal**I had concerns regarding the clarity, theory, and experiments. During the rebuttal phase, the authors actively discussed various points raised by the reviewers and the AC. Despite its length and breadth, I do not find them addressing the core of the raised concerns. That is except my 2nd point of the major theory concerns, regarding the time complexity and convergence time which is at least partially addressed.  The time complexity is addressed since the length of the first round of training for transition matrix is negligible compared to the main training. The convergence time would also be addressed if the number of epochs for the proposed method is the same as the baselines. The reviewer is not entirely sure that is the case though. Given the outstanding majority of the concerns my final feedback is as follows:The paper provides an original idea for learning with label noise which is positive, to rate the demonstration of the relevance of the idea, I can either consider the paper from an empirical study lens or a theoretical one. From the latter perspective, the concerns above effectively affect the whole theoretical arguments of the paper. The main claim of the paper, even in the latest revision, is based on the noise rate of similarity labels being lower than the noise rate of the corresponding class labels and that this is what can bring improvement in the final performance. This reads absolutely unfounded to the reviewer. As discussed, the similarity rate is mostly influenced by dissimilar labels (in the balanced c>2-classification scenario) and can be made arbitrarily low. Furthermore, the discussion still does not make a clear formal connection between the error bound on the noisy similarity learning and the noisy classification for the general case. Nevertheless, such a connection would require a major revision/addition to the paper that would need a proper round of review.When it comes to the empirical view, the experiments are inconclusive and not thorough-enough for an empirical paper due to 1) the drastic change in the learning setup (which is implemented inhouse including the base transition-matrix methods) in tandem with the fact that hyperparameter optimization is not done per method (e.g., the hyperparameters are taken from the papers for baselines while they are optimized for the proposed method's training). This is especially important since when learning with noisy labels, the choice of hyperparameters are extremely influential in the final results. 2) the improvements are mostly marginal except for CIFAR100. 3) Results are not provided for the original Clothing1M dataset. The shortcoming that led to changing Clothing1M needs to be thoroughly studied for an empirical work since it directly affects the applicability of the paper.On top of these, the final version of the pdf is still lacking on clarity several instances of which were listed in the original review. Thus, considering all the points above leads me to confidently keep the original rating as "clear reject". |||| rating: 3: Clear rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper proposes a new perspective on dealing with label noise, called Class2Simi, by transforming the training examples with noisy labels into pairs of examples with noisy similarity labels and then learning a deep model with the noisy similarity labels. Experimental results on real datasets show that Class2Simi achieves better classification accuracy than its baselines that directly deals with the noisy class labels. The idea to deal with label noise by transforming noisy class labels into noisy similarity labels seems to be novel. The proposed Class2Simi provides a framework to improve different existing learning methods. Furthermore, the paper proves that the noise rate for the noisy similarity labels is lower than that of the noisy class labels. In addition, the paper is well written with good organization. Although in most cases the proposed Class2Simi can improve the accuracy compared with baselines, the improvement is not significant in many cases like those on MNIST and CIFAR10. It is better to provide deep analysis about the principle of the proposed method and the experimental results, and give insight for readers about when the proposed method will achieve significant improvement and what is the underlying reason.-------------------After rebuttal:I thank the authors for clarification. I would like to keep with my score. |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper presented a working framework for learning a robust classifier with noisy labels. It proves that if the number of the classes is more than 8 then the noise rate in the similarity matrix is less than that in the noise rate in labels. Hence after learning classifiers from noisy labels, it updates the classifier using entropy loss between predicted similarity matrix and actual similarity matrix calculated from similarity matrix from data multiplied with noisy transition matrix derived using existing techniques(Xia et al., 2019; Patrini et al., 2017). It also provides the generalization bound for the proposed techniques. The paper has a strong experimental evaluation of the proposed method against recent models on robust learning with noisy labels. My concern is the technical novelty of the proposed model, as learning from the noise transition matrix (Xia et al., 2019; Patrini et al., 2017) and learning from the similarity matrix (Hsu et al., 2019) both are well known. It will be better if the author discusses their contribution in detail. It will be good if the authors also comment on the running time of the proposed method as it is learning the classifier first from the noisy labels. What happens if one learns ‘f’ in stage 2 with a random classifier? In Tabel 1 for most of the cases, forward+C2S is outperforming others. Why? In Table 2, we have seen that model does not make a significant change in the news20 data set. Why so?We have seen that difference in performance is higher with a higher noise rate in the labels. The authors have stated in Theorem 1 that the noise rate for the noisy similarity labels is lower than that of the noisy class labels. But they did not provide any quantitative analysis of that. It will be good to see what is the reduction in the rate with respect to the number of classes and also the noise rate in labels.I tend to accept this paper with clarification asked. Though the paper has combined existing ideas to learn a robust classifier, the proposed classifier is outperforming when the noise rate is higher. Hence it can be useful for the ML communities. Along with this, the paper has also given a generalization bound. ---------------------I thank the author for clarification. I would like to be with my score. |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	In this work the authors propose a method to learn from noisy labels. The propose method converts the noisy labels to similarity labels which are more robust to noise and helps in reducing the noise ratio in the training data. The proposed method has been evaluated on multiple datasets with consistent improvement across all noise ratios.Pros:The proposed idea to convert the class labels to similarity labels is very interesting and intuitive. It has some good properties which leads to a better performance.The authors provide a theoretical analysis of the proposed method to estimate similarity noise transition matrix which makes it more grounded.The authors have provided sufficient experimental results on multiple datasets to demonstrate effectiveness of the proposed learning technique.Cons:For asymmetric noise, the results are shown only for a noise rate of 0.3. How will the method perform when the noise rate is lower or higher in case of asymmetric noise. Also, how does the proposed method compares with existing approaches for asymmetric noisy learning? The asymmetric noisy learning in Li et. al. 2019 (and many others) can achieve around 93% accuracy on CIFAR-10 which is 10% higher then the proposed method. The presented ablation study is not very meaningful. The authors have shown that on a clean dataset, the proposed method does not have any effect, sometimes the score improves, sometimes it goes down. Ideally the performance should not have been gone down, how will the authors explain the results on news20?-- post rebuttal --After carefully reading the authors response and the discussions with other reviewers, I am updating my ratings. The authors acknowledged that it is not a theory paper and therefore, the biggest concern is empirical evaluation. The shown performance is not comparable with existing high performing methods and therefore it is hard to judge without any direct comparison. The authors stated that their method can be applied on top of any existing method, which was not shown in this submission and therefore it will not be meaningful to judge just based on this statement. I believe showing this empirically will definitely strengthen this submission. |||| rating: 5: Marginally below acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
PULSD5qI2N1	Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime	long	Summary: This paper considers the convergence property of averaged stochastic gradient descent on a overparameterized two-layer neural networks for a regression problem. This paper is the first to achieve the optimal convergence rate under the NTK regime. They show that smooth target functions efficiently specified by the NTK are learned rapidly at faster convergence rate.##########################################################################Pros:  + The paper is technically sound. It adapt the neural networks and the RKHSs theory into the NTK regime and the proof techniques are different from existing literatures.+ It achieves the minimax optimal convergence rate of $O(T^{-2r\beta/2r\beta + 1})$ which is always faster than $O(T^{-1/2})$.+ This work shows the connection between RKHS and NN in term of the $L_{\infty}(\rho_X)$ norm while the convergence result does not need the positivity of the NTK##########################################################################Cons:  - The writing of this paper is not clear enough, for example $L_2(\rho(x))$ appeared without any explaination- The practical choice of M in their experiment is 2e4, which is impractical in NN tasks ##########################################################################Overall, I vote for accepting. This paper combine the convergence analysis of averaged stochastic gradient descent on kernel methods with the connection of kernel method with neural network to derive an optimal convergence rate of NN in NTK regime while the proof technique is novel. My major concern is about the practical influence of this paper to the theory of deep learning training, as the width of the output layer is required to be large.#########################################################################Questions: In Assumption A3, why the parameter $r$ has to be in the range [1/2, 1]Is the averaging mechanism at the output of the algorithm beneficial to the rate in Theorem 1? |||| rating: 7: Good paper, accept |||| confidence: 2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper	This paper analyzed the averaged SGD for overparameterized two-layer NNs for regression problems. Particularly, they show that the averaged SGD can achieve the minimax optimal convergence rate, with the global convergence guarantee. To achieve, they propose a new parameter which captures the ``complexities’’ of the target function and the RKHS associated with the NTK.The paper is well-written, and the result looks very interesting. I am tending to accept the paper. This paper is a theory, so experiments are a plus. If the authors can address some of my comments, I am tending to increase the score of the paper.Here are some comments about writing.I believe Assumption 1 and 2 are reasonable. But each statement is just math, it is good to write a 2~4 words to summarize each A1, A2 .. Also several math statements highly replied on the definitions, and it is hard to find them in the paper.After Assumption 1, in the next page, page 5, there is a Remark that has 4 bullets, maybe write Ai at the beginning of each bullet.Algorithm 1 requires more words and explanation, it is hard to understand this algorithm in the following sense : the size of each matrix/vector is not mentioned and hard to find them in the paper.In page 6, is it possible to simplify the statement of Theorem 1 a bit? e.g. write a simplified version here, and put the full version in appendix.In Theorem 1, what is M_0? Is that over-parameterization size? Is that polynomial in parameters or exponentially in parameters? (This is not major point of the paper, I am just curious about the bound) In page 4, Eq. (2), is it possible to consider a simple model where gamma = 0? This is quite common in previous work.In appendix, e.g. page 30, the last step of many equations use ->0. I don’t follow the meaning of this notation. Is that possible to avoid it?This paper is focusing on average SGD, is there any intuition why non-average SGD won’t give the similar result?I felt the following paper is highly close to this work, and should be cited and discussed more deeply. Usually optimization has two parts, one is the number of iterations, and the other is cost per iteration. This paper focused on improving the number of iterations. The following paper improved the cost per iteration, in the NTK overparameterized regime. Training (Overparametrized) Neural Networks in Near-Linear TimeJan van den Brand, Binghui Peng, Zhao Song, Omri WeinsteinMinor commentsIn page 1, second paragraph, the place cited Du et al. 2019b, Allen-Zhu et al. 2019 and Du et al. 2019a.The following two papers should also be citedZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural networks.  [This paper shows the result for recurrent neural networks. Note that RNN is  a harder case, In deep neural networks the weight matrices in different layers are different. However in RNN, the weights matrix are the same over all the layers]Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound.[This paper improved the over-parameterization bound from m >= n^6 (Du et al. 2019b) to m >= n^4, where m is the width of a neural network, and n is the number of input data points.]In page 2, the first paragraph, the place cited Du et al. 2019b, Arora et al. 2019a, Weinan et al. 2019, Arora et al. 2019b, Lee et al,. 2019.The following papers should also be citedJason D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, and Zheng Yu. Generalized leverage score sampling for neural networks. [Arora et al. paper shows a connection between neural networks with neural tangent kernel regression. This paper generalizes the Arora et al result, and shows the connection between regularized neural networks with neural tangent kernel ridge regression.]Similarly, in page 5, some citations should be added.Small typos:The last paragraph, Page 2 “the key to show” -> “the key to showing”The third paragraph, Page 3 “which enable” -> “which enables”The fifth paragraph, Page 4 “ A stochastic gradient descent” -> “Stochastic gradient descent”The third paragraph Page 5 “a neural networks” -> “a neural network”The third paragraph Page 6 “arbitrary small” -> “arbitrarily small”The first paragraph Page 8 “the single-layer learning” -> “single-layer learning” |||| rating: 8: Top 50% of accepted papers, clear accept |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature	Here is the review of the article: ``Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime''.**SUMMARY**The authors show that under the Neural Tangent Kernel Regime investigated lately, averaged SGD achieves optimal rates in the attainable case.  Note that this result is not a *plug and play* based on the current kernelized-SGD litterature: the difficulty the authors achieve to overcome is the fact that they could bound the difference between the dynamics of SGD on the neural networks and on the neural tangent kernel. This is stated in Proposition A and it represents the novelty of the articleMoreover, they give an explicit representation of the capacity condition (decrease rate of the eigenvalues of the covariance matrix) in the cases where they have a smooth approximation of the ReLu.**Clarity**The paper is outstandly clear. Indeed, despite the fact that optimality in RKHS can be technical to introduce, I found the paper very clearly presented and well motivated. It was very pleasant and smooth to read. The references are also both precise and sufficient to understand well the problem.The only default of the paper is that, in my opinion, the authors do not stress enough their contribution and their novelty. Indeed, despite Proposition A and the sketch of the proof, the article consists in a *plug and play* result on averaged SGD. The authors should then stress the outline of the proof (going to a M-approximation of the Neural Tangent RKHS) and comparing the dynamics on these.**Quality and Originality**The paper is not super original, and as accustomed to this literature, there is no surprise seeing this result. However, the quality of the paper is undeniable and fills the gap between optimality of kernel methods and the NTK literature. I thank the authors to have done it very clearly.**Comments**-My main comment is about the fact that except from Proposition A, this article is a *plug and play* one. This proposition, and the full sketch of the proof should be emphasized as they consist on the true novelty of the work.-Three remarks concerning the plots :1-*Minor comment.* They should be bigger.2-*Minor comment.* Figure 1 should illustrate the fact that $\beta = 1 + \frac{1}{d-1}$. I suggest a log-log plot.3-*Intermediate comment.* I do not really see how exactly the discussion of the experimental part illustrates really the result. The discussion is fairly interesting, but I really would like to see a theoretical proposition showing that taking the two layers is better for learning.*** Conclusion***Yet the fact remains that, I would really like this article to be published when the commentaries of the reviewers will be taken into accounts.  |||| rating: 8: Top 50% of accepted papers, clear accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper considers the optimization of a wide two layers neural network (for a regression task) using averaged SGD. The authors consider the Neural Tangent Kernel (NTK) regime. The NTK is a kernel defined using the activation function and the initial distribution of the parameters of the input layer. The RKHS H associated to this NTK is assumed to contain the Bayes predictor. Based on this, the authors derive a convergence rate for the predictor constructed from the T-th iterate of averaged SGD and the Bayes predictor in terms of the L2 distance wrt the distribution of the features. By specifying this bound in terms of the decay of the eigenvalues of the integral operator in H, they obtain an explicit generalization error bound, which is optimal for the class of problems considered in the paper.It seems that the paper solves an important problem related to the training of neural nets. The paper is rather well written even if some paragraph are hard to understand for a non expert. For example, several paragraphs of the paper are dedicated to compare the obtained results with those of concurrent work. The level of technicality of these discussions makes the reading experience difficult (e.g. last paragraph of Page 6), often because the discussion happens at a step where the reader is not familiar with the results (e.g. Section 1.2). These paragraphs seem like a discussion with the authors of these concurrent works. I would suggest to gather these discussions at the end of the paper, once the reader understands the results. Moreover, a better approach in my opinion would be to explicitly state the results (mathematically) of these concurrent work. This way, the comparison will be easier.As a non expert, I believe that the results are new but I cannot be sure because I cannot compare with existing works (see my comment above). I recommend acceptation.Overall, the paper shows an important result: optimal rate for generalization bounds for 2 layers NN in the NTK regime. The result is well explained but some precision could make the paper even more insightful. For instance, why considering the NTK regime? What is the intuition behind that? How would you define mathematically "the NTK regime" ? I would also like to understand better the relaxation of the positivity of the NTK. Does it have to do with the assumption that the norm of the integral operator is greater than lambda?Moreover, Proposition A, which is fundamental in the approach, should be stated in the main paper for a better understanding. I think that the authors can move the numerical experiments to the appendix to win some space.MINOR:Check the def of excess risk, last equation of Page 3"negative dependence on" should be "inverse dependence on"", That is, "Why is Figure 1 in Section 1? Is it a mistake? It should not be placed here. Moreover it is not commented in the text. |||| rating: 8: Top 50% of accepted papers, clear accept |||| confidence: 2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper	Summary:The paper focuses on the understanding of neural tangent kernel (NTK), which has been a central topic in deep learning theory recently and plays an important role in characterizing the generalization ability of artificial neural networks. In Particular, the authors derive minimax-optimal learning rates of the averaged stochastic gradient descent method for over-parametrized two-layer neural networks with smooth activation functions. The results are novel and offer insights into the connections between deep learning methods and kernel methods. One difference of this paper from other studies is that the positivity of NTK is not required in the error analysis. Numerical experiments are also illustrated to confirm the theoretical results. The paper is well written and interesting to read. Overall, I vote for accepting.Concerns:1. Although the paper considers smooth approximations of the ReLU and also shows the explicit optimal rate, its analysis does not apply to ReLU networks which may be of greater interest to researchers. 2. It looks a little bit strange to me that the regularization is conducted around the initial values. Will this lead to a big difference in the numerical experiments?3. Is there an explicit form for the lower bound of network width in Theorem 1, i.e., $M_0$? |||| rating: 7: Good paper, accept |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct
cvNYovr16SB	Unsupervised Active Pre-Training for Reinforcement Learning	long	The paper presents a pre-training scheme (APT) for RL with two components: contrastive representation learning and particle based entropy maximization. Experiments are done in DeepMind Control Suite (DMControl) and Atari suite to show improved performance and data efficiency. **Strength**: Pre-training good representations and policy initialization without reward is obviously an important direction in RL. The proposed method is conceptually simple and intuitive, yet achieves some promising results. Particle based entropy estimation (eq. (5)) can be a simple and interesting solution to estimating observation entropy in high dimension space.**Weakness**: Some concerns about experiment results:- For DMControl results, it seems APT only improves significantly over "From scratch" and "Count-based pre-training" schemes when reward is sparse? I also think the result can be made much stronger if more powerful baselines can be adopted, e.g. DIAYN, VISR, etc.- For Atari results, APT versus VISR is interesting. APT achieves better median normalized scores but VISR achieves better average normalized scores. Is it possible to show a game breakdown to understand where these methods work better?- In section 4.3, it is awkward that I find APT and APT (representation) similar across different curves in Figure 4, so I'm not convinced the policy initialization is useful. Maybe some more results (e.g. on Atari) can help with this. Also see the **Novelty** part below.**Novelty**: The two cores of the paper, contrastive representation learning and entropy maximization, are quite established in RL. Particle based entropy estimation is from prior work, but I think it is fairly novel and interesting in the RL domain (despite its usefulness being doubted). **Clarity**: I think the paper is written clearly in general.**Question**: Conceptually eq. (5) looks quite noisy (dependent on batch samples a lot), and like some form of contrastive learning objective still (maybe entropy maximization is equivalent to making representation contrastive?). So is it really a stable reward signal? Is it possible to just use eq. (1) as reward also? |||| rating: 5: Marginally below acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	This submission presents a technique for unsupervised pre-training of representations and policies for RL. Unsupervised representation learning has obtained impressive results in supervised scenarios, and adapting these methods to RL is an important research direction. One of the main challenges in the RL setting is that of defining the distribution of data to learn from, as well as sampling from it. The learned representations are unlikely to be useful for observations that are out of the pre-training distribution, so it is desirable to perform representation learning on data that is representative of the full state space. Previous works (Hazan et al., Lee et al.) proposed strategies to train agents that induce maximally entropic state visitation distributions, but they involve density estimation whose underlying assumptions are not well suited for pixel observations. The authors propose to overcome these limitations by using a particle based entropy estimate in the learned representation space. The pre-trained representations and policies can be used for RL from pixels, obtaining faster convergence and higher end scores than the considered baselines in both DMControl and Atari.The method is novel and the experimental results are strong. The paper includes a proper literature review and it is generally easy to follow. However, it would benefit from a more detailed analysis in order to understand where the reported gains come from. The ablation studies suggest that most of the gains are due to the learned representations: fine-tuning the policy as well only provides slightly faster convergence in two Hopper tasks, but the end performance is the same as when transferring representations only. I would like to suggest the following:- Reporting the performance of zero-shot transfer for all experiments, i.e. the average return of the pre-trained policy in each task (APT@0). I suggest doing this for both DMControl and Atari experiments.- It would be very helpful to see per-game scores in Atari in order to understand what type of environments benefit the most from pre-training. Gains could come from faster convergence on dense reward games thanks to the pre-trained representations, or from higher end performance on hard exploration tasks due to the exploratory behavior of the pre-trained policy. This is important given the fact that APT obtains much higher median human normalized scores than VISR while reaching much lower mean scores.- Related to the previous point, comparing full APT and APT (representation) in a selection of Atari games requiring different degrees of exploration would provide insight on the type of challenges APT is addressing.- APT (representation) can be understood as pre-training a state representation. How does this version of APT compare to RL from true state on DMControl? A similar analysis was reported by Srinivas et al. (Figure 7, see full reference below).Please note that most of the suggestions above do not require running new experiments and can be addressed by providing additional information about the ones that are already reported in the paper.Other comments:- To the best of my knowledge, there doesn’t exist a standard procedure for fine-tuning policies, especially for actor-critic architectures such as the one in SAC. It would be helpful to include a description on how this is performed. Is the critic fine-tuned as well, or is it initialized from scratch?- Section 3 mentions a robustness analysis showing the impact of varying k in kNN, but I didn’t see it in the paper. Is it referring to initial experiments, or to some ablation study that was not included for some reason?- SimCLR fits representations using an objective that is based on cosine similarity, but the reward derived from the particle-based entropy estimate employs L2 distance. Is there a reason for this discrepancy?- The subtitle “Supervised training @100k” in Table 1 is not very accurate, as two out of five rows use more than 100k interactions. I suggest using “Fully-supervised training” as the title and appending “@100k” to the name of the first three methods.- A single unsupervised pre-training can be leveraged to solve multiple tasks as long as the environment does not change, as shown in the DMControl experiments. I believe this is one of the most appealing properties of the method and might not be highlighted enough in the paper.The following works are relevant and might be worth citing:- Laskin, Michael, Aravind Srinivas, and Pieter Abbeel. "Curl: Contrastive unsupervised representations for reinforcement learning". ICML 2020.- Lee, Lisa, et al. "Efficient exploration via state marginal matching". arXiv preprint arXiv:1906.05274 (2019). |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	*SUMMARY*The paper proposes a method to simultaneously learn effective representations and efficient exploration in a reward-free context. The algorithm iterates between minimizing a contrastive loss and maximizing an intrinsic reward derived from a k-NN entropy estimation of the state distribution. Then, authors empirically evaluate the method over a set of visual Mujoco tasks and Atari games.*STRENGTHS*- the paper addresses a very relevant reward-free exploration objective as a preprocessing to RL- the paper combines representation learning and state entropy maximization into a promising practical method*WEAKNESSES*- the paper might be too incremental with respect to previous (albeit unpublished) work- the paper somewhat fails to empirically illustrate and validate the reward-free phase*EVALUATION*Unfortunately, over some concerns regarding the novelty of the presented method and its experimental validation, which I find somewhat weak for an essentially empirical work, I would lean towards rejecting the paper.*DETAILED COMMENTS*C1) The exploration component of APT has striking similarities with the method in [1], which also seeks the optimization of a k-NN estimate of the state distribution entropy in a reward-free context. While it would be in general acceptable to overlook unpublished work, I think that the connections with [1] are too many to avoid a deeper discussion over distinctive contributions. The method in [1] does not seem to learn representations, but I am wondering if this contribution alone would be substantial enough.C2) The paper does not present an explicit empirical evaluation of the reward-free phase, thus I am not sure on how APT is performing in entropy maximization. Especially, what is the impact of the three sources of bias that are introduced in the entropy estimation (avoiding bias correction and constants, avoiding importance weighting, scaling distances with the standard deviation)?Can authors present state entropy plots, and possibly compare the performance of APT with other methods seeking a similar objective, such as MaxEnt (with representation learning), SMM [2] or MEPOL [1].C3) I have some concerns on the theoretical and practical implications of considering a reward function that is actually depending on the current policy. It is sound to fit a value function for a reward of this kind? Maximizing an ever-changing reward might cause instability and prevent convergence?C4) The benefit of employing a non-parametric method to estimate the entropy of high-dimensional inputs is clear, since density modeling would be quite hard. Could density modeling over a reduced latent space be a viable option instead?C5) I would argue that the idea of simultaneously learning representations and exploration is the main selling point of the presented method, since maximizing the state entropy might help learning superior representations and viceversa. But from the ablation study in Section 4.3 this conclusion does not arise naturally, as learning representations alone seems almost as good as learning both. May I ask the authors to clarify this point?C6) The scores on Atari games are reported without confidence intervals, leaving some doubts over the statistical significance of the results. Moreover, it is not completely clear from the aggregate performance where and how APT is helping in these experiments. Can authors provide a deeper explanation on why the original performance of SimPLe and VISR is not reproducible?[1] Mutti and Restelli. A policy gradient method for task-agnostic exploration. Arxiv, 2020.[2] Lee et al. Efficient exploration via state marginal matching. Arxiv, 2019*QUESTIONS*May the authors address the comments listed above in their response?*ADDITIONAL FEEDBACK*- For the Atari experiments I would suggest to focus more on hard-exploration games, such as Montezuma or Pitfalls, instead of providing just an aggregate performance over full sets of games. It would be nice to include some visualizations and interpretations on the behavior that APT learns in the reward-free phase.- I believe that the multi-environment pre-training setting is quite interesting and, to the best of my knowledge, completely novel. The results are promising, and I would suggest to include this setting, together with a deeper analysis, in the main paper.- Dashing the lines in the reported plots would help visibility, especially without colors.- typos and rephrasing:- when referring to the environment I would use reward-free instead of task agnostic (e.g., page 2, paragraph 3)- the 26 games subset instead of the 100k subset (page 7, last lines)- I would rephrase "The notable difference is that APT (representation) decouples the action space dimension from pre-trained models" which is not crystal clear (Section 4.3). ####################AFTER RESPONSEI would like to thank authors for their detailed response and for their effort in improving the paper according to reviewers' suggestions. Unfortunately, after authors clarifications, I still have some doubts on the concerns raised with C1 and C2 (see below). Thus, I am keeping a slightly negative evaluation for this paper. Authors claim that the main benefit of APT over a prior work method (MEPOL, Mutti er al., 2020) is a lower variance of the gradient estimation, thanks to the choice of avoiding importance weights corrections. However, in Figure 6 MEPOL does not seem to suffer a particularly high-variance. To me, the most likely reason for the improved performance is that APT guarantees an action-level feedback as opposed to a trajectory feedback (see [1]).However, I am still skeptical about this action feedback: the reward-to-go becomes non-Markovian and Bellman equations does not hold anymore (see [2]). This casts some doubts on the actor-critic procedure APT employs to optimize the rewards. Authors may have a good point on the notion that the encoder is breaking the dependence between policy and rewards, but I think the topic warrant some additional discussion.I would suggest the authors to rephrase this work to give a more central role to the scalability to high-dimensional observations, which I believe is the main contribution of the paper, and to include a more thorough discussion of (Mutti et al., 2020) in the main text (beyond the related work section).[1] Efroni et al. Reinforcement learning with trajectory feedback. Arxiv, 2020.[2] Zhang et al. Variational policy gradient method for reinforcement learning with general utilities. NeurIPS 2020. |||| rating: 5: Marginally below acceptance threshold |||| confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature
H1xEwsR9FX	Convolutional CRFs for Semantic Segmentation	long	+ well written+ Good idea- Technical section not fully clear- Some experimental issuesThe paper is well written, and clearly explains the background material and concepts. It might almost be a bit too detailed, as the main technical section (4) feels a bit rushed. (more below). From what I can judge the main idea in the paper is sound. The authors replace the large filtering step in the permutohedral lattice with a spatially varying convolutional kernel. They show that inference is more efficient and training is easier.The technical section is not very clear. For example: Are the filter weights recomputed for each spatial location, is there any acceleration that speeds this up? How large can the authors make the filter kernel, before the perhutohedral lattice is faster again?Finally, the experimental section has some room for improvement. I liked the comparison of decoupled and coupled CRF training, but I didn't get much out of the synthetic experiments. I found it particularly confusing since Table 1 doesn't mention that the experiments use ground truth (test) labels that were corrupted.Second, it would be nice to have a side-to-side comparison between ConvCRF and CRFasRNN. I'd recommend the authors to either use the CRFasRNN training setup for both methods, or spend the week or two training CRFasRNN using their training procedure. It is fine to do either of the two experiments and have four entries in that table. |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The authors propose an efficient method to perform message passing on a truncated Gaussian kernel CRF. The main contributions are the definition of a specific form of truncated Gaussian kernel that allows for fast message passing via convolutions, and the implementation of such parallelized message passing on GPU. In my opinion, the paper fails to convey the main idea in a clear and precise manner, the notation is mixed and often confusing, furthermore there are a number of sentences that should be rephrased to be less sensationalist, or removed. The experiments seem to show performance in par with the FullCRF on decoupled training, which seem in contrast with the much bigger performance gain of the first experiment on syntetic data. No discussion has been provided as to the possible reasons of this performance gap, although the experimental settings appear to be similar. Finally, in the last experiments with end-to-end training the authors report a performance improvement over CRFasRNN, a 3 years old paper that is far away in terms of performance with the current SOTA on Pascal VOC. The authors base on a different network than that of the CRFasRNN baseline (i.e., the difference is not only in the CRF implementation, but rather the whole network before the CRF in the proposed method), it is therefore difficult to say whether the performance improvement is due to the ResNet101 + FCN unary potentials, which is not a contribution of this manuscript, or to the proposed CRF. In general, I believe that the considerable speed gain of the proposed method might be enough to justify a publication, but the paper should be phrased in that sense if that was the intention of the authors. It is unclear to me whether the main contribution they claim is segmentation performance (IoU) or speed or both. The main contributions of this work should be stated clearly, and the modelling differences w.r.t. the FullCRF model that they aim to improve should be more explicit in the text rather than let to the reader to infer comparing the formulas.On these grounds, I suggest a major revision of the paper and I don't recommend publication at this stage.MAJOR1) I firmly advocate against making strong claims, unless supported by solid proofs. I strongly recommend to rephrase, if not remove, exaggerate claims such as:a) "[deep networks] lack the capability to utilize context information and cannot model interactions between predictions directly". This is simply not true. Any CNN with enough layers will exploit contextual information. Furthermore, any autoregressive model will model the interaction between predictions directly. See e.g., "RiFCN: Recurrent Network in Fully Convolutional Network for Semantic Segmentation of High Resolution Remote Sensing Images" by Mou et Al., "ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation" by Visin et Al., or "Predicting Deeper into the Future of Semantic Segmentation" by Luc et Al. for video semantic segmentation.b) "CRF inference is two orders of magnitude slower than CNN inference": this, of course, depends on the kind of CRF.c) "The long training times of the current generation of CRFs also make more in-depth research and experiments with such structured models impractical": again, not true. While it's true that CRFs tend to be slow, research with such models is not impractical and indeed there are papers that focus exactly on that (among the others, some of the ones cited in this manuscript).d) "we propose to add the strong and valid assumption of conditional independence": as with every assumption, this is an approximation. I wouldn't claim it to be valid nor invalid, as it is simply a modeling choice.e) "Predictions are pixel-wise and conditionally independendent (given the common feature base of nearby pixels). Structured knowledge and background context is ignored in these models.". It's unclear what is meant by "structured knowledge", but to my best understanding this sentence is misleading or wrong. Background context is considered by CNN-based models, as well as the general structure in the image.f) "In the context of semantic segmentation most CRF based approaches are based on the Fully Connected CRF". CRFs have been used much earlier than 2011, before Fully Connected CRF was published.g) "This makes the theoretical foundation of ConvCRF very promising, strong and valid assumptions are the powerhouse of machine learning modelling." The authors here propose a logical, but quite obvious, approximation, i.e., to constrain the CRF to model dependencies in a local neighborhood. This is the same implicit assumption of many other Gaussian kernels and algorithms for approximated inference. For how it's surely valid, and possibly strong, I don't see how it can make a "theoretical foundation". The self-congratulatory closure is unnecessary, and inappropriate.2) While CRFs have been used for a long time on visual data, the citations in this work focus mostly on the last few years. I suggest to add at least one of the following:* “Discriminative fields for modeling spatial dependencies in natural images" 2003* "Multiscale conditional random fields for image labelling" 2004* “Textonboost: Joint appearance, shape and context modeling for mulit-class object recognition and segmentation,” 2006It could also be beneficial to the reader to add to the references broad overview works, such as "An Introduction to Conditional Random Fields" by Charles Sutton and Andrew McCallum, 2011, and/or "Structured prediction and learning in computer vision" by Nowozin and Lampert, 2011.3) Line 5 of the algorithm adds the unary potentials at each iteration of message passing. Can you elaborate on the motivation behind this choice? The algorithm is already initialized with such potentials, and to my best knowledge unary potentials are not usually added in the mean field message passing loop. It would be interesting to compare the performance of the algorithm with and without this addition.4) Sec4.1 reports that the filters are constant over the channel dimension c and that, in other words, this can be seen as applying the convolution over the dimension c. I fail to understand this sentence. My understanding from the formula is that the same kernel is applied to all the channels of the input, i.e., the channels of the input fed to the CRF are all processed in the same way. This should be explained clearly and the reasoning behind this choice should be explained as well. Furthermore, if I am not mistaken the authors learn a different filter in each position *of the input* rather than reusing the same filters at every position. This choice should be clarified and discussed.5) Regarding the implementation, is there a reason not to apply a convolution with a flipped kernel to compute the cross-correlation? Also, IIRC if the kernel is symmetric (as should be for a Gaussian kernel) convolution and cross-correlation are the same. 6) The notation is often ambiguous and at times unnecessarely heavy. I strongly recommend to go over the manuscript and use a consistent notation, making sure that every element of the notation is introduced before or right after it's used. In particular,a) Sec3: in the text, a segmentation instance is referred to as X, while in the formula as \hat x. \tilde I is never introduced. k_{\alpha} is defined but I believe never used (I suggest to drop the name if there is no reference to it). Is there a reason to drop the subscript G in the FullCRF pairwise potential? Or conversely, is there a reason to have it everywhere else? Furthermore, there doesn't seem to be a difference between k_G, k_g and g, I suggest to use only one consistent notation to refer to the Gaussian kernels throughout the paper. It's also unclear if the I superscript is needed for the feature vectors. b) Sec4.1, The shape of the Gaussian kernels is the same as that of the input. I believe that the input in this context refers to a patch and not to the whole image. If so, this should be specified, otherwise the dimensions of the kernel should be referred to with a different letter than those of the input. c) Sec4.1, I believe dx and dy refer to the in-kernel displacement. Their semantic is not clear from the text and should be defined properly.d) Sec4.1, the feature vectors are defined in the text as f_1..f_d. The formula of the kernel uses f_i^{(d)} instead. It's unclear what the superscripts stands for and whether it is actually useful or redundant.e) Sec4.1, x and y are not defined, I suspect they refer to the position of the pixel, which was previously encoded as p_i and p_j. Once again, the notation should be consistent across the manuscript.f) In the definition of the convCRFs, w is used for the width of the input, w_i for the weights of the kernels. In the FullCRF, w^{(1)} and w^{(2)} for the potentials, w^{(m)} for the sum over the kernels. k is used for the kernel dimension, k_G to refer to the kernel itself, as well as g. The notation could be made less ambiguous and consistent (superscript vs subscript semantics).g) Sec3, the number of pixel is defined as n but in Formula 2 N is used instead.g) Vectors and matrices should be bold-face. The use of capital letters for constants might also improve the readability of the manuscript.7) The experiments with the Conv (ConvCRF?) variants of Table 2 are not discussed in the text.8) Although Sec5.2 concludes with "The experiments also confirm the observation of Sec5.1, that ConvCRF performs slightly better than FullCRF", Sec5.1 reported that "it can be seen that ConvCRFs outperform FullCRFs significantly". The authors should decide whether the results are slightly better or outperform the baseline. In general a in-depth discussion on the performance of the algorithm is missing.9) Sec5.3, it's unclear what this sentence means "introduce an auxiliary unary loss to counterbalance the vanishing gradient problem". If such a term has been added, it should be reported in a formula and it's effectiveness should be supported by experimental data.MINORm1) In the related work, the sentence "transposed convolution layers are applied at the end of the prediction pipeline ot produce high-resolution output" seems to suggest these are always applied, while many recent methods rely on bilinear upsampling to recover the original resolution. Please rephrase it accordingly.m2) In Parameter learning in CRF: "the idea utilizes, that for the message passing the identity .. is valid." This sentence doesn't make any sense to me. Is it possible it is a leftover?m4) In Sec3, the features vectors [...] may depend on the input image I. I am confused as to when they might be independent of the image. Can you elaborate on that?m5) In Sec3, it's unclear to me what the vertical bar in the Pot model stands for. I believe the correct formula should be 1_{[xi != xj]}.m6) In mean field inference, Algorithm 1 does not refer to FullCRFs.m7) End of page 6, "Note that this gives FullCRFs a natural advantage. The performance of CRFs however is very robust [...]". Why is this an advantage for FullCRFs? How does that relate to the following sentence?m8) Sec4.1, the authors claim that one of the key contribution of the paper is that exact message passing is efficient. Given the locality assumption, message passing is approximate - which is also why it's efficient. The authors could instead argue that using convolutions is faster and possibly leads to better final performance than using the permutohedral lattice approximation (although it's unclear whether this is the case from the experiments), with proper reference to compelling results in this direction.Finally, a few typos:* Abstract, space missing after GPUs* Introduction, Convolutional Neuronal -> Convolutional Neural* Introduction, order of magnitude slower then -> than* Introduction, to slow -> too slow* Parameter learning in CRF: missing space before proposed to use gradient descent* Parameter learning in CRF: gradient decent -> descent* Parameter learning in CRF: extra comma after "another advantage of this method is"* Sec 3: "weighted sum of Gaussian kernels", the apex of the second should be "M" I believe.* Sec 3: "can be chosen arbitrary" -> arbitrarily* Sec 5.2, beginning of page 8: then -> than |||| rating: 4: Ok but not good enough - rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	The paper is well written with many relevant references and easy to read. Some points that need clarification and mentioned below. The main points of this paper are the use of the convolution operator to perform the message passing mean field inference. Using this operation allows us to get away from the permutohedral lattice and yet allows speed up of 100x. This also means, that training will be able to done faster. Besides this the training parameters can also be learnt. These are the main contributions. The denoising task experiment shows positive results. The idea could be used in the future by others looking for faster model inference and training.If a Manhattan distance d is used i.e. dx,dy<k in equation (6), why is this a FullCRF? It seems like the new CRF is no longer a fully connected one. Page 5, first paragraph describing how the reorganization in the GPU is avoided is not very clear. It would be helpful to a reader to have more details and explanations about this.It is not clear from the experimental results how much improvement allowing to train the CRF parameters gets or might get. Comparing to the Deeplab results etc for the non-trained case, the non-trained model still seems to be performing competitively. Table 2 of Table 3 does not really bring out the advantage of training. The +C, +T, +CT don't seem to be hugely different in terms of validation metrics. Note that Table 3 does not mention other models that might not be trained (assuming that those results are in Table 2) but the text also mentions that the training is not completely fair.In section 5, Unary, it is mentioned that the network is not trained on larger datasets like other work, why?And under CRF, what does iterations are unrolled mean?In section 5.1, why does the random flipping help in simulating inaccuracies?Minor points:Abstract: Add space after "GPUs.".Would be good to define what Q, *, ' indicate in paragraph 4, page 2."hight" -> "height" in section 4.1 |||| rating: 6: Marginally above acceptance threshold |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct
Skh4jRcKQ	Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets	long	The paper examines the use of STE for learning simple one-layer convolutional networks with binary activations and non overlapping patches. In this setting, the gradients are 0 almost everywhere (gradient of sign(x)) hence it is not clear how to use gradient descent. The approach studied here is to instead use the gradient of an alternative function such as ReLU or identity which is not always 0. The authors prove that if the ReLU's gradient is used then under gaussian distribution, the algorithm will converge to the local minimas/saddle points of the expected squared loss. they also show that the same does not hold for the identity's gradient.The proof technique is interesting and the results do show the validity of the STE approach. The fact that the loss is provably monotonically decreasing is a strong validation. The paper is clearly written. However, I do have the following concerns/questions:- The authors claim that their analysis is the first to analyze STE however I would like to point out that [1] studies the same setting (they allow overlapping patches and other distributions) with ReLU activation and show convergence guarantees to the global optima with using identity gradient instead of the ReLu gradient. Also for a single binary output case, using STE as identity equals the perceptron algorithm which is very well studied in literature.- Restrictive setting: gaussian input, no label noise, non-overlapping architecture. Not clear what the motivation for this setting is. Also binary activations are rarely used in practice. Analysis also seems tied to the gaussian distribution.- Infinite sample assumption is strong.- No guarantees for convergence to the optimal solution unlike prior work.- Assumptions on the weights being lower and upper bounded by a constant at each iteration seems strong unless an explicit projection step is used. Could the authors explain why this is a valid assumption to make?- In the experimental section, momentum is used whereas it is not mentioned in the analysis. Does the STE perform well without the momentum? It is unclear why quantized ReLU is used.[1] Surbhi Goel, Adam Klivans, and Raghu Meka. "Learning One Convolutional Layer with Overlapping Patches." ICML 2018.----------Apart from one concern (refer to comment), the authors have responded to most of my other comments. Based on this, I think the paper does offer an interesting analysis of the STE approach used for training binary networks, hence I'm increasing my score. |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	Summary:The paper presents an analysis of training single-layer hard-threshold (binary activation) networks for regression with a mean-squared loss function using two different straight-through estimators: the original identity-function STE and a ReLU-based STE. The paper demonstrates that training with the latter against the population loss is guaranteed to converge to a critical point, whereas using the former can cause instability in the training.    Pros:        - Interesting analysis that provides a novel method for determining which gradient estimators are effective for training single-layer binarized networks and which are not.        - The paper is fairly clear, despite being quite technical; however, I did find myself jumping around a lot to refer back to previous results or definitions so the ordering and layout could definitely be improved.    Cons:        - Related work is missing and some claims in the paper are wrong as a result.        - A single-layer binarized network is essentially just a perceptron, which we know how to learn already, so it’s not clear how this analysis will benefit analysis of multi-layer binarized networks (however, since it seems like a novel analysis approach, it’s possible that it can be extended). This connection is not made in the paper.        - The paper does not analyze the most common and successful straight-through estimator: the saturated straight-through estimator, which uses the derivative of the hard_tanh activation (e.g., see [2]) and is a shifted and scaled version of the clipped ReLU STE.Overall, I like the paper but it has too many issues currently for me to give it a high score. However, if my questions and comments are addressed sufficiently, I would be happy to improve my score.Detailed questions and comments:1.The claim that “we make the first theoretical justification for the concept of STE” is wrong and should be significantly toned down and clarified. Bengio et al. (2013), which this paper cites, provides some theoretical justification already, as do papers on target propagation, such as [1] and [2]. These, as well as additional papers cited in [2] are quite relevant and should also be cited and discussed.2.The claim that “it is not the gradient of any function” is also wrong. Each STE is the gradient of a particular function, but is not the gradient of the function used in the forward pass. Please clarify.3.The single-layer binarized network architecture studied in this paper can equivalently be framed as a linear function of a collection of single-layer perceptrons with shared weights. Obviously, much work has been done on analyzing the perceptron architecture. Why is none of it discussed in this paper? How does that work relate to the work done in this paper? How does the convolutional layer used here change the results of that related work? 4.(a) Is there an intuition for why the derivative of the ReLU performs better (i.e., converges) better than the identity? Why does clipping the bottom make it work better? I do not see this explained in the text anywhere and it would be helpful to include this. (b) Further, depending on the reasoning given, it seems that clipping the top may also be useful (as in the clipped ReLU, which is a shifted and scaled version of the saturated STE discussed in Hubara et al. and [2]). Does your analysis extend to this STE? This would be very useful, as the SSTE/clipped ReLU is the most commonly used STE and the most empirically successful (as validated by your own experiments, as well as in previous work on training binary networks). Also, the SSTE/clipped ReLU is an even better approximation of the step function.(c) Cai et al. (2017) is not the first use of the clipped ReLU activation function, since it is equivalent to the SSTE when using sign(x) \in {-1, +1} instead of your activation function (\sigma(x) \in {0, 1}) (i.e., you can shift and scale everything to get equivalent results).5.In section 3.1, you mention that when using the derivative of the ReLU for the STE then \mu`(x) = \sigma(x). Is this just a coincidence or does this fact help with convergence?6.Why did you choose to train your networks initialized with the weights from their full-precision counterparts? When you train using different initializations, does this significantly affect your results?7.The improved empirical performance of the clipped ReLU / SSTE is unsurprising but why does the vanilla ReLU STE perform so poorly on CIFAR-10 with ResNet-20 with 2 bit quantization?8.In the end, it’s not clear that training single-layer hard-threshold networks is particularly important. Instead, the goal of quantization, etc. is to train multi-layer hard-threshold networks. Can this analysis be extended to such networks? Does it say anything about training such networks currently?9.The acknowledgments section is just the text from the style file.10.The capitalization is wrong in a number of places in your references.[1] Difference Target Propagation. Lee, Zhang, Fischer, and Bengio. ECML/PKDD (2015).[2] Deep Learning as a Mixed Convex-Combinatorial Optimization Problem. Friesen and Domingos. ICLR (2018).------------------------After reading the author response, they have sufficiently addressed my main concerns. I think that this is a good paper that will be of interest to those concerned with understanding the training of activation-quantized / hard-threshold neural networks. I have thus increased my score. |||| rating: 7: Good paper, accept |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	This paper provides theoretical analysis for two kinds of straight-through estimation (STE) for activation bianrized neural networks. It is theoretically shown that the ReLU STE has better convergence properties the identity STE,  by studying the properties of the orientation and norm of the course gradients for STE.While the paper presents many theoretical results which might be useful for the community, they are not organized very well.  It is a bit hard for readers to quickly find the most important theoretical results.  Moreover, some symbols are used without definition, e.g. g_{relu} is used before being defined in sec 3.1. The discussions for most theoretical results are very short or not organized well, making the whole paper hard to follow,  e.g., "the key observation ..." after Lemma 4 is actually not about the Lemma 4 above, but Lemma 5 in the next Lemma.  Another major concern is that activation quantization is usually used in combination with weight quantization.  It would be more useful if weight and activation quantizations can be analyzed together.Clarity in the experiment part can also be further improved. From Table 1, the clipped ReLU STE has the best performance, however, there is no theoretical analysis for it. For ResNet-20 with 2-bit activation, the training loss/accuracy results of vanilla ReLU is much worse than clipped ReLU, is there any explanation for this?  For the discussion in sec 4.2, what information does it want to convey?  What is the "normal schedule of learning rate"? What if the small learning rate 1e-5 is kept after 20 epochs?Typo: The last sentence on page 3, the definition of y*.------------------------The author response have addressed most of my concerns. Thus I have increased my score.  |||| rating: 6: Marginally above acceptance threshold |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct
r1V0m3C5YQ	Coupled Recurrent Models for Polyphonic Music Composition	long	The paper is well written and presented, giving a good literature review and clearly explaining the design decisions and trade-offs. The paper proposes a novel factorisation approach and uses recurrent networks. The evaluation is both quantitative and qualitative. The qualitative experiment is interesting, but there is no information given about the level of musical training the participants had. You would expect very different results from music students compared to the general public. How did you control for musical ability/ understanding?The paper has a refreshing honesty in its critical evaluation of the results, highlighting fundamental problems in this field.Overall, while I am not an expert in musical composition and machine learning, the paper is clear, and appears to be advancing the art in a reliable fashion. |||| rating: 7: Good paper, accept |||| confidence: 3: The reviewer is fairly confident that the evaluation is correct	Composing polyphonic music is a hard computational problem. This paper views the problem as modelling a probability distribution over musical scores that is parametrized using convolutional and recurrent networks. Emphasis is given to careful evaluation, both quantitatively and qualitatively. The technical parts are quite poorly written.The introduction is quite well written and it is easy to follow. It provides a good review that is nicely balanced between older and recent literature. Unfortunately, at the technical parts, the paper starts to suffer due to sloppy notation. The cross entropy definition is missing important details. What does S exactly denote? Are you referring to a binary piano roll or some abstract vector valued process? This leaves a lot of guess work to the reader. Even the footnote makes it evident that the authors may have a different mental picture -- I would argue that a piano roll does not need two bits. Take a binary matrix: Roll(note=n, time=t) = 1 (=0) when note n is present (absent) at time t. I also think the term factorization is sometimes used freely as a synonym for representation in last paragraphs of 4 and first two paragraphs of 5 -- I find this misleading without proper definitions.The models, which are central to the message of the paper, are not described clearly. Pleasedefine function a(\cdot) in (2), (3), (4), : this maybe possibly a typesetting issue (and a is highly likely a sigmoid) but what does x_p W_hp x x_pt etc stand for? Various contractions? You have only defined the tensor as x_tpn. Even there, the proposed encoding is difficult to follow -- using different names for different ranges of the same index (n and d) seems to be avoiding important details and calling for trouble. Why not just introduce an order 4 tensor and represent everything in the product space as every note must have a duration? While the paper includes some interesting ideas about representation of relative pitch, the poor technical writing makes it not suitable to ICLR and hard to judge/interpret the extensive simulation results.Minor:For tensors, 'rank-3' is not correct use, please use order-3 here if you are referring to the number of dimensions of the multiway array. What is a non-linear sampling scheme? Please be more precise.The Allan-Williams citation and year is broken:Moray Allan and Christopher K. I. Williams. Harmonising Chorales by Probabilistic Inference. Advances in Neural Information Processing Systems 17, 2005. |||| rating: 3: Clear rejection |||| confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct	PROs-seemingly reasonable approach to polyphonic music generation: figuring out a way to splitting the parts, share parameters appropriately, measuring entropy per time, all make sense-the resulting outputs tend to have very short-term harmonic coherence (e.g. often a ‘standard chord’ with some resolving suspensions, etc), with individual parts often making very small stepwise motion (i.e. reasonable local voice leading)-extensive comparison of architectural variations-positive results from listening experimentsCONs-musical outputs are *not* clearly better than some of the polyphonic systems described; despite the often small melodic steps, the individual lines are quite random sounding; this is perhaps a direct result of the short history-I do not hear the rhythmic complexity that is described in the introduction-the work by Johnson (2015) (ref. provided below) should be looked at and listened to; it too uses coupled networks, albeit in a different way but with a related motivation, and has rhythmic and polyphonic complexity and sounds quite good (better, in my opinion) -some unclear sections (fixable, especially with an appendix; more detail below)-despite the extensive architectural comparisons, I was not always clear about rationale behind certain choices, eg. if using recurrent nets, why not try LSTM or GRU? (more questions below)-would like to have heard the listening tests; or at least read more about how samples were selected (again, perhaps in an appendix and additional sample files) quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).Quality -- In this work, various good/reasonable choices are made. The quality of the actual output is fine. It is comparable to-- and to my ears not better than-- existing polyphonic systems such as the ones below (links to sample audio are provided here):-Bachbot - https://soundcloud.com/bachbot (Liang et al 2017)- tied parallel nets - http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/ (Johnson 2015, ref below)-performanceRNN - https://magenta.tensorflow.org/performance-rnn - (Simon & Oore 2017)..others as well..Clarity -- Some of the writing is "locally" clear, but one large, poorly-organized section makes the whole thing confusing (details below). It is very helpful that the authors subsequently added a comment with a link to some sample scores; without that, it had been utterly impossible to evaluate the quality. There are a few points that could be better clarified:-p5”a multi-hot vector of notes N”. It sounds like N will be used to denote note-numbers, but in fact it seems like N is the total number of notes, i.e. the length of the vector, right? What value of N is used?-p5 “a one-hot vector of durations D”. It sounds like D will be used to denote durations, but actually I think D is the length of the 1-hot vector encoding durations right? What value of D is used, and what durations do the elements of this vector represent?-similarly, does T represent the size of the history? This should really be clarified.-p5 Polyphonic models.-Eq (2), (3), (4): Presumably the h’s are the hidden activations layers?-the networks here correspond to the blue circles in Fig 1, right? If so, make the relationship clear and explicit -Note that most variables in most equations are left undefined       -actually defining the W’s in Eq(2-4)  would allow the authors to refer to the W’s later (e.g. in Section 5.2) when describing weight-sharing ideas. Otherwise, it’s all rather confusing. For example, the authors could write, “Thus, we can set W_p1 = W_p2 = W_p3 = W_p4” (or whatever is appropriate). -Generally, I found that pages 5-7 describe many ideas, and some of them are individually fairly clearly described, but it is not always clear when one idea is beginning, and one idea is ending, and which ideas can be combined or not. On my first readings, I thought that I was basically following it, until I got to Table 5, which then convinced me that I was in fact *not* quite following it. For example, I had been certain that all the networks described are recurrent (perhaps due to Fig1?), but then it turned out that many are in fact *not* recurrent, which made a lot more sense given the continual reference to the history and the length of the model’s Markov window etc. But the reader should not have had to deduce this. For example, one could write, “We will consider 3 types of architectures: convolutional, recurrent, .... In each architecture, we will have [...] modules, and we will try a variety of combinations of these modules. The modules/components are as follows:”. It’s a bit prosaic, but it can really help the reader. -Appendices, presented well, could be immensely helpful in clarifying the exact architectures; obviously not all 22 architectures from Table 5 need to be shown, but at least a few of them shown explicitly would help clarify. For example, in Fig1, the purple boxes seem to represent notes (according to the caption), but do they actually represent networks? If they really do represent notes, then how can “notes” receive inputs from both the part-networks and the global network? Also, I was not entirely clear on the relationship of the architecture of the individual nets (for the parts) to that of the global integrating network. E.g. for experiment #20, the part-net is an RNN (with how many layers?? with regular or LSTM cells?) followed by a log-linear predictor (with one hidden layer of 300 units right? or are there multiple layers sometimes?), but then what is the global network? Why does the longest part-history vector appear to have length 10 based on Table 5, but according to Table 3 the best-performing history length was 20? Though, I am not sure the meaning of the “bottom/top” column was explained anywhere, so maybe I am completely misunderstanding that aspect of the table? Etc.-Many piano scores do not easily deconstruct into clean 4-part polyphony; the example in Appendix A is an exception. It was not clear to me how piano scores were handled during training. -Terminology: it is not entirely clear to me why one section is entitled “homophonic models”, instead of just “monophonic models”. Homophonic music usually involves a melody line that is supported by other voices, i.e. a sort of asymmetry in the part-wise structure. Here, the outputs are quite the opposite of that: the voices are independent, they generally function well together harmonically, and there is usually no sense of one voice containing a melody. If there’s some reason to call it homophonic, that would be fine, but otherwise it doesn’t really serve to clarify anything. However, the authors do say that the homophonic composition tasks are a “minor generalization of classic monophonic composition tasks”, so this suggests to me that there is something here that I am not quite understanding.The last sentence of Section 5.3 is very confusing-- I don’t understand what lin_n is, or 1_n is, or how to read the corresponding entries of the table. The first part of the paragraph is fairly clear. Table 4: “The first row” actually seems like it is referring to the second row. I know what the authors mean, but it is unnecessarily confusing to refer to it in this way. One might as well refer to “the zeroth row” as listing the duration of the clip :)The experimental evaluation: I would like to hear some of the paired samples that were played for subjects. Were classical score excerpts chosen starting at random locations in the score, or at the beginning of the score? It is known that listening to a 10-second excerpt without context can sometimes not make sense. I would be curious to see the false positives versus the false negatives. Nevertheless, I certainly appreciate the authors’ warning to interpret the listening results with caution.Originality & Significance -- So far, based both on the techniques and the output, I am not entir